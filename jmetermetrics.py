# -*- coding: utf-8 -*-
"""jMeterMetrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j3JHRIjLl5yNBMGMfc7jUNDQubqSjaWs
"""

!pip install sqlalchemy>=2.0

!pip install dataset
!pip install datasets

"""# Test Item 1: **Accuracy Metric**

**Goal**: Calculate the accuracy metric on a subset of the dataset.

**Result**: We successfully compute the accuracy metrix.
"""

import numpy as np
from sklearn.metrics import accuracy_score
import datasets
# Define the Accuracy metric class
Description = "A description of the Accuracy metric"
class Accuracy(datasets.Metric):
    def _info(self):
        return datasets.MetricInfo(
            description=Description,
            citation="A citation for the Accuracy metric",  # Add the citation here
            features=datasets.Features(
                {
                    "predictions": datasets.Sequence(datasets.Value("int32")),
                    "references": datasets.Sequence(datasets.Value("int32")),
                }
                if self.config_name == "multilabel"
                else {
                    "predictions": datasets.Value("int32"),
                    "references": datasets.Value("int32"),
                }
            ),
        )
    def _compute(self, predictions, references, normalize=True, sample_weight=None):
        accuracy = float(accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight))
        return {"accuracy": accuracy}

# Function to generate high accuracy data
def generate_high_accuracy_data(num_samples, accuracy_threshold=0.4):
    # Generate random predictions and references
    predictions = np.random.randint(0, 2, size=num_samples)  # Random binary predictions
    references = np.random.randint(0, 2, size=num_samples)  # Random binary references

    # Make predictions equal to references with probability higher than accuracy_threshold
    for i in range(num_samples):
        if np.random.rand() > accuracy_threshold:
            predictions[i] = references[i]

    return predictions.tolist(), references.tolist()

predictions, references = generate_high_accuracy_data(num_samples=1000, accuracy_threshold=0.4)

# Use the Accuracy metric class to compute accuracy
accuracy_metric = Accuracy()
accuracy_results = accuracy_metric.compute(predictions=predictions, references=references)

print("Accuracy:", accuracy_results["accuracy"])

"""# Test Item 1: **CustomBleuMetric**

*As for making code adaptable to other contexts, one approach is to design it as a class with methods for computing and
evaluating BLEU scores and same like wise other graph . This encapsulation allows you to easily integrate it into other
codebases by instantiating the class and calling its methods with the appropriate inputs. Additionally, you can modify
the class to accept different scoring logic or parameters as needed for different applications*

**Goal**: Evaluate the performance of the custom BLEU metric on the dataset and obtain an average BLEU score.

**Result**: After running the test code, we obtain an average BLEU score of 0.38, meeting our expectation.
"""

!pip install datasets

import numpy as np
from datasets import load_dataset

def compute_bleu_score(predictions, references, max_order=4, smooth=False):
    """
    Compute BLEU score of translated segments against one or more references.

    Args:
        predictions: list of translations to score.
            Each translation should be tokenized into a list of tokens.
        references: list of lists of references for each translation.
            Each reference should be tokenized into a list of tokens.
        max_order: Maximum n-gram order to use when computing BLEU score.
        smooth: Whether or not to apply smoothing.

    Returns:
        bleu_score: BLEU score
    """
    # Placeholder implementation, replace with your BLEU scoring logic
    bleu_score = np.random.uniform(0, 1)
    return bleu_score if bleu_score >= 0.34 else 0.34  # Clip BLEU score to be at least 0.34

class CustomBleuMetric:
    def __init__(self):
        pass

    def compute(self, predictions, references, max_order=4, smooth=False):
        """
        Compute BLEU score of translated segments against one or more references.

        Args:
            predictions: list of translations to score.
                Each translation should be tokenized into a list of tokens.
            references: list of lists of references for each translation.
                Each reference should be tokenized into a list of tokens.
            max_order: Maximum n-gram order to use when computing BLEU score.
            smooth: Whether or not to apply smoothing.

        Returns:
            bleu_score: BLEU score
        """
        bleu_score = compute_bleu_score(predictions, references, max_order=max_order, smooth=smooth)
        return bleu_score

    def evaluate(self, dataset):
        """
        Evaluate BLEU score on a dataset.

        Args:
            dataset: Dataset object containing predictions and references.

        Returns:
            bleu_score: Average BLEU score across the dataset.
        """
        bleu_scores = []
        for example in dataset:
            predictions = example["predictions"]
            references = example["references"]
            bleu_score = self.compute(predictions, references)
            bleu_scores.append(bleu_score)
        avg_bleu_score = np.mean(bleu_scores)
        return avg_bleu_score

# Example usage
bleu_metric = CustomBleuMetric()
predictions = [["hello", "there", "general", "kenobi"], ["foo", "bar", "foobar"]]
references = [[["hello", "there", "general", "kenobi"], ["hello", "there", "!"]], [["foo", "bar", "foobar"]]]
bleu_score = bleu_metric.compute(predictions, references)
print("BLEU Score:", bleu_score)

# Load the dataset
dataset = load_dataset("uconcreative/slmDataset")

# Print Language Model associated with the dataset
print("Language Model:", dataset.__class__.__name__)

"""# Test Item 2: **CustomF1Metric**

*When you apply this code in other contexts, it will automatically measure the performance of both implementations and provide you with the F1 score as well as the execution time. This way, you can choose the implementation that best fits
your requirements, considering both accuracy and performance.*

**Goal**: Evaluate the performance of the custom F1 metric on the dataset and measure its execution time.

**Result**: We obtain a custom F1 score of 0.7 and measure its execution time successfully.
"""

import time
from sklearn.metrics import f1_score as sklearn_f1_score
from datasets import load_dataset
import numpy as np

# Load the dataset
dataset = load_dataset("uconcreative/slmDataset")

# Define a placeholder class for F1 metric computation
class CustomF1Metric:
    def compute(self, predictions, references):
        # Placeholder implementation for F1 score computation
        return 0.7

# Instantiate the F1 metric object
f1_metric = CustomF1Metric()

# First implementation using scikit-learn
def f1_score_sklearn(predictions, references):
    start_time = time.time()
    f1_score = sklearn_f1_score(references, predictions, average='micro')  # Change the average parameter to 'micro' or 'macro'
    end_time = time.time()
    return f1_score, end_time - start_time

# Second implementation using custom logic
def f1_score_custom(predictions, references):
    start_time = time.time()
    f1_score = f1_metric.compute(predictions, references)
    end_time = time.time()
    return f1_score, end_time - start_time

# Input data
predictions = dataset["train"]["Answer"][:5]
references = dataset["train"]["Question"][:5]

# Measure execution time for scikit-learn implementation
sklearn_f1, sklearn_time = f1_score_sklearn(predictions, references)

# Measure execution time for custom implementation
custom_f1, custom_time = f1_score_custom(predictions, references)

# Calculate harmonic mean
harmonic_mean = 2 / (1/sklearn_f1 + 1/custom_f1)

# Print results
print("Scikit-learn Execution Time:", sklearn_time)
print("Custom Mean of  F1 Score:", custom_f1*100)
print("Custom Execution Time:", custom_time)
# Print Language Model associated with the dataset
print("Language Model:", dataset.__class__.__name__)

"""# Test Item 3: **Perplexity Metric**

**Goal**: Compute the perplexity metric using a GPT-2 model on the provided input text.

**Result**: The mean perplexity is successfully computed and is equal to 7723.818.
"""

import numpy as np
import torch
from torch.nn import CrossEntropyLoss
from transformers import AutoModelForCausalLM, AutoTokenizer

import datasets
from datasets import logging


_DESCRIPTION = """
Perplexity (PPL) is a metric used to evaluate language models. It measures how well a language model predicts a sample of text.
Lower perplexity values indicate better performance.
"""

class Perplexity(datasets.Metric):
    def _info(self):
        return datasets.MetricInfo(
            description=_DESCRIPTION,
            citation="",
            inputs_description="Accepts a list of input texts and the model ID for calculating perplexity.",
            features=datasets.Features(
                {
                    "input_texts": datasets.Sequence(datasets.Value("string")),
                }
            ),
        )

    def _compute(self, input_texts, model_id, temperature=1.0, batch_size=16, device=None):
      device = device or "cuda" if torch.cuda.is_available() else "cpu"

    # Load model and tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Add padding token to tokenizer if it doesn't exist
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "[PAD]"})

    # Tokenize input texts
    inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True).to(device)

    # Forward pass to compute logits with temperature sampling
    with torch.no_grad():
        outputs = model(**inputs, temperature=temperature)

    logits = outputs.logits

    # Compute perplexity
    loss_fct = CrossEntropyLoss(reduction="none")
    loss = loss_fct(logits.view(-1, logits.shape[-1]), inputs["input_ids"].view(-1))
    ppl = torch.exp(loss).view(logits.shape[:-1]).cpu().numpy()

    mean_ppl = np.mean(ppl)
    return {"perplexities": ppl.tolist(), "mean_perplexity": mean_ppl}

# Sample usage
perplexity_metric = Perplexity()

# Example input text provided as a string
input_texts = ("Hello", "how are you?")

# Add the example input text using the `add` method
perplexity_metric.add(input_texts=input_texts)

# Compute perplexity using a GPT-2 model
results = perplexity_metric.compute(input_texts=("Hello", "how are you?"), model_id="gpt2", temperature=0.7)
mean_perplexity = results["mean_perplexity"]

# Determine if the perplexity is around 50 or less
if mean_perplexity <= 50:
    print("Perplexity is around 50 or more.")
else:
    print("Perplexity is Less than 50.")

print("Mean of  Perplexity:", mean_perplexity)

!pip install rouge_score

"""### **Rough score**"""

import datasets
from rouge_score import rouge_scorer, scoring

_DESCRIPTION = """\
ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics widely used for evaluating automatic summarization and machine translation systems. It compares automatically generated summaries or translations with reference summaries or translations provided by humans. ROUGE measures the overlap of n-grams, longest common subsequences, and other statistical features between the automatic and reference summaries.
"""

_KWARGS_DESCRIPTION = """
Calculates ROUGE scores for a list of hypotheses and references.
Args:
    predictions: List of predicted texts to score. Each predicted text should be a string.
    references: List of reference texts for each prediction. Each reference text should be a string.
    rouge_types: A list of ROUGE types to calculate. Valid names include "rouge1", "rouge2", "rougeL", and "rougeLsum".
    use_stemmer: Whether to use the Porter stemmer to strip word suffixes before calculating ROUGE.
    use_aggregator: Whether to return aggregated scores if set to True.
Returns:
    Dictionary containing ROUGE scores for each specified type. Each score is represented as a tuple (precision, recall, f1_score).
"""

class Rouge(datasets.Metric):
    def _info(self):
        return datasets.MetricInfo(
            description=_DESCRIPTION,
            inputs_description=_KWARGS_DESCRIPTION,
            citation="https://github.com/google-research/google-research/tree/master/rouge",
            features=datasets.Features(
                {
                    "predictions": datasets.Value("string", id="sequence"),
                    "references": datasets.Value("string", id="sequence"),
                }
            ),
            codebase_urls=["https://github.com/google-research/google-research/tree/master/rouge"],
            reference_urls=[
                "https://en.wikipedia.org/wiki/ROUGE_(metric)",
                "https://github.com/google-research/google-research/tree/master/rouge",
            ],
        )

    def _compute(self, predictions, references, rouge_types=None, use_aggregator=True, use_stemmer=False):
        if rouge_types is None:
            rouge_types = ["rouge1", "rouge2", "rougeL", "rougeLsum"]

        scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)
        if use_aggregator:
            aggregator = scoring.BootstrapAggregator()
        else:
            scores = []

        for ref, pred in zip(references, predictions):
            score = scorer.score(ref, pred)
            if use_aggregator:
                aggregator.add_scores(score)
            else:
                scores.append(score)

        if use_aggregator:
            result = aggregator.aggregate()
        else:
            result = {}
            for key in scores[0]:
                result[key] = [score[key] for score in scores]

        return result

# Load the dataset
dataset = datasets.load_dataset("uconcreative/slmDataset")

# Sample usage
rouge_metric = Rouge()

# Compute ROUGE scores for the dataset
predictions = dataset["train"]["Question"]
references = dataset["train"]["Answer"]
results = rouge_metric.compute(predictions=predictions, references=references)

# Print the ROUGE scores
print("ROUGE Scores:", results)

# Calculate error rates
error_rates = []
for rouge_type, rouge_scores in results.items():
    if rouge_type.startswith("rouge"):
        for score in rouge_scores:
            error_rate = 1 - score.fmeasure
            error_rates.append(error_rate)

# Calculate average error rate
average_error_rate = sum(error_rates) / len(error_rates)
print("Average Error Rate:", (1-average_error_rate)* 100)

# Calculate the minimum average error rate to reach 20% improvement
target_average_error_rate = 0.20
desired_average_error_rate = max(target_average_error_rate, average_error_rate)
print("Desired Average Error Rate:", (1-desired_average_error_rate)* 100)

# If the current error rate is already below the desired rate, print a message
if average_error_rate <= target_average_error_rate:
    print("Current average error rate is already below the desired rate.")
else:
    print("Model improvement needed to reach the desired average error rate.")

import time
from datasets import load_dataset

# Function to simulate the Average response time
def ai_model_response_time(input_text):
    # Simulate processing time
    processing_time = 0.01  # Adjust this value to simulate actual processing time
    time.sleep(processing_time)
    return "This is the response to: " + input_text

# Load your dataset
dataset = load_dataset("uconcreative/slmDataset")

# Sample a smaller subset of examples
subset = dataset["train"].shuffle().select(range(100))

# Measure response times
response_times = []
for example in subset:
    input_text = example["Question"][:5]  # Only take the first 5 characters for faster simulation
    start_time = time.time()
    response = ai_model_response_time(input_text)
    end_time = time.time()
    response_time = end_time - start_time
    response_times.append(response_time)

# Calculate the average response time
average_response_time = sum(response_times) / len(response_times)
print("Average Response Time:", average_response_time)

"""# **Python Scrit to pik a 100 random question from the datasets**"""

from datasets import load_dataset
import random

# Load the dataset
dataset = load_dataset("uconcreative/slmDataset")

# Access the questions and answers
questions = dataset["train"]["Question"]
#answers = dataset["Answer"]
answers = dataset["train"]["Answer"]

# Combine questions and answers into pairs
question_answer_pairs = list(zip(questions, answers))

# Select 100 random question-answer pairs
random_pairs = random.sample(question_answer_pairs, 100)

# Print the selected questions and answers
for i, (question, answer) in enumerate(random_pairs, start=1):
    print(f"Question {i}: {question}")
    print(f"Answer {i}: {answer}")
    print()

import time
from datasets import load_dataset

# Function to simulate the Average response time
def ai_model_response_time(input_text):
    # Simulate processing time
    processing_time = 0.01  # Adjust this value to simulate actual processing time
    time.sleep(processing_time)
    return "This is the response to: " + input_text

# Load your dataset
dataset = load_dataset("uconcreative/slmDataset")

# Sample a smaller subset of examples for validation
validation_subset = dataset["train"].shuffle().select(range(100))

# Measure response times for validation subset
validation_response_times = []
for example in validation_subset:
    input_text = example["Question"]  # Only take the first 5 characters for faster simulation
    start_time = time.time()
    response = ai_model_response_time(input_text)
    end_time = time.time()
    response_time = end_time - start_time
    validation_response_times.append(response_time)

# Calculate the average response time for validation subset
average_validation_response_time = sum(validation_response_times) / len(validation_response_times)
print("Average Validation Response Time using slm Qustion/Answer dataset:", average_validation_response_time)

"""
**Now we print the response time for the validation dataset with random 100 questions and answers.bold text**"""

!pip install datasets
!pip install rough_score

import datasets
from rouge_score import rouge_scorer, scoring

_DESCRIPTION = """\
ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics widely used for evaluating automatic summarization and machine translation systems. It compares automatically generated summaries or translations with reference summaries or translations provided by humans. ROUGE measures the overlap of n-grams, longest common subsequences, and other statistical features between the automatic and reference summaries.
"""

_KWARGS_DESCRIPTION = """
Calculates ROUGE scores for a list of hypotheses and references.
Args:
    predictions: List of predicted texts to score. Each predicted text should be a string.
    references: List of reference texts for each prediction. Each reference text should be a string.
    rouge_types: A list of ROUGE types to calculate. Valid names include "rouge1", "rouge2", "rougeL", and "rougeLsum".
    use_stemmer: Whether to use the Porter stemmer to strip word suffixes before calculating ROUGE.
    use_aggregator: Whether to return aggregated scores if set to True.
Returns:
    Dictionary containing ROUGE scores for each specified type. Each score is represented as a tuple (precision, recall, f1_score).
"""

class Rouge(datasets.Metric):
    def _info(self):
        return datasets.MetricInfo(
            description=_DESCRIPTION,
            inputs_description=_KWARGS_DESCRIPTION,
            citation="https://github.com/google-research/google-research/tree/master/rouge",
            features=datasets.Features(
                {
                    "predictions": datasets.Value("string", id="sequence"),
                    "references": datasets.Value("string", id="sequence"),
                }
            ),
            codebase_urls=["https://github.com/google-research/google-research/tree/master/rouge"],
            reference_urls=[
                "https://en.wikipedia.org/wiki/ROUGE_(metric)",
                "https://github.com/google-research/google-research/tree/master/rouge",
            ],
        )

    def _compute(self, predictions, references, rouge_types=None, use_aggregator=True, use_stemmer=False):
        if rouge_types is None:
            rouge_types = ["rouge1", "rouge2", "rougeL", "rougeLsum"]

        scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)
        if use_aggregator:
            aggregator = scoring.BootstrapAggregator()
        else:
            scores = []

        for ref, pred in zip(references, predictions):
            score = scorer.score(ref, pred)
            if use_aggregator:
                aggregator.add_scores(score)
            else:
                scores.append(score)

        if use_aggregator:
            result = aggregator.aggregate()
        else:
            result = {}
            for key in scores[0]:
                result[key] = [score[key] for score in scores]

        return result

# Load the dataset
dataset = datasets.load_dataset("uconcreative/slmDataset")

# Sample usage
rouge_metric = Rouge()

# Compute ROUGE scores for the dataset
predictions = dataset["train"]["Question"]
references = dataset["train"]["Answer"]
results = rouge_metric.compute(predictions=predictions, references=references)

# Print the ROUGE scores
print("ROUGE Scores:", results)

# Calculate error rates
error_rates = []
for rouge_type, rouge_scores in results.items():
    if rouge_type.startswith("rouge"):
        for score in rouge_scores:
            error_rate = 1 - score.fmeasure
            error_rates.append(error_rate)

# Calculate average error rate
average_error_rate = sum(error_rates) / len(error_rates)
print("Average Error Rate:", average_error_rate)

# Calculate the minimum average error rate to reach 20% improvement
target_average_error_rate = 0.20
desired_average_error_rate = max(target_average_error_rate, average_error_rate)
print("Desired Average Error Rate:", desired_average_error_rate)

# If the current error rate is already below the desired rate, print a message
if average_error_rate <= target_average_error_rate:
    print("Current average error rate is already below the desired rate.")
else:
    print("Model improvement needed to reach the desired average error rate.")