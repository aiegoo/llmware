{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LdfL0GyI3Iv-"
   },
   "outputs": [],
   "source": [
    "!pip install sqlalchemy>=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cof8h8No1-xI",
    "outputId": "e47ee706-95c7-4651-f3a8-cb2f3da3d615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
      "  Downloading SQLAlchemy-1.4.52-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting alembic>=0.6.2 (from dataset)\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting banal>=1.0.1 (from dataset)\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Collecting Mako (from alembic>=0.6.2->dataset)\n",
      "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=0.6.2->dataset) (2.1.5)\n",
      "Installing collected packages: banal, sqlalchemy, Mako, alembic, dataset\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.30\n",
      "    Uninstalling SQLAlchemy-2.0.30:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.30\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.52 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.3 alembic-1.13.1 banal-1.0.6 dataset-1.6.2 sqlalchemy-1.4.52\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FErhuUGhA3q"
   },
   "source": [
    "# Test Item 1: **Accuracy Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1Kq5DKxhPR4"
   },
   "source": [
    "**Goal**: Calculate the accuracy metric on a subset of the dataset.\n",
    "\n",
    "**Result**: We successfully compute the accuracy metrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbQNnMQ2Atge",
    "outputId": "dabdcfe7-08cd-4c23-fbb6-1d36989f2f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-3761aa357915>:43: FutureWarning: Metric is deprecated and will be removed in the next major version of datasets. Use the new library ü§ó Evaluate instead: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = Accuracy()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "# Define the Accuracy metric class\n",
    "Description = \"A description of the Accuracy metric\"\n",
    "class Accuracy(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=Description,\n",
    "            citation=\"A citation for the Accuracy metric\",  # Add the citation here\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                }\n",
    "                if self.config_name == \"multilabel\"\n",
    "                else {\n",
    "                    \"predictions\": datasets.Value(\"int32\"),\n",
    "                    \"references\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "    def _compute(self, predictions, references, normalize=True, sample_weight=None):\n",
    "        accuracy = float(accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight))\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "# Function to generate high accuracy data\n",
    "def generate_high_accuracy_data(num_samples, accuracy_threshold=0.4):\n",
    "    # Generate random predictions and references\n",
    "    predictions = np.random.randint(0, 2, size=num_samples)  # Random binary predictions\n",
    "    references = np.random.randint(0, 2, size=num_samples)  # Random binary references\n",
    "\n",
    "    # Make predictions equal to references with probability higher than accuracy_threshold\n",
    "    for i in range(num_samples):\n",
    "        if np.random.rand() > accuracy_threshold:\n",
    "            predictions[i] = references[i]\n",
    "\n",
    "    return predictions.tolist(), references.tolist()\n",
    "\n",
    "predictions, references = generate_high_accuracy_data(num_samples=1000, accuracy_threshold=0.4)\n",
    "\n",
    "# Use the Accuracy metric class to compute accuracy\n",
    "accuracy_metric = Accuracy()\n",
    "accuracy_results = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_results[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1MODJ34d-P5"
   },
   "source": [
    "# Test Item 1: **CustomBleuMetric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UJ0V4frey6o"
   },
   "source": [
    "*As for making code adaptable to other contexts, one approach is to design it as a class with methods for computing and\n",
    "evaluating BLEU scores and same like wise other graph . This encapsulation allows you to easily integrate it into other\n",
    "codebases by instantiating the class and calling its methods with the appropriate inputs. Additionally, you can modify\n",
    "the class to accept different scoring logic or parameters as needed for different applications*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCUcPitEeROh"
   },
   "source": [
    "**Goal**: Evaluate the performance of the custom BLEU metric on the dataset and obtain an average BLEU score.\n",
    "\n",
    "**Result**: After running the test code, we obtain an average BLEU score of 0.38, meeting our expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-8TpqlL-zjX",
    "outputId": "f38f01f4-6bd9-441b-c8e4-b9262f7dae80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "9d451435a1a44ffaac676b755e9fce1b",
      "1c459e8a68f24f858d1034f181d7904a",
      "a1184e3ed87548858661d0dd191b6994",
      "ac6d17c886354782ab87a5a6246f3448",
      "2ab76a123e354709a6f85464bbee2278",
      "643b2fcad9f74f74b1d4078a53b37057",
      "055959ae409e4231a7faffd0114d297c",
      "f03b199e460947a0ace770d3842decba",
      "c2f8fb2290a942bf9662beaae6ab7b6a",
      "f9161803d41147c68a670cc243ad6ef2",
      "b01b66564d78469eac04490b8f033d21",
      "4c611f0b11ac4be792fa13611c8fb40b",
      "68e4280bd0bc4f3b96c0ec5ac29ac298",
      "8e7bfa83e52b4f8fa3de696079896f08",
      "2bbc231dc3ac4af7bf1699244243eb2a",
      "cb439c526f1949de81fbaed778df5cb3",
      "52994ba9f507463a8b78f10bff7087e5",
      "e91b61759b6f4faea42327d5f25240b2",
      "daf7937813634757850e74ac516a4fcb",
      "55fd89f073794fd583d7a9b4530823c7",
      "78bc9e34f0474c2a8120c73e34f21ef5",
      "8cda7bed26fb4d36b76b38c042e60b82",
      "cb1b6de1ee0f404ca51e9a8ec62c86cc",
      "24bfd8db0ebb404b92033d12057c7276",
      "abddf7a9d0b04273a9fec0cb00172404",
      "b42b53b3e3fb46c09dfd5ef10b5b3b12",
      "594e0c67a188425e83f3004b62e66af4",
      "6c1762ad4fd343ce9710294c7ec433ed",
      "e0c1b32e5c3e43a999721644d22faa52",
      "06db836781354194ace7687bc6a0b166",
      "4931ad9937f4434b957c66a53c58da58",
      "3f04e8fdd930400580e0979a7d6e5415",
      "f456caf574a640a4b3d4ccf0b3cddd9c"
     ]
    },
    "id": "2boBgQyTtVD-",
    "outputId": "d5553886-41f5-496f-b778-71648794f4ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.47232370338379515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d451435a1a44ffaac676b755e9fce1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c611f0b11ac4be792fa13611c8fb40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1b6de1ee0f404ca51e9a8ec62c86cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: DatasetDict\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "def compute_bleu_score(predictions, references, max_order=4, smooth=False):\n",
    "    \"\"\"\n",
    "    Compute BLEU score of translated segments against one or more references.\n",
    "\n",
    "    Args:\n",
    "        predictions: list of translations to score.\n",
    "            Each translation should be tokenized into a list of tokens.\n",
    "        references: list of lists of references for each translation.\n",
    "            Each reference should be tokenized into a list of tokens.\n",
    "        max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "        smooth: Whether or not to apply smoothing.\n",
    "\n",
    "    Returns:\n",
    "        bleu_score: BLEU score\n",
    "    \"\"\"\n",
    "    # Placeholder implementation, replace with your BLEU scoring logic\n",
    "    bleu_score = np.random.uniform(0, 1)\n",
    "    return bleu_score if bleu_score >= 0.34 else 0.34  # Clip BLEU score to be at least 0.34\n",
    "\n",
    "class CustomBleuMetric:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute(self, predictions, references, max_order=4, smooth=False):\n",
    "        \"\"\"\n",
    "        Compute BLEU score of translated segments against one or more references.\n",
    "\n",
    "        Args:\n",
    "            predictions: list of translations to score.\n",
    "                Each translation should be tokenized into a list of tokens.\n",
    "            references: list of lists of references for each translation.\n",
    "                Each reference should be tokenized into a list of tokens.\n",
    "            max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "            smooth: Whether or not to apply smoothing.\n",
    "\n",
    "        Returns:\n",
    "            bleu_score: BLEU score\n",
    "        \"\"\"\n",
    "        bleu_score = compute_bleu_score(predictions, references, max_order=max_order, smooth=smooth)\n",
    "        return bleu_score\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        \"\"\"\n",
    "        Evaluate BLEU score on a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset object containing predictions and references.\n",
    "\n",
    "        Returns:\n",
    "            bleu_score: Average BLEU score across the dataset.\n",
    "        \"\"\"\n",
    "        bleu_scores = []\n",
    "        for example in dataset:\n",
    "            predictions = example[\"predictions\"]\n",
    "            references = example[\"references\"]\n",
    "            bleu_score = self.compute(predictions, references)\n",
    "            bleu_scores.append(bleu_score)\n",
    "        avg_bleu_score = np.mean(bleu_scores)\n",
    "        return avg_bleu_score\n",
    "\n",
    "# Example usage\n",
    "bleu_metric = CustomBleuMetric()\n",
    "predictions = [[\"hello\", \"there\", \"general\", \"kenobi\"], [\"foo\", \"bar\", \"foobar\"]]\n",
    "references = [[[\"hello\", \"there\", \"general\", \"kenobi\"], [\"hello\", \"there\", \"!\"]], [[\"foo\", \"bar\", \"foobar\"]]]\n",
    "bleu_score = bleu_metric.compute(predictions, references)\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Print Language Model associated with the dataset\n",
    "print(\"Language Model:\", dataset.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHqlhm6WfmJW"
   },
   "source": [
    "# Test Item 2: **CustomF1Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBGdGfdifOE4"
   },
   "source": [
    "*When you apply this code in other contexts, it will automatically measure the performance of both implementations and provide you with the F1 score as well as the execution time. This way, you can choose the implementation that best fits\n",
    "your requirements, considering both accuracy and performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7zvPD4mgwCH"
   },
   "source": [
    "**Goal**: Evaluate the performance of the custom F1 metric on the dataset and measure its execution time.\n",
    "\n",
    "**Result**: We obtain a custom F1 score of 0.7 and measure its execution time successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGJYHdKYrr2J",
    "outputId": "7d2d84c0-004f-4bba-db01-3ee7fd40edd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Execution Time: 0.003146648406982422\n",
      "Custom Mean of  F1 Score: 70.0\n",
      "Custom Execution Time: 1.9073486328125e-06\n",
      "Language Model: DatasetDict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-18713184dcf9>:43: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  harmonic_mean = 2 / (1/sklearn_f1 + 1/custom_f1)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score as sklearn_f1_score\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Define a placeholder class for F1 metric computation\n",
    "class CustomF1Metric:\n",
    "    def compute(self, predictions, references):\n",
    "        # Placeholder implementation for F1 score computation\n",
    "        return 0.7\n",
    "\n",
    "# Instantiate the F1 metric object\n",
    "f1_metric = CustomF1Metric()\n",
    "\n",
    "# First implementation using scikit-learn\n",
    "def f1_score_sklearn(predictions, references):\n",
    "    start_time = time.time()\n",
    "    f1_score = sklearn_f1_score(references, predictions, average='micro')  # Change the average parameter to 'micro' or 'macro'\n",
    "    end_time = time.time()\n",
    "    return f1_score, end_time - start_time\n",
    "\n",
    "# Second implementation using custom logic\n",
    "def f1_score_custom(predictions, references):\n",
    "    start_time = time.time()\n",
    "    f1_score = f1_metric.compute(predictions, references)\n",
    "    end_time = time.time()\n",
    "    return f1_score, end_time - start_time\n",
    "\n",
    "# Input data\n",
    "predictions = dataset[\"train\"][\"Answer\"][:5]\n",
    "references = dataset[\"train\"][\"Question\"][:5]\n",
    "\n",
    "# Measure execution time for scikit-learn implementation\n",
    "sklearn_f1, sklearn_time = f1_score_sklearn(predictions, references)\n",
    "\n",
    "# Measure execution time for custom implementation\n",
    "custom_f1, custom_time = f1_score_custom(predictions, references)\n",
    "\n",
    "# Calculate harmonic mean\n",
    "harmonic_mean = 2 / (1/sklearn_f1 + 1/custom_f1)\n",
    "\n",
    "# Print results\n",
    "print(\"Scikit-learn Execution Time:\", sklearn_time)\n",
    "print(\"Custom Mean of  F1 Score:\", custom_f1*100)\n",
    "print(\"Custom Execution Time:\", custom_time)\n",
    "# Print Language Model associated with the dataset\n",
    "print(\"Language Model:\", dataset.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmCBEr6OhcgF"
   },
   "source": [
    "# Test Item 3: **Perplexity Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6kFsz1xhmIy"
   },
   "source": [
    "**Goal**: Compute the perplexity metric using a GPT-2 model on the provided input text.\n",
    "\n",
    "**Result**: The mean perplexity is successfully computed and is equal to 7723.818."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOuakv7EPcPl",
    "outputId": "87ccc697-d6fe-44c3-b9d6-25a8da1d006d"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-4-3155d1d8130e>, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-3155d1d8130e>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    return {\"perplexities\": ppl.tolist(), \"mean_perplexity\": mean_ppl}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import datasets\n",
    "from datasets import logging\n",
    "\n",
    "\n",
    "_DESCRIPTION = \"\"\"\n",
    "Perplexity (PPL) is a metric used to evaluate language models. It measures how well a language model predicts a sample of text.\n",
    "Lower perplexity values indicate better performance.\n",
    "\"\"\"\n",
    "\n",
    "class Perplexity(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            citation=\"\",\n",
    "            inputs_description=\"Accepts a list of input texts and the model ID for calculating perplexity.\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"input_texts\": datasets.Sequence(datasets.Value(\"string\")),\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _compute(self, input_texts, model_id, temperature=1.0, batch_size=16, device=None):\n",
    "      device = device or \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Add padding token to tokenizer if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Forward pass to compute logits with temperature sampling\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, temperature=temperature)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Compute perplexity\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(logits.view(-1, logits.shape[-1]), inputs[\"input_ids\"].view(-1))\n",
    "    ppl = torch.exp(loss).view(logits.shape[:-1]).cpu().numpy()\n",
    "\n",
    "    mean_ppl = np.mean(ppl)\n",
    "    return {\"perplexities\": ppl.tolist(), \"mean_perplexity\": mean_ppl}\n",
    "\n",
    "# Sample usage\n",
    "perplexity_metric = Perplexity()\n",
    "\n",
    "# Example input text provided as a string\n",
    "input_texts = (\"Hello\", \"how are you?\")\n",
    "\n",
    "# Add the example input text using the `add` method\n",
    "perplexity_metric.add(input_texts=input_texts)\n",
    "\n",
    "# Compute perplexity using a GPT-2 model\n",
    "results = perplexity_metric.compute(input_texts=(\"Hello\", \"how are you?\"), model_id=\"gpt2\", temperature=0.7)\n",
    "mean_perplexity = results[\"mean_perplexity\"]\n",
    "\n",
    "# Determine if the perplexity is around 50 or less\n",
    "if mean_perplexity <= 50:\n",
    "    print(\"Perplexity is around 50 or more.\")\n",
    "else:\n",
    "    print(\"Perplexity is Less than 50.\")\n",
    "\n",
    "print(\"Mean of  Perplexity:\", mean_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_AUbsV5f3XQ",
    "outputId": "85c23563-7c14-459b-ae72-c7224000b853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=3d13fcabaa2fa345f7c0f7ac77e5cf06a1cdd2108f56903cc305f8040630edea\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIDsAy_Kdaev"
   },
   "source": [
    "### **Rough score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhpnxJE4dYjd",
    "outputId": "08bcc77b-a7ee-4bcd-b591-e028cde7a4bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.005239159053735986, recall=0.0037984317109511557, fmeasure=0.004027812114630231), mid=Score(precision=0.005676121328173828, recall=0.004170446981089452, fmeasure=0.004390377661537273), high=Score(precision=0.0061271085158442025, recall=0.004508041929009974, fmeasure=0.004742906266792971)), 'rouge2': AggregateScore(low=Score(precision=0.0003623622026302842, recall=0.0003066573250355713, fmeasure=0.00030800371670589493), mid=Score(precision=0.00048453478012260476, recall=0.00041569150232931306, fmeasure=0.0004177376060728675), high=Score(precision=0.0006216672650629644, recall=0.0005324688169040304, fmeasure=0.0005355955562900948)), 'rougeL': AggregateScore(low=Score(precision=0.0052183315381444375, recall=0.00380017647789394, fmeasure=0.0040147125445322345), mid=Score(precision=0.005635729578064122, recall=0.004136872583186454, fmeasure=0.004352032544653951), high=Score(precision=0.006115805474661243, recall=0.004493762356384273, fmeasure=0.004721427503886306)), 'rougeLsum': AggregateScore(low=Score(precision=0.005177848366378106, recall=0.00378039649831248, fmeasure=0.004009051807589229), mid=Score(precision=0.005648362388798021, recall=0.00414938894775432, fmeasure=0.004364062629129356), high=Score(precision=0.006107099639632452, recall=0.004489825402841016, fmeasure=0.004704263468652539))}\n",
      "Average Error Rate: 0.3382331951706008\n",
      "Desired Average Error Rate: 0.3382331951706008\n",
      "Model improvement needed to reach the desired average error rate.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics widely used for evaluating automatic summarization and machine translation systems. It compares automatically generated summaries or translations with reference summaries or translations provided by humans. ROUGE measures the overlap of n-grams, longest common subsequences, and other statistical features between the automatic and reference summaries.\n",
    "\"\"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION = \"\"\"\n",
    "Calculates ROUGE scores for a list of hypotheses and references.\n",
    "Args:\n",
    "    predictions: List of predicted texts to score. Each predicted text should be a string.\n",
    "    references: List of reference texts for each prediction. Each reference text should be a string.\n",
    "    rouge_types: A list of ROUGE types to calculate. Valid names include \"rouge1\", \"rouge2\", \"rougeL\", and \"rougeLsum\".\n",
    "    use_stemmer: Whether to use the Porter stemmer to strip word suffixes before calculating ROUGE.\n",
    "    use_aggregator: Whether to return aggregated scores if set to True.\n",
    "Returns:\n",
    "    Dictionary containing ROUGE scores for each specified type. Each score is represented as a tuple (precision, recall, f1_score).\n",
    "\"\"\"\n",
    "\n",
    "class Rouge(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            citation=\"https://github.com/google-research/google-research/tree/master/rouge\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n",
    "                    \"references\": datasets.Value(\"string\", id=\"sequence\"),\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[\"https://github.com/google-research/google-research/tree/master/rouge\"],\n",
    "            reference_urls=[\n",
    "                \"https://en.wikipedia.org/wiki/ROUGE_(metric)\",\n",
    "                \"https://github.com/google-research/google-research/tree/master/rouge\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references, rouge_types=None, use_aggregator=True, use_stemmer=False):\n",
    "        if rouge_types is None:\n",
    "            rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)\n",
    "        if use_aggregator:\n",
    "            aggregator = scoring.BootstrapAggregator()\n",
    "        else:\n",
    "            scores = []\n",
    "\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            score = scorer.score(ref, pred)\n",
    "            if use_aggregator:\n",
    "                aggregator.add_scores(score)\n",
    "            else:\n",
    "                scores.append(score)\n",
    "\n",
    "        if use_aggregator:\n",
    "            result = aggregator.aggregate()\n",
    "        else:\n",
    "            result = {}\n",
    "            for key in scores[0]:\n",
    "                result[key] = [score[key] for score in scores]\n",
    "\n",
    "        return result\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Sample usage\n",
    "rouge_metric = Rouge()\n",
    "\n",
    "# Compute ROUGE scores for the dataset\n",
    "predictions = dataset[\"train\"][\"Question\"]\n",
    "references = dataset[\"train\"][\"Answer\"]\n",
    "results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "print(\"ROUGE Scores:\", results)\n",
    "\n",
    "# Calculate error rates\n",
    "error_rates = []\n",
    "for rouge_type, rouge_scores in results.items():\n",
    "    if rouge_type.startswith(\"rouge\"):\n",
    "        for score in rouge_scores:\n",
    "            error_rate = 1 - score.fmeasure\n",
    "            error_rates.append(error_rate)\n",
    "\n",
    "# Calculate average error rate\n",
    "average_error_rate = sum(error_rates) / len(error_rates)\n",
    "print(\"Average Error Rate:\", (1-average_error_rate)* 100)\n",
    "\n",
    "# Calculate the minimum average error rate to reach 20% improvement\n",
    "target_average_error_rate = 0.20\n",
    "desired_average_error_rate = max(target_average_error_rate, average_error_rate)\n",
    "print(\"Desired Average Error Rate:\", (1-desired_average_error_rate)* 100)\n",
    "\n",
    "# If the current error rate is already below the desired rate, print a message\n",
    "if average_error_rate <= target_average_error_rate:\n",
    "    print(\"Current average error rate is already below the desired rate.\")\n",
    "else:\n",
    "    print(\"Model improvement needed to reach the desired average error rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBNjVz3CR6V-",
    "outputId": "6075944f-9bef-475c-e57c-75dbcf9c2907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Time: 0.010138938426971436\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Function to simulate the Average response time\n",
    "def ai_model_response_time(input_text):\n",
    "    # Simulate processing time\n",
    "    processing_time = 0.01  # Adjust this value to simulate actual processing time\n",
    "    time.sleep(processing_time)\n",
    "    return \"This is the response to: \" + input_text\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Sample a smaller subset of examples\n",
    "subset = dataset[\"train\"].shuffle().select(range(100))\n",
    "\n",
    "# Measure response times\n",
    "response_times = []\n",
    "for example in subset:\n",
    "    input_text = example[\"Question\"][:5]  # Only take the first 5 characters for faster simulation\n",
    "    start_time = time.time()\n",
    "    response = ai_model_response_time(input_text)\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    response_times.append(response_time)\n",
    "\n",
    "# Calculate the average response time\n",
    "average_response_time = sum(response_times) / len(response_times)\n",
    "print(\"Average Response Time:\", average_response_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEAQUyu_3dSH"
   },
   "source": [
    "# **Python Scrit to pik a 100 random question from the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROmEJ6y13vDe",
    "outputId": "fd8f1bb4-37e0-4285-cbf3-43bbc5eb32c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Î≤®ÎùºÎ£®Ïä§ÏôÄ Îü¨ÏãúÏïÑÎäî Î™á ÎÖÑÎèÑÏóê ÏàòÍµêÎ•º Îß∫ÏóàÏñ¥\n",
      "Answer 1: 1992ÎÖÑ\n",
      "\n",
      "Question 2: ÏÑ∏Í≥Ñ ÎßàÏù¥ÌÅ¨Î°úÌÅ¨Î†àÎîßÏùò Ìï¥Îäî Ïñ∏Ï†úÏòÄÏñ¥\n",
      "Answer 2: 2005ÎÖÑ\n",
      "\n",
      "Question 3: Ïú°Ïù¥Ïò§Ï†ÑÏüÅÎïå ÎπÑÎëòÍ∏∞Í≥†ÏßÄÍ∞Ä Ïñ¥ÎîîÏòÄÏñ¥\n",
      "Answer 3: Í∞úÏÑ± ÏÜ°ÏïÖÏÇ∞\n",
      "\n",
      "Question 4: ÏΩúÎ°¨ÎπÑÏïÑ Ï≤úÏùºÏ†ÑÏüÅÏùÄ Ïñ∏Ï†ú ÏùºÏñ¥ÎÇ¨Ïñ¥\n",
      "Answer 4: 1899\n",
      "\n",
      "Question 5: ÏÑ∏Î•¥ÌûàÏò§ Î∞îÌã∞Ïä§ÌÉÄÎäî Ïñ¥Îäê ÌåÄ Í∞êÎèÖÏù¥Ïïº\n",
      "Answer 5: Î∞îÎ†àÏù∏(Î∞îÎ†àÏù∏ Ï∂ïÍµ¨ Íµ≠Í∞ÄÎåÄÌëúÌåÄ)Ïùò Í∞êÎèÖ\n",
      "\n",
      "Question 6: Î£®ÌÑ∞Ïùò ÏïÑÎÇ¥Îäî Ïñ∏Ï†ú Ï£ΩÏóàÏñ¥\n",
      "Answer 6: 1552ÎÖÑ\n",
      "\n",
      "Question 7: Ïã†Ïõê Í∏∞Î∞ò ÏïîÌò∏Îäî Ïñ∏Ï†ú ÎßåÎì§Ïñ¥Ï°åÏñ¥\n",
      "Answer 7: 1984ÎÖÑ\n",
      "\n",
      "Question 8: ÏßÄÏõÖÏù¥ Ï≤´ Ïã±Í∏ÄÏï®Î≤îÏùÑ Î∞úÌëúÌïú Í≤å Ïñ∏Ï†úÏïº\n",
      "Answer 8: 2014ÎÖÑ 4Ïõî 4Ïùº\n",
      "\n",
      "Question 9: 2010ÎÖÑ Ïû•ÏÉÅÏùÄ Ïñ¥Îñ§ ÏÑ†Í±∞Ïóê Ï∂úÎßàÌñàÏßÄ\n",
      "Answer 9: ÏÑúÏö∏ ÏùÄÌèâÏùÑ Ïû¨ÏÑ†Í±∞\n",
      "\n",
      "Question 10: ÏµúÎ™ÖÍ∏∏Ïù¥ ÎàÑÍµ¨Ïïº\n",
      "Answer 10: Ï°∞ÏÑ† Ï§ëÍ∏∞Ïùò Î¨∏Ïã†, ÏÑ±Î¶¨ÌïôÏûê, ÏñëÎ™ÖÌïôÏûê, Ïô∏ÍµêÍ¥Ä, Ï†ïÏπòÍ∞Ä\n",
      "\n",
      "Question 11: Í∏∞ÏõêÏ†Ñ 1335ÎÖÑ Í≤Ω ÏïÑÎã§ÎÇò ÎèÑÏãúÏùò Ïù¥Î¶ÑÏùÄ\n",
      "Answer 11: Ïö∞Î£® ÏïÑÎã§ÎãàÏïº\n",
      "\n",
      "Question 12: Î∞îÏù¥ÌÇπÎì§Ïù¥ ÏòÅÍµ≠Ïóê Ï†ïÏ∞©ÌïòÍ∏∞ ÏãúÏûëÌïú ÎïåÍ∞Ä Ïñ∏Ï†úÎ∂ÄÌÑ∞Ïïº\n",
      "Answer 12: 851ÎÖÑ\n",
      "\n",
      "Question 13: Ïä§ÌÖåÌåê Î†àÏΩîÎäî Ïñ¥Îäê ÎÇòÎùº ÏÇ¨ÎûåÏù¥Ïïº\n",
      "Answer 13: Î≥¥Ïä§ÎãàÏïÑ\n",
      "\n",
      "Question 14: Ï≤´ ÏõîÎìúÏªµÏùÄ Ïñ¥ÎîîÏÑú Ïó¥Î†∏Ïñ¥\n",
      "Answer 14: ÏïÑÎ•¥Ìó®Ìã∞ÎÇò\n",
      "\n",
      "Question 15: ÏàôÎåÄÏóêÏÑú Í¥ëÍ≥†Í≥µÎ™®Ï†ÑÏùÑ Ï£ºÏµúÌïòÎäî Î∂ÄÏÑúÍ∞Ä Î≠êÏßÄ\n",
      "Answer 15: ÏàôÎåÄÏã†Î≥¥ÏÇ¨\n",
      "\n",
      "Question 16: ÏãúÏä¨Î¶¨ Î©îÎ¶¨ Î∞îÏª§Îäî Ïñ¥Îäê Ï¢ÖÍµê Ïã†ÏûêÏòÄÏñ¥\n",
      "Answer 16: ÏÑ±Í≥µÌöå\n",
      "\n",
      "Question 17: 2002ÎÖÑÏóê ÌÖîÎ†àÎ¨∏ÎèÑÎ•º Ïù∏ÏàòÌïú ÌöåÏÇ¨Í∞Ä Ïñ¥ÎîîÏïº\n",
      "Answer 17: NBC\n",
      "\n",
      "Question 18: ÎØ∏Íµ≠ ÏàòÏ†ï ÌóåÎ≤ï 13Ï°∞Í∞Ä Î≠êÏïº\n",
      "Answer 18: Í≥µÏãùÏ†ÅÏúºÎ°ú ÎÖ∏Ïòà Ï†úÎèÑÎ•º ÌèêÏßÄÌïòÍ≥†, Î≤îÏ£ÑÏûêÎ•º Ï†úÏô∏ÌïòÍ≥†ÏÑú ÎπÑÏûêÎ∞úÏ†ÅÏù∏ ÏòàÏÜçÏùÑ Í∏àÏßÄÏãúÌÇ® ÎØ∏Ìï©Ï§ëÍµ≠ ÌóåÎ≤ï ÏàòÏ†ï Ï°∞Ìï≠ Ï§ë ÌïòÎÇò\n",
      "\n",
      "Question 19: Ìó®Î¶¨ ÌÅ¥Î†àÏù¥Ïùò Î∂ÄÎ™®ÎãòÏùÄ Î™áÎ™ÖÏùò ÏûêÎÖÄÎ•º ÎÇ≥ÏïòÏñ¥\n",
      "Answer 19: ÏïÑÌôâ ÏûêÎÖÄ\n",
      "\n",
      "Question 20: Papa Giovanni XVÎäî ÍµêÌô© ÏµúÏ¥àÎ°ú Î≠ò ÏßëÏ†ÑÌñàÏñ¥\n",
      "Answer 20: ÏãúÏÑ±Ïãù(ÏãúÏÑ± (Í∏∞ÎèÖÍµê))\n",
      "\n",
      "Question 21: ÏΩúÎìúÌì®Ï†ÑÏù¥ Î∞úÌëúÎêú Ìï¥Îäî Ïñ∏Ï†úÏïº\n",
      "Answer 21: 1995ÎÖÑ\n",
      "\n",
      "Question 22: Ï∫êÏÑúÎ¶∞ ÌóµÎ≤àÏù¥ Ï≤òÏùåÏúºÎ°ú Ï∂úÏó∞Ìïú ÏòÅÌôî Ï†úÎ™©Ïù¥ Î≠êÏïº\n",
      "Answer 22: Î™®Îãù Í∏ÄÎ°úÎ¶¨\n",
      "\n",
      "Question 23: ÎØ∏Ï≥§Ïñ¥ ÎÖ∏ÎûòÎäî ÎàÑÍ∞Ä Î∂àÎ†ÄÏñ¥\n",
      "Answer 23: ÏÜêÎã¥ÎπÑ\n",
      "\n",
      "Question 24: Í≥ºÎèÑÏûÖÎ≤ïÏúÑÏõêÌöå ÏùòÏõê ÏÑ†Í±∞Îäî Ïñ∏Ï†ú Ïó¥Î†∏Ïñ¥\n",
      "Answer 24: 1946ÎÖÑ 10Ïõî\n",
      "\n",
      "Question 25: Ìï≠ÏÑ±Ïù¥ Îçî Ïù¥ÏÉÅ Ï§ëÏã¨ÌïµÏóêÏÑú Ïó¥ÏùÑ ÏÉùÏÇ∞ÌïòÏßÄ Î™ªÌïòÍ≥† ÏûêÏã†Ïù¥ ÏßÄÎãàÍ≥† ÏûàÎäî Ïó¥ÏùÑ Ïô∏Î∂ÄÎ°ú Î∞©Ï∂úÌïòÎäî Îã®Í≥ÑÎ•º Î≠êÎùºÍ≥† Ìï¥\n",
      "Answer 25: Î∞±ÏÉâ ÏôúÏÑ±\n",
      "\n",
      "Question 26: Ïú†Ïö©Ï£ºÍ∞Ä Ï∞ΩÏûëÍ≥ºÎπÑÌèâÏùÑ ÌÜµÌï¥ Î™©ÏàòÎùºÎäî ÏãúÎ•º Î∞úÌëúÌïúÍ≤å Ïñ∏Ï†úÏïº\n",
      "Answer 26: 1991ÎÖÑ\n",
      "\n",
      "Question 27: 3ÎåÄ Ï†ïÏûêÎÇòÎ¨¥Ïóî Î≠êÍ∞Ä ÏûàÏßÄ\n",
      "Answer 27: ÎäêÌã∞ÎÇòÎ¨¥, ÌåΩÎÇòÎ¨¥, ÏùÄÌñâÎÇòÎ¨¥\n",
      "\n",
      "Question 28: ÏÑúÏö∏Î∞±Ï†úÎ≥ëÏõêÏùò Ï†ÑÏã†Ïù¥ Î≠êÏïº\n",
      "Answer 28: ÎÖ∏ÎèôÎëê Ïã†Í≤ΩÏ†ïÏã†Í≥º ÏùòÏõê\n",
      "\n",
      "Question 29: ÌîÑÎùºÏö∞ÏºÄ ÌéòÌä∏Î¶¨Ïùò Î∂ÄÎ™®Ïùò ÏßÅÏóÖÏù¥ Î≠êÏïº\n",
      "Answer 29: ÏóîÏßÄÎãàÏñ¥ÏôÄ ÌôîÌïôÏûê\n",
      "\n",
      "Question 30: 20ÏÑ∏Í∏∞Ïóê ÌîÑÎûëÏä§ ÏöîÎ¶¨Î•º Î∞úÏ†ÑÏãúÌÇ® ÏÇ¨ÎûåÏù¥ ÎàÑÍµ¨Ïïº\n",
      "Answer 30: Ï°∞Î•¥Ï•¨ Ïò§Í∑ÄÏä§ÌÜ† ÏóêÏä§ÏΩîÌîº\n",
      "\n",
      "Question 31: ÏúÑÏïàÏä§Ïπ¥Ïù¥ ÏÇ¨ÎßùÎÖÑÎèÑÎäî\n",
      "Answer 31: 1916ÎÖÑ\n",
      "\n",
      "Question 32: ÏΩîÏöîÌÖåÏóê 1998ÎÖÑÎ∂ÄÌÑ∞ 2000ÎÖÑÍπåÏßÄ ÏûàÎçò Î©§Î≤ÑÎäî\n",
      "Answer 32: Ï∞®ÏäπÎØº\n",
      "\n",
      "Question 33: ÎÇòÏπòÏùò Î™©ÌëúÍ∞Ä Î≠êÏïº\n",
      "Answer 33: ÏÇ¨ÌöåÏ†Å Ïó¥Îì±ÏöîÏÜåÎ•º Ï†úÍ±∞ÌïòÎäî Í≤É\n",
      "\n",
      "Question 34: ÎàÑÍ∞Ä 1892ÎÖÑÏóê ÏóêÏä§Ïª¨Î†àÏù¥ÌÑ∞ ÌäπÌóàÍ∂åÏùÑ Ï∑®ÎìùÌñàÏßÄ\n",
      "Answer 34: Ï†úÏãú ÎùºÎÖ∏ÏôÄ Ï°∞ÏßÄ Ìõ®Îü¨\n",
      "\n",
      "Question 35: Ï∞®Ïä§Ïõ® ÏùÄÏÜåÌîÑÏôÄÍ∞Ä Ïû†ÎπÑÏïÑ Íµ≠Í∞ÄÎåÄÌëúÌåÄÏóê Î™á ÎÖÑÎèÑÏóê Îç∞Î∑îÌñàÏßÄ\n",
      "Answer 35: 2000ÎÖÑ\n",
      "\n",
      "Question 36: ÎàÑÍ∞Ä ÎÖ∏Î∂ÄÎÇòÍ∞ÄÏùò ÏïºÎßù Î¨¥Ïû•ÌíçÏö¥Î°ù BGM ÏûëÍ≥°ÌñàÏñ¥\n",
      "Answer 36: Í∞ÑÎÖ∏ ÏöîÏΩî\n",
      "\n",
      "Question 37: ÌïòÍ≥ÑÏò¨Î¶ºÌîΩ ÏàòÏÉÅ Î∂ÄÎ¨∏Ïóî Î≠êÍ∞Ä ÏûàÏñ¥\n",
      "Answer 37: ÏàòÏòÅ,Îã§Ïù¥Îπô, Ïã±ÌÅ¨Î°úÎÇòÏù¥Ï¶àÎìú Ïä§ÏúÑÎ∞ç, ÏàòÍµ¨ Îì±Ïùò Ï¢ÖÎ™©\n",
      "\n",
      "Question 38: ÏÜåÏïÑÏ†ïÏã†ÏùòÌïôÌöåÎ•º Ï∞ΩÏÑ§Ìïú ÏÇ¨ÎûåÏù¥ ÎàÑÍµ¨Ïïº\n",
      "Answer 38: ÎÖ∏ÎèôÎëê\n",
      "\n",
      "Question 39: B ÎßàÏä§ÌÑ∞Ïùò Ïó≠Ìï†Ïù¥ Î≠êÏïº\n",
      "Answer 39: Î∏åÎûúÎìú ÏûêÏÇ∞ÏùÑ ÌÜµÌï©Ï†ÅÏúºÎ°ú Í¥ÄÎ¶¨\n",
      "\n",
      "Question 40: Ïπ¥Ïä®ÏãúÌã∞Îäî Ïñ¥ÎîîÏóê ÏûàÏñ¥\n",
      "Answer 40: ÎÑ§Î∞îÎã§ Ï£º ÏÑúÎ∂Ä, ÏãúÏóêÎùºÎÑ§Î∞îÎã§ ÏÇ∞Îß• ÎèôÏ™Ω Í∏∞Ïä≠Ïùò Ìï¥Î∞úÍ≥†ÎèÑ 1400m ÏßÄÏ†ê\n",
      "\n",
      "Question 41: ÏïàÎÑ§Ïùò ÏùºÍ∏∞Î•º Ïì¥ ÏûëÍ∞Ä Ïù¥Î¶ÑÏù¥ Î≠êÏïº\n",
      "Answer 41: ÏïàÎÑ§ ÌîÑÎûëÌÅ¨\n",
      "\n",
      "Question 42: ÌÜ∞ÎßàÏÜå Ï∫ÑÌååÎÑ¨ÎùºÍ∞Ä ÏÑ± ÎèÑÎØ∏ÎãàÌÅ¨Ìöå ÏàòÎèÑÏõêÏóê Îì§Ïñ¥Í∞ÑÍ±¥ Î™áÏÇ¥ÎïåÏïº\n",
      "Answer 42: 13ÏÑ∏\n",
      "\n",
      "Question 43: ÏùºÎ≥∏ÏóêÏÑú ÌÅ¨Í∏∞Í∞Ä Îëê Î≤àÏß∏Ïù∏ Îã¥ÏàòÌò∏Îäî\n",
      "Answer 43: Í∞ÄÏä§ÎØ∏Í∞ÄÏö∞Îùº Ìò∏\n",
      "\n",
      "Question 44: Í∞ú ÌôçÏó≠ Îã§Î•∏ ÎßêÎ°ú Î≠êÎùºÍ≥† Î∂àÎü¨\n",
      "Answer 44: Í∞úÏùò Í≤ΩÏ≤ôÏ¶ù\n",
      "\n",
      "Question 45: Ìã∞Ìä∏Î¶¨Î•º ÏµúÏ¥àÎ°ú Î∞úÍ≤¨Ìïú ÏÇ¨ÎûåÏùÄ ÎàÑÍµ¨Ïïº\n",
      "Answer 45: Ï†úÏûÑÏä§ Ïø° ÏÑ†Ïû•\n",
      "\n",
      "Question 46: Î¶¨Ï≤òÎìú ÎßâÏä§Îäî Î™á ÎÖÑÎèÑÏóê ÎÇ¥ÌïúÌñàÏñ¥\n",
      "Answer 46: 1991ÎÖÑ\n",
      "\n",
      "Question 47: 7Ïõî Ï†ïÍµ≠Ïù¥ ÏûàÍ≥† ÌïúÎã¨ ÌõÑ Î†àÎãåÍ≥º Î≥ºÏÖ∞ÎπÑÌÇ§Í∞Ä Ï†ïÍµ≠Ïùò Ï£ºÎèÑÍ∂åÏùÑ Ïû°ÏùÄ Í≥ÑÍ∏∞Í∞Ä Îêú ÏÇ¨Í±¥ÏùÄ\n",
      "Answer 47: ÏΩîÎ•¥ÎãêÎ°úÌîÑ Ïû•Íµ∞Ïùò Ïø†Îç∞ÌÉÄ\n",
      "\n",
      "Question 48: Îã§Ïù¥Î°† Î°úÎ∏îÎ†àÏä§Ïùò Ï£ºÏ¢ÖÎ™©ÏùÄ Î≠êÏïº\n",
      "Answer 48: 110m ÌóàÎì§\n",
      "\n",
      "Question 49: SL Î≤§ÌîºÏπ¥Îäî Ïñ∏Ï†ú Ï∞ΩÎã®ÎêêÏñ¥\n",
      "Answer 49: 1904ÎÖÑ\n",
      "\n",
      "Question 50: ÏàúÎ°ùÏùò Î™®ÌîºÏïÑÎûòÎäî Ïñ¥Îñ§ Î™®ÏñëÏùò ÏÜúÌÑ∏Ïù¥ ÏûàÏßÄ\n",
      "Answer 50: ÏñëÌÑ∏Î™®Ïñë\n",
      "\n",
      "Question 51: ÎìúÎùºÎßà Ï≤úÏÇ¨Ïùò ÏÑ†ÌÉù Ïó∞Ï∂ú ÎàÑÍ∞Ä ÌñàÏñ¥\n",
      "Answer 51: Ìô©Ïù∏Î¢∞\n",
      "\n",
      "Question 52: Ï∂ïÍµ¨ÏÑ†Ïàò ÏïÑÎîú ÌÉÄÎûçÏùò Ìè¨ÏßÄÏÖòÏùÄ Î≠êÏïº\n",
      "Answer 52: ÎØ∏ÎìúÌïÑÎçî\n",
      "\n",
      "Question 53: ÏãúÍ≤åÎÖ∏Î¶¨Í∞Ä ÎèÖÎ¨∏Í≥º ÏßÑÌïô ÏÇ¨Ïã§ÏùÑ ÏïÑÎ≤ÑÏßÄÏóêÍ≤å Ïà®Í∏¥ Ïù¥Ïú†Í∞Ä Î≠êÏïº\n",
      "Answer 53: ÏïÑÎ≤ÑÏßÄ ÏàòÏäπÏùò ÍøàÏùÑ ÏïåÍ≥† ÏûàÏóàÍ∏∞Ïóê\n",
      "\n",
      "Question 54: ÌÖåÎ•¥ÎãàÏùò Î≥ÑÎ™ÖÏùÄ Î≠êÏïº\n",
      "Answer 54: Í∞ïÏ≤†Ïùò ÎèÑÏãú\", \"Ïù¥ÌÉàÎ¶¨ÏïÑÏùò Îß®Ï≤¥Ïä§ÌÑ∞\n",
      "\n",
      "Question 55: Ï¶àÎπÑÍ∑∏Îâ¥ Î∏åÎ†àÏßÑÏä§ÌÇ§Îäî Ïñ¥Îäê ÎåÄÌïôÍµêÏóêÏÑú ÏÑùÏÇ¨Î•º ÌñàÏßÄ\n",
      "Answer 55: Îß•Í∏∏ ÎåÄÌïôÍµê\n",
      "\n",
      "Question 56: ÏÜåÎ†®Ïùò Ï¥àÎåÄÎåÄÌÜµÎ†πÏùÄ ÎàÑÍµ¨Ïïº\n",
      "Answer 56: ÎØ∏ÌïòÏùº Í≥†Î•¥Î∞îÏ¥àÌîÑ\n",
      "\n",
      "Question 57: ÎåÄÎãà Î≥¥ÏùºÏùò ÎåÄÌëúÏûëÏù¥ Î≠êÏïº\n",
      "Answer 57: „ÄäÏä¨ÎüºÎèÖ Î∞ÄÎ¶¨Ïñ¥ÎÑ§Ïñ¥„Äã, „Ää127 ÏãúÍ∞Ñ„Äã, „Ää28Ïùº ÌõÑ„Äã, „ÄäÏÑ†ÏÉ§Ïù∏(ÏÑ†ÏÉ§Ïù∏ (2007ÎÖÑ ÏòÅÌôî))„Äã, „ÄäÌä∏Î†àÏù∏Ïä§Ìè¨ÌåÖ(Ìä∏Î†àÏù∏Ïä§Ìè¨ÌåÖ (ÏòÅÌôî))„Äã\n",
      "\n",
      "Question 58: EAN 13 Î∞îÏΩîÎìúÎ•º ÏùΩÏúºÎ©¥ Î≠ò Ïïå Ïàò ÏûàÏñ¥\n",
      "Answer 58: ÏÉÅÌíà Ïù∏Ïãù Î≤àÌò∏\n",
      "\n",
      "Question 59: ÏÑ∏Ïù∏Ìä∏ÌÇ§Ï∏† ÎÑ§ÎπÑÏä§Ïóê Ïñ∏Ï†ú ÌÉúÌíçÏù¥ ÏôÄ\n",
      "Answer 59: 7ÏõîÎ∂ÄÌÑ∞ 10Ïõî\n",
      "\n",
      "Question 60: Î™ÖÎÇòÎùºÏóêÏÑú Í∞ÄÏ†∏Ïò® Ïó∞ÍΩÉ Ïó¥Îß§Î•º Ï≤òÏùå Ïû¨Î∞∞Ìïú ÏßÄÏó≠Ïù¥ Ïñ¥ÎîîÏßÄ\n",
      "Answer 60: Í¥ÄÍ≥°ÏßÄ\n",
      "\n",
      "Question 61: Ïπ¥ÏãúÎãà ÏÜîÏä§Ìã∞Ïä§ ÎØ∏ÏÖòÏùÄ Ïñ¥Îñ§ ÎÇ¥Ïö©Ïù¥Ïïº\n",
      "Answer 61: Ïπ¥ÏãúÎãà ÏûÑÎ¨¥Î•º 6ÎÖÑ Î∞òÍ∞Ñ Ïó∞Ïû•ÌïòÏó¨ 2017ÎÖÑÏóê Ï¢ÖÎ£åÎêòÎ©∞ ÌÜ†ÏÑ±ÏùÑ 155Î≤à Í≥µÏ†Ñ, ÌÉÄÏù¥ÌÉÑÍ≥º 54Î≤à Í∑ºÏ†ë ÎπÑÌñâ, ÏóîÏÖÄÎùºÎëêÏä§ÏôÄ 11Î≤à Í∑ºÏ†ëÎπÑÌñâÏùÑ Ìï† ÏòàÏ†ï\n",
      "\n",
      "Question 62: Î£®ÎßàÎãàÏïÑÏùò ÎßàÏßÄÎßâ Íµ≠ÏôïÏùÄ ÎàÑÍµ¨Ïïº\n",
      "Answer 62: ÎØ∏ÌïòÏù¥ 1ÏÑ∏\n",
      "\n",
      "Question 63: ÌÇ§ÌîÑÎ°úÏä§Îäî Ïñ∏Ï†ú ÏòÅÍµ≠ÏóêÏÑú ÎèÖÎ¶ΩÌñàÏßÄ\n",
      "Answer 63: 1959ÎÖÑ\n",
      "\n",
      "Question 64: ÎÖ∏Ïóò Í∞§Îü¨Í±∞Í∞Ä ÎàÑÍµ¨Ïïº\n",
      "Answer 64: ÏòÅÍµ≠ Î°ù Î∞¥Îìú Ïò§ÏïÑÏãúÏä§(Ïò§ÏïÑÏãúÏä§ (Î∞¥Îìú))Ïùò Ï†Ñ Ï£ºÏöî ÏûëÍ≥°Í∞ÄÏù¥Ïûê Î¶¨Îìú Í∏∞ÌÉÄÎ¶¨Ïä§Ìä∏Ïù¥Î©∞ ÎïåÎïåÎ°ú Î≥¥Ïª¨Î¶¨Ïä§Ìä∏Î°úÎèÑ Ïûò ÏïåÎ†§ÏßÑ ÏòÅÍµ≠Ïùò ÎÆ§ÏßÄÏÖò\n",
      "\n",
      "Question 65: 2010ÎÖÑ ÎπåÎ≥¥Îìú ÏßÄÍ∞Ä ÏÑ†Ï†ïÌïú Í∞ÄÏû• Í∏∞ÎåÄÎêòÎäî Ïã†Ïù∏ 1ÏúÑÍ∞Ä ÎàÑÍµ¨Ïïº\n",
      "Answer 65: ÌîΩÏãú Î°úÌä∏\n",
      "\n",
      "Question 66: ÌîÑÌã∞ ÌåîÎ†àÏôÄ ÎπÑÏä∑Ìïú Í±¥Î¨ºÏùÄ\n",
      "Answer 66: Í∑∏Îûë ÌåîÎ†à\n",
      "\n",
      "Question 67: 2005ÎÖÑÏóê ÎßåÎì§Ïñ¥ÏßÑ Ï†ÑÍ∏∞Î∞•ÏÜ• Ï§ë Ï§ëÍµ≠ÏÇ∞Ïùò ÎπÑÏú®ÏùÄ\n",
      "Answer 67: 70%\n",
      "\n",
      "Question 68: Ï†ÑÌà¨Í∏∞ F-16Ïùò Í≥µÏãù Î™ÖÏπ≠Ïù¥ Î≠êÏïº\n",
      "Answer 68: ÌååÏù¥ÌåÖ Ìå∞ÏΩò\n",
      "\n",
      "Question 69: ÏïÑÏûêÎ•¥ÏóêÏÑú Ïú†Î£å ÏòµÏÖò ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Î≠ò ÏÇ¨Ïïº ÌïòÏßÄ\n",
      "Answer 69: Î≥¥ÏÑù' ÏïÑÏù¥ÌÖú\n",
      "\n",
      "Question 70: The Slow TapeÎùºÎäî Ï†ïÍ∑úÏï®Î≤îÏù¥ Î∞úÌëúÎêú Ï†ïÌôïÌïú ÎÇ†ÏßúÎ•º Í≤ÄÏÉâÌï¥Ï§ò\n",
      "Answer 70: 2015ÎÖÑ 1Ïõî 14Ïùº\n",
      "\n",
      "Question 71: Ï£ΩÏùåÏùò Î∞îÌÉÑ ÌñâÏßÑÏóêÏÑú Ìè¨Î°úÎì§ÏùÄ Ïñ¥Îñ§ ÌïôÎåÄÎ•º Î∞õÏïòÏñ¥\n",
      "Answer 71: Íµ¨ÌÉÄ, Íµ∂Ï£ºÎ¶º\n",
      "\n",
      "Question 72: Ìó§Í≤îÏù¥ ÏÑ∏Í≥ÑÏÇ¨Î•º Î≠êÎùºÍ≥† Ï†ïÏùòÌñàÎäîÏßÄ ÏïåÎ†§Ï§ò\n",
      "Answer 72: Ï†àÎåÄÏ†ïÏã†(Ïù¥ÏÑ±)Ïù¥ ÏûêÏú†Î•º Ìñ•Ìï¥ ÎÇòÏïÑÍ∞ÄÎäî Í≥ºÏ†ï\n",
      "\n",
      "Question 73: Ï±ÑÏö¥ÏÇ¨ Ïù¥Î¶Ñ ÏßÄÏùÄ ÏÇ¨Îûå Ïù¥Î¶Ñ ÎßêÌï¥Ï§ò\n",
      "Answer 73: ÌòúÏãù Ïä§Îãò\n",
      "\n",
      "Question 74: ÎπåÌó¨Î¶Ñ ÏÖ∞ÌîÑÎßåÏù¥ ÎàÑÍµ¨ÏôÄ Ìï®Íªò Î£®Î•¥ ÏßÄÎ∞© ÎèåÍ≤©ÎåÄÎ•º Îß°ÏïòÏñ¥\n",
      "Answer 74: ÎπÖÌÜ†Î•¥ Î£®Ï≤¥ (Viktor Lutze)\n",
      "\n",
      "Question 75: ÌååÌÉÄ Ïú†ÎãàÏä§Îäî BBCÏôÄ Ïù∏ÌÑ∞Î∑∞ÏóêÏÑúÏÑú Ïπ¥Îã§ÌîºÏùò ÌñâÎ≥¥Î•º Ïñ¥ÎñªÍ≤å ÏòàÏÉÅÌñàÏßÄ\n",
      "Answer 75: ÏµúÌõÑÍπåÏßÄ Ìï≠Ï†ÑÌïòÍ±∞ÎÇò ÏûêÏÇ¥\n",
      "\n",
      "Question 76: Î¨¥ÏûêÌååÎ•¥ ÏôïÏ°∞ Ï∞ΩÎ¶ΩÏûê ÏïåÎ†§Ï§ò\n",
      "Answer 76: Î¨¥Î∞îÎùºÏ¶à ÏïåÎîò\n",
      "\n",
      "Question 77: ÍπÄÏû¨Ìò∏Îäî 2004ÎÖÑ Ïñ¥Îñ§ ÏÇ¨Í±¥Ïóê Ïó∞Î£®ÎêòÏñ¥ ÏãúÏ¶å Ï∂úÏû• Í∏àÏßÄÎ•º Î∞õÏïòÏßÄ\n",
      "Answer 77: 2004ÎÖÑ ÌïúÍµ≠ ÌîÑÎ°ú ÏïºÍµ¨ Î≥ëÏó≠ ÎπÑÎ¶¨ ÏÇ¨Í±¥\n",
      "\n",
      "Question 78: Ïñ¥Îñ§ Í∞ÄÏÑ§Ïù¥ Í≥ºÌïôÏ†ÅÏúºÎ°ú ÌèâÍ∞ÄÎêòÎ†§Î©¥ Í∑∏ Í∞ÄÏÑ§ÏùÄ Î¨¥ÏóáÏù¥ Í∞ÄÎä•Ìï¥ÏïºÌïòÏßÄ\n",
      "Answer 78: Î∞òÏ¶ù\n",
      "\n",
      "Question 79: ÏóÑÏöîÏÑ≠Ïù¥ ÌïúÍ∏∏ÍµêÌöå Î™©ÏÇ¨Í∞Ä Îêú Ìï¥Îäî\n",
      "Answer 79: 1958ÎÖÑ\n",
      "\n",
      "Question 80: ÏöîÏïô ÏôàÎû≠Ïù¥ Ïñ¥Îäê ÌåÄ Í∞êÎèÖÏù¥Ïïº\n",
      "Answer 80: KV ÏΩîÎ•¥Ìä∏Î†àÏù¥ÌÅ¨\n",
      "\n",
      "Question 81: ÍΩÉÏùÄ Ïñ¥Îñ§ ÏãùÎ¨ºÏóêÏÑúÎßå Î≥º Ïàò ÏûàÏñ¥\n",
      "Answer 81: Ï¢ÖÏûêÏãùÎ¨º\n",
      "\n",
      "Question 82: Ï≤†ÎèÑÌöåÏÇ¨ Ïî®ÏóîÏóòÏù¥ Ïö¥ÏòÅÌïòÎäî ÏßÄÏó≠Ïù¥ Ïñ¥ÎîîÏïº\n",
      "Answer 82: ÎèÖÏùº, Ïä§ÏúÑÏä§, Ïò§Ïä§Ìä∏Î¶¨ÏïÑ, ÎÑ§ÎçúÎûÄÎìú, Îç¥ÎßàÌÅ¨, ÌîÑÎûëÏä§, Î≤®Í∏∞Ïóê, Ïù¥ÌÉàÎ¶¨ÏïÑ, Ï≤¥ÏΩî\n",
      "\n",
      "Question 83: 1989ÎÖÑ ÎèÖÏùºÏ≤†ÎèÑÍ∞úÌòÅÏùÑ ÏúÑÌï¥ ÏÑ§ÏπòÎêú Í≥≥ÏùÄ Ïñ¥ÎîîÏßÄ\n",
      "Answer 83: ÎèÖÏùºÏ≤†ÎèÑÏúÑÏõêÌöå\n",
      "\n",
      "Question 84: Ïù¥ÏπòÎ¶¨Ï¶àÏπ¥Ïùò Í∏∞ÏõêÏù∏ ÎÇòÎùºÍ∞Ä Ïñ¥ÎîîÏïº\n",
      "Answer 84: Ï§ëÍµ≠\n",
      "\n",
      "Question 85: Ïò®ÎèÑÏùò Íµ≠Ï†úÎã®ÏúÑÍ∞Ä Î≠êÏïº\n",
      "Answer 85: ÏºàÎπà(K)\n",
      "\n",
      "Question 86: ÏµúÏßÑÏòÅÏù¥ ÎèôÍ≤ΩÌïòÎäî Í∞ÄÏàòÍ∞Ä ÎàÑÍµ¨Ïïº\n",
      "Answer 86: Ï°∞Ïö©ÌïÑ\n",
      "\n",
      "Question 87: ÏûÑÏßÑÏôúÎûÄÏùÄ ÏùºÎ≥∏Ïù¥ Ïñ¥Îäê ÏßÄÏó≠ÏùÑ ÏÑ†Í≥µÌïòÎ©¥ÏÑú Î∞úÎ∞úÌñàÏßÄ\n",
      "Answer 87: Î∂ÄÏÇ∞ÏßÑ\n",
      "\n",
      "Question 88: Îß•ÌîåÎùºÏù¥Í∞Ä Îã§ÏÑØÎ≤àÏß∏Î°ú Î∞úÎß§Ìïú Ïï®Î≤î Ïù¥Î¶ÑÏùÄ\n",
      "Answer 88: Above the Noise\n",
      "\n",
      "Question 89: ÏßÄÎ∞©ÏûêÏπòÏóêÍ¥ÄÌïúÏûÑÏãúÏ°∞ÏπòÎ≤ïÏùÄ Ïñ∏Ï†ú ÏãúÌñâÎêòÏóàÏñ¥\n",
      "Answer 89: 1961ÎÖÑ 10Ïõî 1Ïùº\n",
      "\n",
      "Question 90: PRT ÌååÎ•¥Î∞òÏùÑ Í¥ÄÎ¶¨ÌïòÎäî Î∂ÄÎåÄÍ∞Ä Ïñ¥ÎîîÏßÄ\n",
      "Answer 90: ÎØ∏Íµ≠ Ï†ú3Î≥¥Î≥ëÏÇ¨Îã®\n",
      "\n",
      "Question 91: Î∂ÅÌïúÏùò Íµ∞ÎåÄÎ•º Î≠êÎùºÍ≥† Î∂ÄÎ•¥ÏßÄ\n",
      "Answer 91: Ï°∞ÏÑ†Ïù∏ÎØºÍµ∞\n",
      "\n",
      "Question 92: ÌîÑÎùºÎ∏åÎîòÏä§ÌÅ¨Îäî Ïñ¥ÎîîÏóê ÏûàÏßÄ\n",
      "Answer 92: Îü¨ÏãúÏïÑ ÏπºÎ¶¨ÎãåÍ∑∏ÎùºÎìú Ï£º ÎÇ®Î∂Ä\n",
      "\n",
      "Question 93: ÎÇ®ÏïÖÏã†ÎèÑÏãúÏùò Í≥ÑÌöçÏù∏Íµ¨Îäî Î™á Î™ÖÏù¥Ïïº\n",
      "Answer 93: 15ÎßåÎ™Ö\n",
      "\n",
      "Question 94: ÌÅ¥Î°úÎÇòÏ†úÌåúÏù¥ ÌäπÌóàÎ•º Î∞õÏùÄ Ïó∞ÎèÑÎäî\n",
      "Answer 94: 1964ÎÖÑ\n",
      "\n",
      "Question 95: ÏãúÎÑ§ÎßàÌÖåÌÅ¨Ïùò ÏπúÍµ¨Îì§ Ï¥àÎåÄÎåÄÌëúÍ∞Ä ÎàÑÍµ¨Ïïº\n",
      "Answer 95: Î∞ïÏ∞¨Ïö±\n",
      "\n",
      "Question 96: 2017ÎÖÑ ÎåÄÏÑ†ÏóêÏÑú ÌôçÏ§ÄÌëúÏôÄ Îã®ÏùºÌôî ÌïòÏßÄ ÏïäÏùÄ ÌõÑÎ≥¥Í∞Ä ÎàÑÍµ¨Ïïº\n",
      "Answer 96: Ïú†ÏäπÎØº\n",
      "\n",
      "Question 97: ÏöîÏ¶ò ÎπÑÎ∏åÏΩîÎìúÍ∞Ä Ï£ºÏöî Î≠òÎ°ú Ïì∞Ïó¨\n",
      "Answer 97: Ï≤úÎ¨∏Ìïô Î∞è Î¨ºÎ¶¨ÌïôÏóêÏÑú ÏùºÎ∞òÏ†ÅÏù∏ Î¨∏Ìóå Í¥ÄÎ¶¨ ÏãùÎ≥ÑÏûê\n",
      "\n",
      "Question 98: Ïä§ÌäúÏñ¥Ìä∏ÎùºÎäî ÎèÑÏãúÏùò ÏõêÎûò Ïù¥Î¶ÑÏùÄ\n",
      "Answer 98: ÌÖåÏù¥ÎùºÏ¶àÎπå\n",
      "\n",
      "Question 99: Îã•ÌÑ∞ ÌõÑÏóêÏÑú Î°úÏ¶à ÌÉÄÏùºÎü¨Îäî Ïñ¥Îñ§ Ïù∏Î¨ºÏù¥Ïïº\n",
      "Answer 99: Îü∞ÎçòÏóê ÏÇ¨Îäî ÎÖ∏ÎèôÍ≥ÑÏ∏µÏù∏ 19ÏÑ∏ Ï†êÏõê\n",
      "\n",
      "Question 100: ÏòÅÍµ≠-Ïù¥ÎùºÌÅ¨ Ï†ÑÏüÅÏùÄ Ïñ∏Ï†ú ÏùºÏñ¥ÎÇ¨Ïñ¥\n",
      "Answer 100: 1941ÎÖÑ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Access the questions and answers\n",
    "questions = dataset[\"train\"][\"Question\"]\n",
    "#answers = dataset[\"Answer\"]\n",
    "answers = dataset[\"train\"][\"Answer\"]\n",
    "\n",
    "# Combine questions and answers into pairs\n",
    "question_answer_pairs = list(zip(questions, answers))\n",
    "\n",
    "# Select 100 random question-answer pairs\n",
    "random_pairs = random.sample(question_answer_pairs, 100)\n",
    "\n",
    "# Print the selected questions and answers\n",
    "for i, (question, answer) in enumerate(random_pairs, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"Answer {i}: {answer}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvhcQvbQNilo",
    "outputId": "b727f535-675a-4d52-b4cd-031215a471dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Response Time using slm Qustion/Answer dataset: 0.010227437019348145\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Function to simulate the Average response time\n",
    "def ai_model_response_time(input_text):\n",
    "    # Simulate processing time\n",
    "    processing_time = 0.01  # Adjust this value to simulate actual processing time\n",
    "    time.sleep(processing_time)\n",
    "    return \"This is the response to: \" + input_text\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Sample a smaller subset of examples for validation\n",
    "validation_subset = dataset[\"train\"].shuffle().select(range(100))\n",
    "\n",
    "# Measure response times for validation subset\n",
    "validation_response_times = []\n",
    "for example in validation_subset:\n",
    "    input_text = example[\"Question\"]  # Only take the first 5 characters for faster simulation\n",
    "    start_time = time.time()\n",
    "    response = ai_model_response_time(input_text)\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    validation_response_times.append(response_time)\n",
    "\n",
    "# Calculate the average response time for validation subset\n",
    "average_validation_response_time = sum(validation_response_times) / len(validation_response_times)\n",
    "print(\"Average Validation Response Time using slm Qustion/Answer dataset:\", average_validation_response_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEAeyMtwT57z"
   },
   "source": [
    "\n",
    "**Now we print the response time for the validation dataset with random 100 questions and answers.bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwiN1jqDVjRJ",
    "outputId": "cebad60c-5aa8-42fc-9de8-e706cbc25eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement rough_score (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for rough_score\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install rough_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "lADbgMOYUDVw",
    "outputId": "d4a1ea76-2efa-4bc5-e10c-9df6c0d27084"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-466ec1d06dd4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrouge_scorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m _DESCRIPTION = \"\"\"\\\n\u001b[1;32m      5\u001b[0m \u001b[0mROUGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mRecall\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mOriented\u001b[0m \u001b[0mUnderstudy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mGisting\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0mwidely\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevaluating\u001b[0m \u001b[0mautomatic\u001b[0m \u001b[0msummarization\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmachine\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0msystems\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIt\u001b[0m \u001b[0mcompares\u001b[0m \u001b[0mautomatically\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtranslations\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mreference\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtranslations\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mby\u001b[0m \u001b[0mhumans\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mROUGE\u001b[0m \u001b[0mmeasures\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moverlap\u001b[0m \u001b[0mof\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongest\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0msubsequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m \u001b[0mstatistical\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mautomatic\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreference\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics widely used for evaluating automatic summarization and machine translation systems. It compares automatically generated summaries or translations with reference summaries or translations provided by humans. ROUGE measures the overlap of n-grams, longest common subsequences, and other statistical features between the automatic and reference summaries.\n",
    "\"\"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION = \"\"\"\n",
    "Calculates ROUGE scores for a list of hypotheses and references.\n",
    "Args:\n",
    "    predictions: List of predicted texts to score. Each predicted text should be a string.\n",
    "    references: List of reference texts for each prediction. Each reference text should be a string.\n",
    "    rouge_types: A list of ROUGE types to calculate. Valid names include \"rouge1\", \"rouge2\", \"rougeL\", and \"rougeLsum\".\n",
    "    use_stemmer: Whether to use the Porter stemmer to strip word suffixes before calculating ROUGE.\n",
    "    use_aggregator: Whether to return aggregated scores if set to True.\n",
    "Returns:\n",
    "    Dictionary containing ROUGE scores for each specified type. Each score is represented as a tuple (precision, recall, f1_score).\n",
    "\"\"\"\n",
    "\n",
    "class Rouge(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            citation=\"https://github.com/google-research/google-research/tree/master/rouge\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n",
    "                    \"references\": datasets.Value(\"string\", id=\"sequence\"),\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[\"https://github.com/google-research/google-research/tree/master/rouge\"],\n",
    "            reference_urls=[\n",
    "                \"https://en.wikipedia.org/wiki/ROUGE_(metric)\",\n",
    "                \"https://github.com/google-research/google-research/tree/master/rouge\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references, rouge_types=None, use_aggregator=True, use_stemmer=False):\n",
    "        if rouge_types is None:\n",
    "            rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)\n",
    "        if use_aggregator:\n",
    "            aggregator = scoring.BootstrapAggregator()\n",
    "        else:\n",
    "            scores = []\n",
    "\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            score = scorer.score(ref, pred)\n",
    "            if use_aggregator:\n",
    "                aggregator.add_scores(score)\n",
    "            else:\n",
    "                scores.append(score)\n",
    "\n",
    "        if use_aggregator:\n",
    "            result = aggregator.aggregate()\n",
    "        else:\n",
    "            result = {}\n",
    "            for key in scores[0]:\n",
    "                result[key] = [score[key] for score in scores]\n",
    "\n",
    "        return result\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Sample usage\n",
    "rouge_metric = Rouge()\n",
    "\n",
    "# Compute ROUGE scores for the dataset\n",
    "predictions = dataset[\"train\"][\"Question\"]\n",
    "references = dataset[\"train\"][\"Answer\"]\n",
    "results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "print(\"ROUGE Scores:\", results)\n",
    "\n",
    "# Calculate error rates\n",
    "error_rates = []\n",
    "for rouge_type, rouge_scores in results.items():\n",
    "    if rouge_type.startswith(\"rouge\"):\n",
    "        for score in rouge_scores:\n",
    "            error_rate = 1 - score.fmeasure\n",
    "            error_rates.append(error_rate)\n",
    "\n",
    "# Calculate average error rate\n",
    "average_error_rate = sum(error_rates) / len(error_rates)\n",
    "print(\"Average Error Rate:\", average_error_rate)\n",
    "\n",
    "# Calculate the minimum average error rate to reach 20% improvement\n",
    "target_average_error_rate = 0.20\n",
    "desired_average_error_rate = max(target_average_error_rate, average_error_rate)\n",
    "print(\"Desired Average Error Rate:\", desired_average_error_rate)\n",
    "\n",
    "# If the current error rate is already below the desired rate, print a message\n",
    "if average_error_rate <= target_average_error_rate:\n",
    "    print(\"Current average error rate is already below the desired rate.\")\n",
    "else:\n",
    "    print(\"Model improvement needed to reach the desired average error rate.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "055959ae409e4231a7faffd0114d297c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06db836781354194ace7687bc6a0b166": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c459e8a68f24f858d1034f181d7904a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_643b2fcad9f74f74b1d4078a53b37057",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_055959ae409e4231a7faffd0114d297c",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "24bfd8db0ebb404b92033d12057c7276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c1762ad4fd343ce9710294c7ec433ed",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e0c1b32e5c3e43a999721644d22faa52",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
     }
    },
    "2ab76a123e354709a6f85464bbee2278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bbc231dc3ac4af7bf1699244243eb2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78bc9e34f0474c2a8120c73e34f21ef5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8cda7bed26fb4d36b76b38c042e60b82",
      "value": "‚Äá8.29M/8.29M‚Äá[00:00&lt;00:00,‚Äá30.6MB/s]"
     }
    },
    "3f04e8fdd930400580e0979a7d6e5415": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4931ad9937f4434b957c66a53c58da58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4c611f0b11ac4be792fa13611c8fb40b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68e4280bd0bc4f3b96c0ec5ac29ac298",
       "IPY_MODEL_8e7bfa83e52b4f8fa3de696079896f08",
       "IPY_MODEL_2bbc231dc3ac4af7bf1699244243eb2a"
      ],
      "layout": "IPY_MODEL_cb439c526f1949de81fbaed778df5cb3"
     }
    },
    "52994ba9f507463a8b78f10bff7087e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55fd89f073794fd583d7a9b4530823c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "594e0c67a188425e83f3004b62e66af4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "643b2fcad9f74f74b1d4078a53b37057": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68e4280bd0bc4f3b96c0ec5ac29ac298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52994ba9f507463a8b78f10bff7087e5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e91b61759b6f4faea42327d5f25240b2",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "6c1762ad4fd343ce9710294c7ec433ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78bc9e34f0474c2a8120c73e34f21ef5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cda7bed26fb4d36b76b38c042e60b82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e7bfa83e52b4f8fa3de696079896f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daf7937813634757850e74ac516a4fcb",
      "max": 8292258,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55fd89f073794fd583d7a9b4530823c7",
      "value": 8292258
     }
    },
    "9d451435a1a44ffaac676b755e9fce1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c459e8a68f24f858d1034f181d7904a",
       "IPY_MODEL_a1184e3ed87548858661d0dd191b6994",
       "IPY_MODEL_ac6d17c886354782ab87a5a6246f3448"
      ],
      "layout": "IPY_MODEL_2ab76a123e354709a6f85464bbee2278"
     }
    },
    "a1184e3ed87548858661d0dd191b6994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f03b199e460947a0ace770d3842decba",
      "max": 39,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2f8fb2290a942bf9662beaae6ab7b6a",
      "value": 39
     }
    },
    "abddf7a9d0b04273a9fec0cb00172404": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06db836781354194ace7687bc6a0b166",
      "max": 100268,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4931ad9937f4434b957c66a53c58da58",
      "value": 100268
     }
    },
    "ac6d17c886354782ab87a5a6246f3448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9161803d41147c68a670cc243ad6ef2",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b01b66564d78469eac04490b8f033d21",
      "value": "‚Äá39.0/39.0‚Äá[00:00&lt;00:00,‚Äá2.07kB/s]"
     }
    },
    "b01b66564d78469eac04490b8f033d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b42b53b3e3fb46c09dfd5ef10b5b3b12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f04e8fdd930400580e0979a7d6e5415",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f456caf574a640a4b3d4ccf0b3cddd9c",
      "value": "‚Äá100268/100268‚Äá[00:00&lt;00:00,‚Äá300790.21‚Äáexamples/s]"
     }
    },
    "c2f8fb2290a942bf9662beaae6ab7b6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cb1b6de1ee0f404ca51e9a8ec62c86cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24bfd8db0ebb404b92033d12057c7276",
       "IPY_MODEL_abddf7a9d0b04273a9fec0cb00172404",
       "IPY_MODEL_b42b53b3e3fb46c09dfd5ef10b5b3b12"
      ],
      "layout": "IPY_MODEL_594e0c67a188425e83f3004b62e66af4"
     }
    },
    "cb439c526f1949de81fbaed778df5cb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf7937813634757850e74ac516a4fcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0c1b32e5c3e43a999721644d22faa52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e91b61759b6f4faea42327d5f25240b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f03b199e460947a0ace770d3842decba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f456caf574a640a4b3d4ccf0b3cddd9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9161803d41147c68a670cc243ad6ef2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
