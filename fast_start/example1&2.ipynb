{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Fast Start Example #1 - Library - converting document files into an indexed knowledge collection.<br>\n",
    "    In this example, we will illustrate a basic recipe for completing the following steps:<br>\n",
    "      1. Create a library as a organizing construct for your knowledge-base<br>\n",
    "      2. Download sample files for a Fast Start - easy to 'swap out' and replace with your own files<br>\n",
    "      3. Use library.add_files method to automatically parse, text chunk and index the documents<br>\n",
    "      4. Run a basic text query against your new Library<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llmware.library import Library\n",
    "from llmware.retrieval import Query\n",
    "from llmware.setup import Setup\n",
    "from llmware.configs import LLMWareConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_documents_into_library(library_name, sample_folder):\n",
    "    print(f\"\\nExample - Parsing Files into Library\")\n",
    "\n",
    "    #   create new library\n",
    "    print (f\"\\nStep 1 - creating library {library_name}\")\n",
    "    library = Library().create_new_library(library_name)\n",
    "\n",
    "    #   load the llmware sample files\n",
    "    #   -- note: if you have used this example previously, UN-Resolutions-500 is new path\n",
    "    #   -- to pull updated sample files, set: 'over_write=True'\n",
    "    sample_files_path = Setup().load_sample_files(over_write=False)\n",
    "    print (f\"Step 2 - loading the llmware sample files and saving at: {sample_files_path}\")\n",
    "\n",
    "    #   note: to replace with your own documents, just point to a local folder path that has the documents\n",
    "    ingestion_folder_path = os.path.join(sample_files_path, sample_folder)\n",
    "    print (f\"Step 3 - parsing and indexing files from {ingestion_folder_path}\")\n",
    "\n",
    "    #   add files is the key ingestion method - parses, text chunks and indexes all files in folder\n",
    "    #       --will automatically route to correct parser based on file extension\n",
    "    #       --supported file extensions:  .pdf, .pptx, .docx, .xlsx, .csv, .md, .txt, .json, .wav, and .zip, .jpg, .png\n",
    "    parsing_output = library.add_files(ingestion_folder_path)\n",
    "    print (f\"Step 4 - completed parsing - {parsing_output}\")\n",
    "\n",
    "    #   check the updated library card\n",
    "    updated_library_card = library.get_library_card()\n",
    "    doc_count = updated_library_card[\"documents\"]\n",
    "    block_count = updated_library_card[\"blocks\"]\n",
    "    print(f\"Step 5 - updated library card - documents - {doc_count} - blocks - {block_count} - {updated_library_card}\")\n",
    "\n",
    "    #   check the main folder structure created for the library - check /images to find extracted images\n",
    "    library_path = library.library_main_path\n",
    "    print(f\"Step 6 - library artifacts - including extracted images - saved at folder path - {library_path}\")\n",
    "\n",
    "    #   use .add_files as many times as needed to build up your library, and/or create different libraries for\n",
    "    #   different knowledge bases\n",
    "\n",
    "    #   now, your library is ready to go and you can start to use the library for running queries\n",
    "    #   if you are using the \"Agreements\" library, then a good easy 'hello world' query is \"base salary\"\n",
    "    #   if you are using one of the other sample folders (or your own), then you should adjust the query\n",
    "\n",
    "    #   queries are always created the same way, e.g., instantiate a Query object, and pass a library object\n",
    "    #   --within the Query class, there are a variety of useful methods to run different types of queries\n",
    "    test_query = \"retrieve autmented generation\"\n",
    "    print(f\"\\nStep 7 - running a test query - {test_query}\\n\")\n",
    "    query_results = Query(library).text_query(test_query, result_count=10)\n",
    "    for i, result in enumerate(query_results):\n",
    "\n",
    "        #   note: each result is a dictionary with a wide range of useful keys\n",
    "        #   -- we would encourage you to take some time to review each of the keys and the type of metadata available\n",
    "\n",
    "        #   here are a few useful attributes\n",
    "        text = result[\"text\"]\n",
    "        file_source = result[\"file_source\"]\n",
    "        page_number = result[\"page_num\"]\n",
    "        doc_id = result[\"doc_ID\"]\n",
    "        block_id = result[\"block_ID\"]\n",
    "        matches = result[\"matches\"]\n",
    "\n",
    "        #   -- if print to console is too verbose, then pick just a few keys for print\n",
    "        print(\"query results: \", i, result)\n",
    "    return parsing_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example - Parsing Files into Library\n",
      "\n",
      "Step 1 - creating library example1_library\n",
      "Step 2 - loading the llmware sample files and saving at: C:\\Users\\hsyyu\\llmware_data\\sample_files\n",
      "Step 3 - parsing and indexing files from C:\\Users\\hsyyu\\llmware_data\\sample_files\\Agreements\n",
      "Step 4 - completed parsing - {'docs_added': 0, 'blocks_added': 0, 'images_added': 0, 'pages_added': 0, 'tables_added': 0, 'rejected_files': []}\n",
      "Step 5 - updated library card - documents - 84 - blocks - 15546 - {'_id': 1, 'library_name': 'example1_library', 'embedding': [{'embedding_status': 'yes', 'embedding_model': 'mini-lm-sbert', 'embedding_db': 'chromadb', 'embedding_dims': 384, 'embedded_blocks': 2211, 'time_stamp': '2024-05-15_162551'}], 'knowledge_graph': 'no', 'unique_doc_id': 84, 'documents': 84, 'blocks': 15546, 'images': 9, 'pages': 1865, 'tables': 10, 'account_name': 'llmware'}\n",
      "Step 6 - library artifacts - including extracted images - saved at folder path - C:\\Users\\hsyyu\\llmware_data\\accounts\\llmware\\example1_library\n",
      "\n",
      "Step 7 - running a test query - retrieve autmented generation\n",
      "\n",
      "query results:  0 {'query': 'retrieve autmented generation', '_id': '15329', 'text': \"\\n\\u200b● \\u200bReal-World Application: Fine-tuned models are ready for real-world deployment in applications like virtual assistants, content generation, and more.\\n\\n\\n\\n\\n\\nFine-Tuning for Text Generation - Crafting Creativity\\n\\n\\nThe Art of Text Generation\\n\\nText generation is a fascinating application of language models. It empowers your model to create content, whether it's for storytelling, content generation, or personalized messages. Let's delve into the possibilities and why text generation is so exciting:\\n\", 'doc_ID': 84, 'block_ID': 64, 'page_num': 1, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'Understanding Langchain.txt', 'added_to_collection': '2024-05-17_193822', 'table': '', 'coords_x': 0, 'coords_y': 0, 'coords_cx': 0, 'coords_cy': 0, 'external_files': '', 'score': -11.209611628049267, 'similarity': 0.0, 'distance': 0.0, 'matches': [[132, 'generation'], [180, 'generation'], [231, 'generation'], [248, 'generation'], [389, 'generation'], [475, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  1 {'query': 'retrieve autmented generation', '_id': '13527', 'text': 'Code generation compilers With a code generation approach, a code-generating compiler takes a model file as input and transforms it into a program that implements it. For operator support, the program relies on a library of prewritten operators, calling them in the correct order and passing the appropriate parameters. Code generation provides many of the same benefits as an interpreter-based approach ', 'doc_ID': 83, 'block_ID': 881, 'page_num': 152, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'C:\\\\\\\\Users\\\\\\\\hsyyu\\\\\\\\llmware_data\\\\\\\\tmp\\\\\\\\parser_tmp\\\\\\\\process_pdf_files\\\\\\\\aiEdge.pdf', 'added_to_collection': '05/17/24 19:38:12', 'table': '', 'coords_x': 22579, 'coords_y': 294, 'coords_cx': 151, 'coords_cy': 5, 'external_files': '', 'score': -9.158699302702809, 'similarity': 0.0, 'distance': 0.0, 'matches': [[5, 'generation'], [38, 'generation'], [325, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  2 {'query': 'retrieve autmented generation', '_id': '15388', 'text': '\\n3. Content Generation: Writers, content creators, and marketers have embraced language models to automate content generation. These models can generate a wide range of content, from articles and product descriptions to advertisements, saving time and resources.\\n\\n4. Accessibility: Language models play a crucial role in making technology more accessible. They assist individuals with disabilities by providing natural language interactions with devices and applications, bridging the accessibility gap.\\n', 'doc_ID': 84, 'block_ID': 123, 'page_num': 1, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'Understanding Langchain.txt', 'added_to_collection': '2024-05-17_193822', 'table': '', 'coords_x': 0, 'coords_y': 0, 'coords_cx': 0, 'coords_cy': 0, 'external_files': '', 'score': -8.764980715038831, 'similarity': 0.0, 'distance': 0.0, 'matches': [[12, 'generation'], [115, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  3 {'query': 'retrieve autmented generation', '_id': '15330', 'text': '\\n\\u200b● \\u200bCreative Writing: Fine-tuning your model for text generation lets you craft engaging stories, poems, and articles. It can be your virtual co-author or a source of inspiration for your writing projects.\\n\\n\\u200b● \\u200bContent Generation: In a fast-paced digital world, the demand for content is insatiable. Your fine-tuned model can automatically create product descriptions, news articles, or any text-based content.\\n\\n\\u200b● \\u200bPersonalization: With a fine-tuned model, you can generate personalized messages for users, customers, or readers.', 'doc_ID': 84, 'block_ID': 65, 'page_num': 1, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'Understanding Langchain.txt', 'added_to_collection': '2024-05-17_193822', 'table': '', 'coords_x': 0, 'coords_y': 0, 'coords_cx': 0, 'coords_cy': 0, 'external_files': '', 'score': -8.453332310796165, 'similarity': 0.0, 'distance': 0.0, 'matches': [[55, 'generation'], [220, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  4 {'query': 'retrieve autmented generation', '_id': '15313', 'text': \" Let's explore the key factors that will guide your choice and learn how to load a specific model in Langchain.\\n\\nTask Requirements: A Model Tailored to Your Needs\\n\\nThe first and most crucial step in model selection is understanding your task's requirements. Different language model architectures are designed for specific purposes. Here's how to align your task with the right model:\\n\\n1. Text Generation: If your goal is creative text generation, models like GPT-2 or GPT-3 are your best bet.\", 'doc_ID': 84, 'block_ID': 48, 'page_num': 1, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'Understanding Langchain.txt', 'added_to_collection': '2024-05-17_193822', 'table': '', 'coords_x': 0, 'coords_y': 0, 'coords_cx': 0, 'coords_cy': 0, 'external_files': '', 'score': -8.3056736886395, 'similarity': 0.0, 'distance': 0.0, 'matches': [[394, 'generation'], [436, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  5 {'query': 'retrieve autmented generation', '_id': '15331', 'text': ' Engage your audience with content tailored to their preferences and needs.\\n\\nPython Code: Fine-Tuning for Text Generation\\n\\nTo illustrate the process of fine-tuning for text generation, let\\'s take a simplified example using Langchain. In this code, we\\'ll fine-tune a GPT-2 model for crafting creative stories:\\n\\n```\\n\\n# Import Langchain\\n\\nfrom langchain import Langchain\\n\\n# Load a pre-trained GPT-2 model\\n\\nmodel = Langchain(\"gpt2\")\\n\\n# Fine-tune the model with your creative writing data\\n\\nfine_tuned_model = model.fine_tune(your_story_data)\\n\\n```\\n', 'doc_ID': 84, 'block_ID': 66, 'page_num': 1, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'Understanding Langchain.txt', 'added_to_collection': '2024-05-17_193822', 'table': '', 'coords_x': 0, 'coords_y': 0, 'coords_cx': 0, 'coords_cy': 0, 'external_files': '', 'score': -8.3056736886395, 'similarity': 0.0, 'distance': 0.0, 'matches': [[111, 'generation'], [173, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  6 {'query': 'retrieve autmented generation', '_id': '15332', 'text': \"\\nBy fine-tuning a pre-trained model like GPT-2, you'll witness your language model's transformation into a creative storyteller.\\n\\nTeasing Out Creativity\\n\\nThe applications of text generation are as diverse as your imagination. Whether you're an aspiring novelist, a content creator, or a developer looking to enhance user experiences, fine-tuning for text generation opens the door to limitless creative possibilities. Here's what you can achieve:\\n\\n- Novel Writing: Collaborate with your language model to co-author a novel, generate plot ideas, or even create compelling dialogues.\\n\", 'doc_ID': 84, 'block_ID': 67, 'page_num': 1, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'Understanding Langchain.txt', 'added_to_collection': '2024-05-17_193822', 'table': '', 'coords_x': 0, 'coords_y': 0, 'coords_cx': 0, 'coords_cy': 0, 'external_files': '', 'score': -8.16308496339754, 'similarity': 0.0, 'distance': 0.0, 'matches': [[179, 'generation'], [355, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  7 {'query': 'retrieve autmented generation', '_id': '15326', 'text': \"\\n\\u200b● \\u200bGeneralization: Pre-trained models come with a strong understanding of language. Fine-tuning makes them adaptable to a wide range of tasks, from text generation to translation.\\n\\n\\u200b● \\u200bSpeed: With transfer learning, you can rapidly develop applications without building a model from the ground up. This speed is invaluable in today's fast-paced tech landscape.\\n\\nPython Code: Fine-Tuning with Langchain\\n\\nTo illustrate how transfer learning and fine-tuning work, let's look at a simplified example using Langchain. In this snippet, we'll fine-tune a GPT-2 model for text generation:\\n\\n```\\n\", 'doc_ID': 84, 'block_ID': 61, 'page_num': 1, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'Understanding Langchain.txt', 'added_to_collection': '2024-05-17_193822', 'table': '', 'coords_x': 0, 'coords_y': 0, 'coords_cx': 0, 'coords_cy': 0, 'external_files': '', 'score': -8.116637157997824, 'similarity': 0.0, 'distance': 0.0, 'matches': [[155, 'generation'], [571, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  8 {'query': 'retrieve autmented generation', '_id': '14667', 'text': 'manually retrieve the images from the camera, a sometimes labor-intensive task depending on where the camera was placed in the wild and how remote the location was. Once the images were retrieved, it would take weeks or months for researchers with trained eyes to comb through the images by hand to find their target species in the photographs. By integrating AI into the camera device itself, researchers ', 'doc_ID': 83, 'block_ID': 2021, 'page_num': 335, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'C:\\\\\\\\Users\\\\\\\\hsyyu\\\\\\\\llmware_data\\\\\\\\tmp\\\\\\\\parser_tmp\\\\\\\\process_pdf_files\\\\\\\\aiEdge.pdf', 'added_to_collection': '05/17/24 19:38:12', 'table': '', 'coords_x': 3093, 'coords_y': 249, 'coords_cx': 334, 'coords_cy': 4, 'external_files': '', 'score': -7.648726637544372, 'similarity': 0.0, 'distance': 0.0, 'matches': [[9, 'retrieve'], [186, 'retrieve']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n",
      "query results:  9 {'query': 'retrieve autmented generation', '_id': '13276', 'text': \"Deep learning interpreters, like TensorFlow Lite for Microcontrollers, use an interpreter to execute a model that is stored as a file. They are flexible and easy to work with, but they come with some computational and memory overhead, and they don't support every type of model. Code generation Code generation tools, like EON, take a trained deep learning model and translate it into optimized embedded \", 'doc_ID': 83, 'block_ID': 630, 'page_num': 113, 'content_type': 'text', 'author_or_speaker': '', 'special_field1': '', 'file_source': 'C:\\\\\\\\Users\\\\\\\\hsyyu\\\\\\\\llmware_data\\\\\\\\tmp\\\\\\\\parser_tmp\\\\\\\\process_pdf_files\\\\\\\\aiEdge.pdf', 'added_to_collection': '05/17/24 19:38:12', 'table': '', 'coords_x': 17920, 'coords_y': 1046, 'coords_cx': 112, 'coords_cy': 37, 'external_files': '', 'score': -6.984069187942767, 'similarity': 0.0, 'distance': 0.0, 'matches': [[284, 'generation'], [300, 'generation']], 'account_name': 'llmware', 'library_name': 'example1_library'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #  note on sample documents - downloaded by Setup()\n",
    "    #       UN-Resolutions-500 is 500 pdf documents\n",
    "    #       Invoices is 40 pdf invoice samples\n",
    "    #       Agreements is ~15 contract documents\n",
    "    #       AgreementsLarge is ~80 contract documents\n",
    "    #       FinDocs is ~15 financial annual reports and earnings\n",
    "    #       SmallLibrary is a mix of ~10 pdf and office documents\n",
    "    #       metafactory is a mix of pdfs and csv files\n",
    "\n",
    "    #   optional - set the active DB to be used - by default, it is \"mongo\"\n",
    "    #   if you are just getting started, and have not installed a separate db, select \"sqlite\"\n",
    "    LLMWareConfig().set_active_db(\"sqlite\")\n",
    "\n",
    "    #   if you want to see a different log view, e.g., see a list of each parsed files 'in progress',\n",
    "    #   you can set a different debug mode view anytime\n",
    "\n",
    "    #   debug_mode options -\n",
    "    #       0 - default - shows status manager (useful in large parsing jobs) and errors will be displayed\n",
    "    #       2 - file name only - shows file name being parsed, and errors only\n",
    "\n",
    "    #   for purpose of this example, let's change so we can see file-by-file progress\n",
    "    LLMWareConfig().set_config(\"debug_mode\", 2)\n",
    "\n",
    "    #   this is a list of document folders that will be pulled down by calling Setup()\n",
    "    sample_folders = [\"Agreements\", \"Invoices\", \"UN-Resolutions-500\", \"SmallLibrary\", \"FinDocs\", \"AgreementsLarge\"]\n",
    "    library_name = \"example1_library\"\n",
    "\n",
    "    #   select one of the sample folders\n",
    "    selected_folder = sample_folders[0]     # e.g., \"Agreements\"\n",
    "\n",
    "    #   run the example\n",
    "    output = parsing_documents_into_library(library_name, selected_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llmware.library import Library\n",
    "from llmware.retrieval import Query\n",
    "from llmware.setup import Setup\n",
    "from llmware.status import Status\n",
    "from llmware.models import ModelCatalog\n",
    "from llmware.configs import LLMWareConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import util\n",
    "if not util.find_spec(\"chromadb\"):\n",
    "    print(\"\\nto run this example with chromadb, you need to install the chromadb python sdk:  pip3 install chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_library(library_name):\n",
    "    \"\"\" Note: this setup_library method is provided to enable a self-contained example to create a test library \"\"\"\n",
    "\n",
    "    #   Step 1 - Create library which is the main 'organizing construct' in llmware\n",
    "    print (\"\\nupdate: Creating library: {}\".format(library_name))\n",
    "    library = Library().create_new_library(library_name)\n",
    "\n",
    "    #   check the embedding status 'before' installing the embedding\n",
    "    embedding_record = library.get_embedding_status()\n",
    "    print(\"embedding record - before embedding \", embedding_record)\n",
    "\n",
    "    #   Step 2 - Pull down the sample files from S3 through the .load_sample_files() command\n",
    "    #   --note: if you need to refresh the sample files, set 'over_write=True'\n",
    "    print (\"update: Downloading Sample Files\")\n",
    "    sample_files_path = Setup().load_sample_files(over_write=False)\n",
    "\n",
    "    #   Step 3 - point \".add_files\" method to the folder of documents that was just created\n",
    "    #   this method parses the documents, text chunks, and captures in database\n",
    "    print(\"update: Parsing and Text Indexing Files\")\n",
    "    library.add_files(input_folder_path=os.path.join(sample_files_path, \"Agreements\"),\n",
    "                      chunk_size=400, max_chunk_size=600, smart_chunking=1)\n",
    "    return library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_vector_embeddings(library, embedding_model_name):\n",
    "    \"\"\" This method is the core example of installing an embedding on a library.\n",
    "        -- two inputs - (1) a pre-created library object and (2) the name of an embedding model \"\"\"\n",
    "    library_name = library.library_name\n",
    "    vector_db = LLMWareConfig().get_vector_db()\n",
    "    print(f\"\\nupdate: Starting the Embedding: \"\n",
    "          f\"library - {library_name} - \"\n",
    "          f\"vector_db - {vector_db} - \"\n",
    "          f\"model - {embedding_model_name}\")\n",
    "\n",
    "    #   *** this is the one key line of code to create the embedding ***\n",
    "    library.install_new_embedding(embedding_model_name=embedding_model, vector_db=vector_db,batch_size=100)\n",
    "\n",
    "    #   note: for using llmware as part of a larger application, you can check the real-time status by polling Status()\n",
    "    #   --both the EmbeddingHandler and Parsers write to Status() at intervals while processing\n",
    "    update = Status().get_embedding_status(library_name, embedding_model)\n",
    "    print(\"update: Embeddings Complete - Status() check at end of embedding - \", update)\n",
    "\n",
    "    # Start using the new vector embeddings with Query\n",
    "    sample_query = \"large language models\"\n",
    "    print(\"\\n\\nupdate: Run a sample semantic/vector query: {}\".format(sample_query))\n",
    "\n",
    "    #   queries are constructed by creating a Query object, and passing a library as input\n",
    "    query_results = Query(library).semantic_query(sample_query, result_count=20)\n",
    "    for i, entries in enumerate(query_results):\n",
    "\n",
    "        #   each query result is a dictionary with many useful keys\n",
    "        text = entries[\"text\"]\n",
    "        document_source = entries[\"file_source\"]\n",
    "        page_num = entries[\"page_num\"]\n",
    "        vector_distance = entries[\"distance\"]\n",
    "\n",
    "        #   to see all of the dictionary keys returned, uncomment the line below\n",
    "        #   print(\"update: query_results - all - \", i, entries)\n",
    "\n",
    "        #  for display purposes only, we will only show the first 125 characters of the text\n",
    "        if len(text) > 125:  text = text[0:125] + \" ... \"\n",
    "        print(\"\\nupdate: query results - {} - document - {} - page num - {} - distance - {} \"\n",
    "              .format( i, document_source, page_num, vector_distance))\n",
    "        print(\"update: text sample - \", text)\n",
    "\n",
    "    #   lets take a look at the library embedding status again at the end to confirm embeddings were created\n",
    "    embedding_record = library.get_embedding_status()\n",
    "    print(\"\\nupdate:  embedding record - \", embedding_record)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "update: Creating library: example2_library\n",
      "embedding record - before embedding  [{'embedding_status': 'yes', 'embedding_model': 'mini-lm-sbert', 'embedding_db': 'chromadb', 'embedding_dims': 384, 'embedded_blocks': 2304, 'time_stamp': '2024-05-17_155111'}]\n",
      "update: Downloading Sample Files\n",
      "update: Parsing and Text Indexing Files\n",
      "\n",
      "update: Starting the Embedding: library - example2_library - vector_db - chromadb - model - mini-lm-sbert\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 100 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 200 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 300 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 400 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 500 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 600 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 700 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 800 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 900 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1000 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1100 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1200 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1300 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1400 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1500 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1600 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1700 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1800 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 1900 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2000 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2100 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2200 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2300 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2400 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2500 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2600 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2700 of 2760\n",
      "update: embedding_handler - ChromaDB - Embeddings Created: 2760 of 2760\n",
      "update: Embeddings Complete - Status() check at end of embedding -  [{'_id': 11, 'key': 'example2_library_embedding_mini-lm-sbert', 'summary': '2760 of 2760 blocks', 'start_time': '1715942515.3199558', 'end_time': '1715942566.6275935', 'total': 2760, 'current': 2760, 'units': 'blocks'}]\n",
      "\n",
      "\n",
      "update: Run a sample semantic/vector query: large language models\n",
      "\n",
      "update: query results - 0 - document - Understanding Langchain.txt - page num - 1 - distance - 0.13588636517542463 \n",
      "update: text sample -   The future of language models should be one where technology serves humanity while respecting its values and rights.\n",
      "\n",
      "The ch ... \n",
      "\n",
      "update: query results - 1 - document - Understanding Langchain.txt - page num - 1 - distance - 0.14527255296707153 \n",
      "update: text sample -  \n",
      "Language models have the ability to comprehend the context of words and phrases, which allows them to generate coherent and  ... \n",
      "\n",
      "update: query results - 2 - document - Understanding Langchain.txt - page num - 1 - distance - 0.15398113429546356 \n",
      "update: text sample -   It has brought language models to the forefront of technology, redefining the way we interact with machines, and opening up  ... \n",
      "\n",
      "update: query results - 3 - document - Understanding Langchain.txt - page num - 1 - distance - 0.1560479825329722 \n",
      "update: text sample -   By staying informed, embracing challenges, and pushing the boundaries of what language models can achieve, you contribute to ... \n",
      "\n",
      "update: query results - 4 - document - Understanding Langchain.txt - page num - 1 - distance - 0.1706221211358058 \n",
      "update: text sample -   As the number of users and interactions grows, ensuring that your language models can process requests quickly and efficient ... \n",
      "\n",
      "update: query results - 5 - document - Understanding Langchain.txt - page num - 1 - distance - 0.1924925148487091 \n",
      "update: text sample -   Models are the sophisticated algorithms trained on vast amounts of text data to understand and generate language. They're li ... \n",
      "\n",
      "update: query results - 6 - document - Understanding Langchain.txt - page num - 1 - distance - 0.21836134791374207 \n",
      "update: text sample -  \n",
      "Welcome to the heart of your Langchain journey! In this chapter, we will dive into the exciting process of building your ver ... \n",
      "\n",
      "update: query results - 7 - document - Understanding Langchain.txt - page num - 1 - distance - 0.21949477327857653 \n",
      "update: text sample -   By the end of this chapter, you'll have a better understanding of the ethical considerations that come with creating languag ... \n",
      "\n",
      "update: query results - 8 - document - Understanding Langchain.txt - page num - 1 - distance - 0.22051304578781128 \n",
      "update: text sample -   So, keep turning the pages, as we explore the world of language models together!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 5: Fine-Tuning for Specific Ta ... \n",
      "\n",
      "update: query results - 9 - document - Understanding Langchain.txt - page num - 1 - distance - 0.22055093944072723 \n",
      "update: text sample -  \n",
      "Chapter 1: Introduction\n",
      "\n",
      "\n",
      "Welcome to the fascinating world of language models and the incredible journey that Langchain prom ... \n",
      "\n",
      "update: query results - 10 - document - Understanding Langchain.txt - page num - 1 - distance - 0.22172959017006327 \n",
      "update: text sample -  \n",
      "Addressing bias in language models involves a combination of careful dataset curation, model design, and continuous evaluati ... \n",
      "\n",
      "update: query results - 11 - document - Understanding Langchain.txt - page num - 1 - distance - 0.2263246774673462 \n",
      "update: text sample -  \n",
      "The first step in building your language model is acquiring the necessary data. Depending on your specific task, this data c ... \n",
      "\n",
      "update: query results - 12 - document - Understanding Langchain.txt - page num - 1 - distance - 0.22935190796852112 \n",
      "update: text sample -  \n",
      "Training Your Model\n",
      "\n",
      "\n",
      "The Road to Training Your Model\n",
      "\n",
      "Training a language model involves teaching it to understand and gene ... \n",
      "\n",
      "update: query results - 13 - document - Understanding Langchain.txt - page num - 1 - distance - 0.23930243351938085 \n",
      "update: text sample -   Here's why understanding and mitigating bias is crucial:\n",
      "\n",
      "​● ​Fairness: Language models should provide equitable responses t ... \n",
      "\n",
      "update: query results - 14 - document - Understanding Langchain.txt - page num - 1 - distance - 0.2429048171312219 \n",
      "update: text sample -   Content creators and marketers have integrated language models into their workflows. The process is straightforward:\n",
      "\n",
      "1. Spe ... \n",
      "\n",
      "update: query results - 15 - document - Understanding Langchain.txt - page num - 1 - distance - 0.24378910660743713 \n",
      "update: text sample -  \n",
      "- Community Resources: Leverage the Langchain community to access resources, best practices, and advice from fellow develope ... \n",
      "\n",
      "update: query results - 16 - document - Understanding Langchain.txt - page num - 1 - distance - 0.2454177737236023 \n",
      "update: text sample -  \n",
      "5. Content Summarization: These models can distill lengthy documents into concise summaries, saving time and improving infor ... \n",
      "\n",
      "update: query results - 17 - document - Understanding Langchain.txt - page num - 1 - distance - 0.2498714029788971 \n",
      "update: text sample -   Training data is the raw material that these models consume to understand language patterns, grammar, and context. It's the  ... \n",
      "\n",
      "update: query results - 18 - document - Understanding Langchain.txt - page num - 1 - distance - 0.2504500740954602 \n",
      "update: text sample -  \n",
      "Here's a simplified example of how you can create an SDK for your language model:\n",
      "\n",
      "```\n",
      "\n",
      "class YourModelSDK:\n",
      "\n",
      "def __init__(se ... \n",
      "\n",
      "update: query results - 19 - document - Understanding Langchain.txt - page num - 1 - distance - 0.2516032381862061 \n",
      "update: text sample -  \n",
      "Language models have made significant strides, but their journey is far from over. Here are some key future directions that  ... \n",
      "\n",
      "update:  embedding record -  [{'embedding_status': 'yes', 'embedding_model': 'mini-lm-sbert', 'embedding_db': 'chromadb', 'embedding_dims': 384, 'embedded_blocks': 5064, 'time_stamp': '2024-05-17_194246'}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #   Fast Start configuration - will use no-install embedded sqlite\n",
    "    #   -- if you have installed Mongo or Postgres, then change the .set_active_db accordingly\n",
    "    LLMWareConfig().set_active_db(\"sqlite\")\n",
    "\n",
    "    #   note: as of llmware==0.2.12, we have shifted from faiss to chromadb for the Fast Start examples\n",
    "    #   --if you are using a Python version before 3.12, please feel free to substitute for \"faiss\"\n",
    "    #   --for versions of Python >= 3.12, for the Fast Start examples (e.g., no install required), we\n",
    "    #   recommend using chromadb or lancedb\n",
    "    #   please double-check: `pip3 install chromadb` or pull the latest llmware version to get automatically\n",
    "\n",
    "    #   -- if you have installed any other vector db, just change the name, e.g, \"milvus\" or \"pg_vector\"\n",
    "    LLMWareConfig().set_vector_db(\"chromadb\")\n",
    "\n",
    "    #  Step 1 - this example requires us to have a library created - two options:\n",
    "\n",
    "    #  if you completed example-1 - then load the library you created in that example, e.g., \"example1_library\"\n",
    "    #  uncomment the line below:\n",
    "    # library = Library().load_library(\"example1_library\")\n",
    "    # output = parsing_documents_into_library(library_name, selected_folder)\n",
    "\n",
    "    #  alternatively, to use this example as self-contained, then create a new library from scratch:\n",
    "    library = setup_library(\"example2_library\")\n",
    "\n",
    "    #   Step 2 - Select any embedding model in the LLMWare catalog\n",
    "\n",
    "    #   to see a list of the embedding models supported, uncomment the line below and print the list\n",
    "    embedding_models = ModelCatalog().list_embedding_models()\n",
    "\n",
    "    #   for i, models in enumerate(embedding_models):\n",
    "    #       print(\"embedding models: \", i, models)\n",
    "\n",
    "    #   for this first embedding, we will use a very popular and fast sentence transformer\n",
    "    embedding_model = \"mini-lm-sbert\"\n",
    "\n",
    "    #   note: if you want to swap out \"mini-lm-sbert\" for Open AI 'text-embedding-ada-002', uncomment these lines:\n",
    "    #   embedding_model = \"text-embedding-ada-002\"\n",
    "    #   os.environ[\"USER_MANAGED_OPENAI_API_KEY\"] = \"<insert-your-openai-api-key>\"\n",
    "\n",
    "    #   run the core script\n",
    "    install_vector_embeddings(library, embedding_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
