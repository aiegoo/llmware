{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "     Fast Start Example #4 - RAG with Text Query<br>\n",
    "    This example shows a basic RAG recipe using text query combined with LLM prompt.<br>\n",
    "    We will show two different ways to achieve this basic recipe:<br>\n",
    "    -- Example 4A - this will integrate Library + Prompt - and is the most scalable general solution<br>\n",
    "    -- Example 4B - this will illustrate another capability of the Prompt class to add sources \"inline\"<br>\n",
    "     without necessarily a library in-place.  It is another useful tool when you want to be able to quickly<br>\n",
    "     pick up a document and start asking questions to it.<br>\n",
    "     Note: both of the examples are designed to achieve the same output.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from llmware.prompts import Prompt, HumanInTheLoop\n",
    "from llmware.setup import Setup\n",
    "from llmware.configs import LLMWareConfig\n",
    "from llmware.retrieval import Query\n",
    "from llmware.library import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'YOUR_API_KEY' with your actual Hugging Face API key\n",
    "os.environ[\"HF_HOME\"] = \"/root/.cache/huggingface\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/.huggingface\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/.huggingface\"\n",
    "os.environ[\"USER_HOME\"] = \"/root\"\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_4a_contract_analysis_from_library (model_name, verbose=False):\n",
    "    \"\"\" Example #4a:  Main general case to run a RAG workflow from a Library \"\"\"\n",
    "\n",
    "    # Load the llmware sample files\n",
    "    print (f\"\\n > Loading the llmware sample files...\")\n",
    "    sample_files_path = Setup().load_sample_files()\n",
    "    contracts_path = os.path.join(sample_files_path,\"Agreements\")\n",
    "    contracts_lib = Library().create_new_library(\"example4_library\")\n",
    "    contracts_lib.add_files(contracts_path)\n",
    "\n",
    "    # questions that we want to ask each contract\n",
    "    question_list = [{\"topic\": \"executive employment agreement\", \"llm_query\": \"What are the names of the two parties?\"},\n",
    "                     {\"topic\": \"base salary\", \"llm_query\": \"What is the executive's base salary?\"},\n",
    "                     {\"topic\": \"governing law\", \"llm_query\": \"What is the governing law?\"}]\n",
    "    print (f\"\\n > Loading model {model_name}...\")\n",
    "    q = Query(contracts_lib)\n",
    "\n",
    "    # get a list of all of the unique documents in the library\n",
    "\n",
    "    # doc id list\n",
    "    doc_list = q.list_doc_id()\n",
    "    print(\"update: document id list - \", doc_list)\n",
    "\n",
    "    # filename list\n",
    "    fn_list = q.list_doc_fn()\n",
    "    print(\"update: filename list - \", fn_list)\n",
    "    prompter = Prompt().load_model(model_name)\n",
    "    for i, doc_id in enumerate(doc_list):\n",
    "        print(\"\\nAnalyzing contract: \", str(i+1), doc_id, fn_list[i])\n",
    "        print(\"LLM Responses:\")\n",
    "        for question in question_list:\n",
    "            query_topic = question[\"topic\"]\n",
    "            llm_question = question[\"llm_query\"]\n",
    "            doc_filter = {\"doc_ID\": [doc_id]}\n",
    "            query_results = q.text_query_with_document_filter(query_topic,doc_filter,result_count=5,exact_mode=True)\n",
    "            if verbose:\n",
    "                # this will display the query results from the query above\n",
    "                for j, qr in enumerate(query_results):\n",
    "                    print(\"update: querying document - \", query_topic, j, doc_filter, qr)\n",
    "            source = prompter.add_source_query_results(query_results)\n",
    "\n",
    "            #   *** this is the call to the llm with the source packaged in the context automatically ***\n",
    "            responses = prompter.prompt_with_source(llm_question, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "            #   unpacking the results from the LLM\n",
    "            for r, response in enumerate(responses):\n",
    "                print(\"update: llm response -  \", llm_question, re.sub(\"[\\n]\",\" \", response[\"llm_response\"]).strip())\n",
    "\n",
    "            # We're done with this contract, clear the source from the prompt\n",
    "            prompter.clear_source_materials()\n",
    "\n",
    "    #   Save jsonl report to jsonl to /prompt_history folder\n",
    "    print(\"\\nPrompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(),prompter.prompt_id))\n",
    "    prompter.save_state()\n",
    "\n",
    "    #   Save csv report that includes the model, response, prompt, and evidence for human-in-the-loop review\n",
    "    csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n",
    "    print(\"\\nCSV output saved at:  \", csv_output)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > Loading the llmware sample files...\n",
      "\n",
      " > Loading model llmware/bling-1b-0.1...\n",
      "update: document id list -  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "update: filename list -  ['Amphitrite EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Aphrodite EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Apollo EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Artemis Poseidon EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Athena EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Bia EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Demeter EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Eileithyia EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Gaia EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Leto EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Metis EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Nike EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Nyx EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Persephone EXECUTIVE EMPLOYMENT AGREEMENT.pdf', 'Rhea EXECUTIVE EMPLOYMENT AGREEMENT.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsyyu\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0083cddc5ca940c5a7e6c5cbeae3873a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsyyu\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hsyyu\\.cache\\huggingface\\hub\\models--llmware--bling-1b-0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\hsyyu\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb24339e238497b985bfb7354f70d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c259481a94694011aa81e25d5a9a99fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing contract:  1 1 Amphitrite EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Amphitrite Ares and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $5,000,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  2 2 Aphrodite EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Aphrodite Apollo and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $600,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  3 3 Apollo EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Aphrodite Apollo and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $600,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  4 4 Artemis Poseidon EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? TestCo Software, Inc. and Artemus Poseidon\n",
      "update: llm response -   What is the executive's base salary? $400,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  5 5 Athena EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Athena Zeus and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $500,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  6 6 Bia EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Bia Hermes and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $400,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  7 7 Demeter EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? TestCo Software, Inc. and Executive\n",
      "update: llm response -   What is the executive's base salary? $300,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  8 8 Eileithyia EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? TestCo Software, Inc. and Eileithyia Hades\n",
      "update: llm response -   What is the executive's base salary? $300,000\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  9 9 Gaia EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Gaia Eros and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $250,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  10 10 Leto EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Leto Apollo and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $200,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  11 11 Metis EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Metis Hades and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $200,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  12 12 Nike EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Nike Cronus and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $200,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  13 13 Nyx EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Nyx Pan and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $200,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  14 14 Persephone EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Persephone Zeus and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $200,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Analyzing contract:  15 15 Rhea EXECUTIVE EMPLOYMENT AGREEMENT.pdf\n",
      "LLM Responses:\n",
      "update: llm response -   What are the names of the two parties? Rhea Hecate and TestCo Software, Inc.\n",
      "update: llm response -   What is the executive's base salary? $4350,000.\n",
      "update: llm response -   What is the governing law? State of Massachusetts\n",
      "\n",
      "Prompt state saved at:  C:\\Users\\hsyyu\\llmware_data\\prompt_history\\38f65749-726d-4868-a5c5-c8ca2a5b08a6\n",
      "\n",
      "CSV output saved at:   {'report_name': 'interaction_report_2024-05-14_133850.csv', 'report_fp': 'C:\\\\Users\\\\hsyyu\\\\llmware_data\\\\prompt_history\\\\interaction_report_2024-05-14_133850.csv', 'results': 45}\n"
     ]
    }
   ],
   "source": [
    "def example_4b_contract_analysis_direct_from_prompt(model_name, verbose=False):\n",
    "    \"\"\" Example #4b: Alternative implementation using prompt in-line capabilities without using a library \"\"\"\n",
    "\n",
    "    # Load the llmware sample files\n",
    "    print(f\"\\n > Loading the llmware sample files...\")\n",
    "    sample_files_path = Setup().load_sample_files()\n",
    "    contracts_path = os.path.join(sample_files_path, \"Agreements\")\n",
    "\n",
    "    # questions that we want to ask each contract\n",
    "    question_list = [{\"topic\": \"executive employment agreement\", \"llm_query\": \"What are the names of the two parties?\"},\n",
    "                     {\"topic\": \"base salary\", \"llm_query\": \"What is the executive's base salary?\"},\n",
    "                     {\"topic\": \"governing law\", \"llm_query\": \"What is the governing law?\"}]\n",
    "    print(f\"\\n > Loading model {model_name}...\")\n",
    "    prompter = Prompt().load_model(model_name)\n",
    "    for i, contract in enumerate(os.listdir(contracts_path)):\n",
    "\n",
    "        # exclude potential mac os created file artifact in the samples folder path\n",
    "        if contract != \".DS_Store\":\n",
    "            print(\"\\nAnalyzing contract: \", str(i + 1), contract)\n",
    "            print(\"LLM Responses:\")\n",
    "            for question in question_list:\n",
    "                query_topic = question[\"topic\"]\n",
    "                llm_question = question[\"llm_query\"]\n",
    "                #   introducing \"add_source_document\"\n",
    "                #   this will perform 'inline' parsing, text chunking and query filter on a document\n",
    "                #   input is a file folder path, file name, and an optional query filter\n",
    "                #   the source is automatically packaged into the prompt object\n",
    "                source = prompter.add_source_document(contracts_path,contract,query=query_topic)\n",
    "                if verbose:\n",
    "                    print(\"update: document created source - \", source)\n",
    "                #   calling the LLM with 'source' information from the contract automatically packaged into the prompt\n",
    "                responses = prompter.prompt_with_source(llm_question, prompt_name=\"default_with_context\",\n",
    "                                                        temperature=0.3)\n",
    "                #   unpacking the LLM responses\n",
    "                for r, response in enumerate(responses):\n",
    "                    print(\"update: llm response: \", llm_question, re.sub(\"[\\n]\", \" \",\n",
    "                                                                         response[\"llm_response\"]).strip())\n",
    "                # We're done with this contract, clear the source from the prompt\n",
    "                prompter.clear_source_materials()\n",
    "\n",
    "    # Save jsonl report to jsonl to /prompt_history folder\n",
    "    print(\"\\nupdate: Prompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(), prompter.prompt_id))\n",
    "    prompter.save_state()\n",
    "\n",
    "    # Save csv report that includes the model, response, prompt, and evidence for human-in-the-loop review\n",
    "    csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n",
    "    print(\"\\nupdate: CSV output saved at - \", csv_output)\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #   you can pick any model from the ModelCatalog\n",
    "    #   we list a few representative good choices below\n",
    "\n",
    "    LLMWareConfig().set_active_db(\"sqlite\")\n",
    "\n",
    "    example_models = [\"llmware/bling-1b-0.1\", \"llmware/bling-tiny-llama-v0\", \"llmware/dragon-yi-6b-gguf\"]\n",
    "\n",
    "    #   to swap in a gpt-4 openai model - uncomment these two lines\n",
    "    #   model_name = \"gpt-4\"\n",
    "    #   os.environ[\"USER_MANAGED_OPENAI_API_KEY\"] = \"<insert-your-openai-key>\"\n",
    "\n",
    "    # use local cpu model\n",
    "    model_name = example_models[0]\n",
    "\n",
    "    #   two good recipes to address the use case\n",
    "\n",
    "    #   first let's look at the main way of retrieving and analyzing from a library\n",
    "    example_4a_contract_analysis_from_library(model_name)\n",
    "\n",
    "    #   second - uncomment this line, and lets run the \"in-line\" prompt way\n",
    "    # example_4b_contract_analysis_direct_from_prompt(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "to run this example with chromadb, you need to install the chromadb python sdk:  pip3 install chromadb\n",
      "\n",
      "update: Step 1 - Creating library: example_5_library\n",
      "update: Step 2 - Downloading Sample Files\n",
      "update: Step 3 - Parsing and Text Indexing Files\n",
      "\n",
      "update: Step 4 - Generating Embeddings in chromadb db - with Model- industry-bert-contracts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e454a461b06546afaf8164405d92adab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/809 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsyyu\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hsyyu\\.cache\\huggingface\\hub\\models--llmware--industry-bert-contracts-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21b008ea4f3494690dad27b105e878c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef33b80df45408ca10eb122a81e6c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DependencyNotInstalledException",
     "evalue": "'pip3 install chromadb' needs to be installed to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyNotInstalledException\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 141\u001b[0m\n\u001b[0;32m    135\u001b[0m llm_model_name \u001b[38;5;241m=\u001b[39m example_models[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m#   to swap in a gpt-4 openai model - uncomment these two lines\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m#   llm_model_name = \"gpt-4\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m#   os.environ[\"USER_MANAGED_OPENAI_API_KEY\"] = \"<insert-your-openai-key>\"\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m semantic_rag(lib_name, embedding_model, llm_model_name)\n",
      "Cell \u001b[1;32mIn[6], line 55\u001b[0m, in \u001b[0;36msemantic_rag\u001b[1;34m(library_name, embedding_model_name, llm_model_name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Step 4 - Install the embeddings\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mupdate: Step 4 - Generating Embeddings in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m db - with Model- \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(vector_db, embedding_model))\n\u001b[1;32m---> 55\u001b[0m library\u001b[38;5;241m.\u001b[39minstall_new_embedding(embedding_model_name\u001b[38;5;241m=\u001b[39membedding_model_name, vector_db\u001b[38;5;241m=\u001b[39mvector_db)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# RAG steps start here ...\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mupdate: Loading model for LLM inference - \u001b[39m\u001b[38;5;124m\"\u001b[39m, llm_model_name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llmware\\library.py:627\u001b[0m, in \u001b[0;36mLibrary.install_new_embedding\u001b[1;34m(self, embedding_model_name, vector_db, from_hf, from_sentence_transformer, model, tokenizer, model_api_key, vector_db_api_key, batch_size, max_len, use_gpu)\u001b[0m\n\u001b[0;32m    624\u001b[0m     my_model\u001b[38;5;241m.\u001b[39mmax_len \u001b[38;5;241m=\u001b[39m max_len\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# step 2 - pass loaded embedding model to EmbeddingHandler, which will route to the appropriate resource\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m EmbeddingHandler(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreate_new_embedding(vector_db, my_model, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embeddings:\n\u001b[0;32m    630\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarning: no embeddings created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llmware\\embeddings.py:130\u001b[0m, in \u001b[0;36mEmbeddingHandler.create_new_embedding\u001b[1;34m(self, embedding_db, model, doc_ids, batch_size)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_new_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedding_db, model, doc_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Creates new embedding - routes to correct vector db and loads the model and text collection \"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     embedding_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_embedding_db(embedding_db, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m    131\u001b[0m     embedding_status \u001b[38;5;241m=\u001b[39m embedding_class\u001b[38;5;241m.\u001b[39mcreate_new_embedding(doc_ids, batch_size)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embedding_status:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llmware\\embeddings.py:222\u001b[0m, in \u001b[0;36mEmbeddingHandler._load_embedding_db\u001b[1;34m(self, embedding_db, model, model_name, embedding_dims)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EmbeddingNeo4j(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary, model\u001b[38;5;241m=\u001b[39mmodel, model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    219\u001b[0m                            embedding_dims\u001b[38;5;241m=\u001b[39membedding_dims)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding_db \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchromadb\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EmbeddingChromaDB(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary, model\u001b[38;5;241m=\u001b[39mmodel, model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    223\u001b[0m                            embedding_dims\u001b[38;5;241m=\u001b[39membedding_dims)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llmware\\embeddings.py:2416\u001b[0m, in \u001b[0;36mEmbeddingChromaDB.__init__\u001b[1;34m(self, library, model, model_name, embedding_dims)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;66;03m# Instantiate client.\u001b[39;00m\n\u001b[0;32m   2415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m util\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchromadb\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 2416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalledException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip3 install chromadb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m persistent_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2419\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mEphemeralClient()\n",
      "\u001b[1;31mDependencyNotInstalledException\u001b[0m: 'pip3 install chromadb' needs to be installed to use this function."
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"     Fast Start Example #5 - RAG with Semantic Query\n",
    "\n",
    "    This example illustrates the most common RAG retrieval pattern, which is using a semantic query, e.g.,\n",
    "    a natural language query, as the basis for retrieving relevant text chunks, and then using as\n",
    "    the context material in a prompt to ask the same question to a LLM.\n",
    "\n",
    "    In this example, we will show the following:\n",
    "\n",
    "    1.  Create library and install embeddings (feel free to skip / substitute a library created in an earlier step).\n",
    "    2.  Ask a general semantic query to the entire library collection.\n",
    "    3.  Select the most relevant results by document.\n",
    "    4.  Loop through all of the documents - packaging the context and asking our questions to the LLM.\n",
    "\n",
    "    NOTE: to use chromadb, you may need to install the python sdk:  pip3 install chromadb.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from llmware.status import Status\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.configs import LLMWareConfig\n",
    "from importlib import util\n",
    "\n",
    "if not util.find_spec(\"chromadb\"):\n",
    "    print(\"\\nto run this example with chromadb, you need to install the chromadb python sdk:  pip3 install chromadb\")\n",
    "\n",
    "\n",
    "def semantic_rag (library_name, embedding_model_name, llm_model_name):\n",
    "\n",
    "    \"\"\" Illustrates the use of semantic embedding vectors in a RAG workflow\n",
    "        --self-contained example - will be duplicative with some of the steps taken in other examples \"\"\"\n",
    "\n",
    "    # Step 1 - Create library which is the main 'organizing construct' in llmware\n",
    "    print (\"\\nupdate: Step 1 - Creating library: {}\".format(library_name))\n",
    "\n",
    "    library = Library().create_new_library(library_name)\n",
    "\n",
    "    # Step 2 - Pull down the sample files from S3 through the .load_sample_files() command\n",
    "    #   --note: if you need to refresh the sample files, set 'over_write=True'\n",
    "    print (\"update: Step 2 - Downloading Sample Files\")\n",
    "\n",
    "    sample_files_path = Setup().load_sample_files(over_write=False)\n",
    "    contracts_path = os.path.join(sample_files_path, \"Agreements\")\n",
    "\n",
    "    # Step 3 - point \".add_files\" method to the folder of documents that was just created\n",
    "    #   this method parses all of the documents, text chunks, and captures in MongoDB\n",
    "    print(\"update: Step 3 - Parsing and Text Indexing Files\")\n",
    "\n",
    "    library.add_files(input_folder_path=contracts_path, chunk_size=400, max_chunk_size=600,\n",
    "                      smart_chunking=1)\n",
    "\n",
    "    # Step 4 - Install the embeddings\n",
    "    print(\"\\nupdate: Step 4 - Generating Embeddings in {} db - with Model- {}\".format(vector_db, embedding_model))\n",
    "\n",
    "    library.install_new_embedding(embedding_model_name=embedding_model_name, vector_db=vector_db)\n",
    "\n",
    "    # RAG steps start here ...\n",
    "\n",
    "    print(\"\\nupdate: Loading model for LLM inference - \", llm_model_name)\n",
    "\n",
    "    prompter = Prompt().load_model(llm_model_name)\n",
    "\n",
    "    query = \"what is the executive's base annual salary\"\n",
    "\n",
    "    #   key step: run semantic query against the library and get all of the top results\n",
    "    results = Query(library).semantic_query(query, result_count=50, embedding_distance_threshold=1.0)\n",
    "\n",
    "    #   if you want to look at 'results', uncomment the two lines below\n",
    "    #   for i, res in enumerate(results):\n",
    "    #       print(\"update: \", i, res[\"file_source\"], res[\"distance\"], res[\"text\"])\n",
    "\n",
    "    for i, contract in enumerate(os.listdir(contracts_path)):\n",
    "\n",
    "        qr = []\n",
    "\n",
    "        if contract != \".DS_Store\":\n",
    "\n",
    "            print(\"\\nContract Name: \", i, contract)\n",
    "\n",
    "            #   we will look through the list of semantic query results, and pull the top results for each file\n",
    "            for j, entries in enumerate(results):\n",
    "\n",
    "                library_fn = entries[\"file_source\"]\n",
    "                if os.sep in library_fn:\n",
    "                    # handles difference in windows file formats vs. mac / linux\n",
    "                    library_fn = library_fn.split(os.sep)[-1]\n",
    "\n",
    "                if library_fn == contract:\n",
    "                    print(\"Top Retrieval: \", j, entries[\"distance\"], entries[\"text\"])\n",
    "                    qr.append(entries)\n",
    "\n",
    "            #   we will add the query results to the prompt\n",
    "            source = prompter.add_source_query_results(query_results=qr)\n",
    "\n",
    "            #   run the prompt\n",
    "            response = prompter.prompt_with_source(query, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "            #   note: prompt_with_resource returns a list of dictionary responses\n",
    "            #   -- depending upon the size of the source context, it may call the llm several times\n",
    "            #   -- each dict entry represents 1 call to the LLM\n",
    "\n",
    "            for resp in response:\n",
    "                if \"llm_response\" in resp:\n",
    "                    print(\"\\nupdate: llm answer - \", resp[\"llm_response\"])\n",
    "\n",
    "            # start fresh for next document\n",
    "            prompter.clear_source_materials()\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    LLMWareConfig().set_active_db(\"sqlite\")\n",
    "\n",
    "    #   for this example, we will use an embedding model that has been 'fine-tuned' for contracts\n",
    "    embedding_model = \"industry-bert-contracts\"\n",
    "\n",
    "    #   note: as of llmware==0.2.12, we have shifted from faiss to chromadb for the Fast Start examples\n",
    "    #   --if you are using a Python version before 3.12, please feel free to substitute for \"faiss\"\n",
    "    #   --for versions of Python >= 3.12, for the Fast Start examples (e.g., no install required), we\n",
    "    #   recommend using chromadb or lancedb\n",
    "\n",
    "    #   please double-check: `pip3 install chromadb` or pull the latest llmware version to get automatically\n",
    "    #   -- if you have installed any other vector db, just change the name, e.g, \"milvus\" or \"pg_vector\"\n",
    "\n",
    "    vector_db = \"chromadb\"\n",
    "\n",
    "    # pick any name for the library\n",
    "    lib_name = \"example_5_library\"\n",
    "\n",
    "    example_models = [\"llmware/bling-1b-0.1\", \"llmware/bling-tiny-llama-v0\", \"llmware/dragon-yi-6b-gguf\"]\n",
    "\n",
    "    # use local cpu model\n",
    "    llm_model_name = example_models[0]\n",
    "\n",
    "    #   to swap in a gpt-4 openai model - uncomment these two lines\n",
    "    #   llm_model_name = \"gpt-4\"\n",
    "    #   os.environ[\"USER_MANAGED_OPENAI_API_KEY\"] = \"<insert-your-openai-key>\"\n",
    "\n",
    "    semantic_rag(lib_name, embedding_model, llm_model_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
