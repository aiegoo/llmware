{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "     Fast Start Example #4 - RAG with Text Query<br>\n",
    "    This example shows a basic RAG recipe using text query combined with LLM prompt.<br>\n",
    "    We will show two different ways to achieve this basic recipe:<br>\n",
    "    -- Example 4A - this will integrate Library + Prompt - and is the most scalable general solution<br>\n",
    "    -- Example 4B - this will illustrate another capability of the Prompt class to add sources \"inline\"<br>\n",
    "     without necessarily a library in-place.  It is another useful tool when you want to be able to quickly<br>\n",
    "     pick up a document and start asking questions to it.<br>\n",
    "     Note: both of the examples are designed to achieve the same output.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from llmware.prompts import Prompt, HumanInTheLoop\n",
    "from llmware.setup import Setup\n",
    "from llmware.configs import LLMWareConfig\n",
    "from llmware.retrieval import Query\n",
    "from llmware.library import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'YOUR_API_KEY' with your actual Hugging Face API key\n",
    "os.environ[\"HF_HOME\"] = \"/root/.cache/huggingface\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/.huggingface\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/.huggingface\"\n",
    "os.environ[\"USER_HOME\"] = \"/root\"\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_4a_contract_analysis_from_library (model_name, verbose=False):\n",
    "    \"\"\" Example #4a:  Main general case to run a RAG workflow from a Library \"\"\"\n",
    "\n",
    "    # Load the llmware sample files\n",
    "    print (f\"\\n > Loading the llmware sample files...\")\n",
    "    sample_files_path = Setup().load_sample_files()\n",
    "    contracts_path = os.path.join(sample_files_path,\"Agreements\")\n",
    "    contracts_lib = Library().create_new_library(\"example4_library\")\n",
    "    contracts_lib.add_files(contracts_path)\n",
    "\n",
    "    # questions that we want to ask each contract\n",
    "    question_list = [{\"topic\": \"executive employment agreement\", \"llm_query\": \"What are the names of the two parties?\"},\n",
    "                     {\"topic\": \"base salary\", \"llm_query\": \"What is the executive's base salary?\"},\n",
    "                     {\"topic\": \"governing law\", \"llm_query\": \"What is the governing law?\"}]\n",
    "    print (f\"\\n > Loading model {model_name}...\")\n",
    "    q = Query(contracts_lib)\n",
    "\n",
    "    # get a list of all of the unique documents in the library\n",
    "\n",
    "    # doc id list\n",
    "    doc_list = q.list_doc_id()\n",
    "    print(\"update: document id list - \", doc_list)\n",
    "\n",
    "    # filename list\n",
    "    fn_list = q.list_doc_fn()\n",
    "    print(\"update: filename list - \", fn_list)\n",
    "    prompter = Prompt().load_model(model_name)\n",
    "    for i, doc_id in enumerate(doc_list):\n",
    "        print(\"\\nAnalyzing contract: \", str(i+1), doc_id, fn_list[i])\n",
    "        print(\"LLM Responses:\")\n",
    "        for question in question_list:\n",
    "            query_topic = question[\"topic\"]\n",
    "            llm_question = question[\"llm_query\"]\n",
    "            doc_filter = {\"doc_ID\": [doc_id]}\n",
    "            query_results = q.text_query_with_document_filter(query_topic,doc_filter,result_count=5,exact_mode=True)\n",
    "            if verbose:\n",
    "                # this will display the query results from the query above\n",
    "                for j, qr in enumerate(query_results):\n",
    "                    print(\"update: querying document - \", query_topic, j, doc_filter, qr)\n",
    "            source = prompter.add_source_query_results(query_results)\n",
    "\n",
    "            #   *** this is the call to the llm with the source packaged in the context automatically ***\n",
    "            responses = prompter.prompt_with_source(llm_question, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "            #   unpacking the results from the LLM\n",
    "            for r, response in enumerate(responses):\n",
    "                print(\"update: llm response -  \", llm_question, re.sub(\"[\\n]\",\" \", response[\"llm_response\"]).strip())\n",
    "\n",
    "            # We're done with this contract, clear the source from the prompt\n",
    "            prompter.clear_source_materials()\n",
    "\n",
    "    #   Save jsonl report to jsonl to /prompt_history folder\n",
    "    print(\"\\nPrompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(),prompter.prompt_id))\n",
    "    prompter.save_state()\n",
    "\n",
    "    #   Save csv report that includes the model, response, prompt, and evidence for human-in-the-loop review\n",
    "    csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n",
    "    print(\"\\nCSV output saved at:  \", csv_output)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_4b_contract_analysis_direct_from_prompt(model_name, verbose=False):\n",
    "    \"\"\" Example #4b: Alternative implementation using prompt in-line capabilities without using a library \"\"\"\n",
    "\n",
    "    # Load the llmware sample files\n",
    "    print(f\"\\n > Loading the llmware sample files...\")\n",
    "    sample_files_path = Setup().load_sample_files()\n",
    "    contracts_path = os.path.join(sample_files_path, \"Agreements\")\n",
    "\n",
    "    # questions that we want to ask each contract\n",
    "    question_list = [{\"topic\": \"executive employment agreement\", \"llm_query\": \"What are the names of the two parties?\"},\n",
    "                     {\"topic\": \"base salary\", \"llm_query\": \"What is the executive's base salary?\"},\n",
    "                     {\"topic\": \"governing law\", \"llm_query\": \"What is the governing law?\"}]\n",
    "    print(f\"\\n > Loading model {model_name}...\")\n",
    "    prompter = Prompt().load_model(model_name)\n",
    "    for i, contract in enumerate(os.listdir(contracts_path)):\n",
    "\n",
    "        # exclude potential mac os created file artifact in the samples folder path\n",
    "        if contract != \".DS_Store\":\n",
    "            print(\"\\nAnalyzing contract: \", str(i + 1), contract)\n",
    "            print(\"LLM Responses:\")\n",
    "            for question in question_list:\n",
    "                query_topic = question[\"topic\"]\n",
    "                llm_question = question[\"llm_query\"]\n",
    "                #   introducing \"add_source_document\"\n",
    "                #   this will perform 'inline' parsing, text chunking and query filter on a document\n",
    "                #   input is a file folder path, file name, and an optional query filter\n",
    "                #   the source is automatically packaged into the prompt object\n",
    "                source = prompter.add_source_document(contracts_path,contract,query=query_topic)\n",
    "                if verbose:\n",
    "                    print(\"update: document created source - \", source)\n",
    "                #   calling the LLM with 'source' information from the contract automatically packaged into the prompt\n",
    "                responses = prompter.prompt_with_source(llm_question, prompt_name=\"default_with_context\",\n",
    "                                                        temperature=0.3)\n",
    "                #   unpacking the LLM responses\n",
    "                for r, response in enumerate(responses):\n",
    "                    print(\"update: llm response: \", llm_question, re.sub(\"[\\n]\", \" \",\n",
    "                                                                         response[\"llm_response\"]).strip())\n",
    "                # We're done with this contract, clear the source from the prompt\n",
    "                prompter.clear_source_materials()\n",
    "\n",
    "    # Save jsonl report to jsonl to /prompt_history folder\n",
    "    print(\"\\nupdate: Prompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(), prompter.prompt_id))\n",
    "    prompter.save_state()\n",
    "\n",
    "    # Save csv report that includes the model, response, prompt, and evidence for human-in-the-loop review\n",
    "    csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n",
    "    print(\"\\nupdate: CSV output saved at - \", csv_output)\n",
    "    return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
