{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "     Fast Start Example #6 - RAG - Beyond the Basics<br>\n",
    "    This example builds upon examples #4 and #5 and demonstrates how to layer additional elements to<br>\n",
    "    improve the effectiveness of a RAG workflow over a larger set of documents-<br>\n",
    "    1.  Apply an initial filter across a batch of documents to identify a subset of documents of interest<br>\n",
    "    2.  Analyze the documents of interest to identify key provisions<br>\n",
    "    3.  Use fact-checking and post-processing to validate the accuracy of the LLM response<br>\n",
    "    4.  Write the output to JSON and CSV files for follow-up review and/or the next step in the workflow.<br>\n",
    "    For this example, we also recommend using a more sophisticated DRAGON model in GGUF format, which enables us<br>\n",
    "    to run 6-7B parameter models locally.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmware.setup import Setup\n",
    "from llmware.library import Library\n",
    "from llmware.prompts import Prompt, HumanInTheLoop\n",
    "from llmware.retrieval import Query\n",
    "from llmware.configs import LLMWareConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msa_processing(library_name, llm_model_name):\n",
    "    \"\"\" In this example, we will use the 'AgreementsLarge' sample files which consists of ~80 contracts.  We\n",
    "    need to quickly identify the 'master service agreements' as we only want to analyze those contracts. \"\"\"\n",
    "    local_path = Setup().load_sample_files()\n",
    "    agreements_path = os.path.join(local_path, \"AgreementsLarge\")\n",
    "\n",
    "    #   create a library with all of the Agreements (~80 contracts)\n",
    "    print(f\"\\nStarting:  Parsing 'AgreementsLarge' Folder\")\n",
    "    msa_lib = Library().create_new_library(library_name)\n",
    "    msa_lib.add_files(agreements_path)\n",
    "\n",
    "    #   find the \"master service agreements\" (MSA) - we know that 'master services agreement' will always\n",
    "    #   be on the first page of the agreement, so we can use that as a good proxy for automatically filtering\n",
    "    #   to our target set of documents\n",
    "    print(f\"\\nCompleted Parsing - now, let's look for the 'master service agreements', e.g., 'msa'\")\n",
    "    q = Query(msa_lib)\n",
    "    query = '\"master services agreement\"'\n",
    "    results = q.text_search_by_page(query, page_num=1, results_only=False)\n",
    "\n",
    "    #   results_only = False will return a dictionary with 4 keys:  {\"query\", \"results\", \"doc_ID\", \"file_source\"}\n",
    "    msa_docs = results[\"file_source\"]\n",
    "    msa_doc_ids = results[\"doc_ID\"]\n",
    "\n",
    "    #   load prompt/llm locally\n",
    "    prompter = Prompt().load_model(llm_model_name)\n",
    "    print(\"update: identified the following msa doc id: \", msa_doc_ids)\n",
    "\n",
    "    #   analyze each MSA - \"query\" & \"llm prompt\"\n",
    "    for i, doc_id in enumerate(msa_doc_ids):\n",
    "        print(\"\\n\")\n",
    "        docs = msa_docs[i]\n",
    "        if os.sep in docs:\n",
    "            # handles difference in windows file formats vs. Mac/Linux\n",
    "            docs = docs.split(os.sep)[-1]\n",
    "        print (i+1, \"Reviewing MSA - \", doc_id, docs)\n",
    "\n",
    "        #   look for the termination provisions in each document\n",
    "        doc_filter = {\"doc_ID\": [doc_id]}\n",
    "        termination_provisions = q.text_query_with_document_filter(\"termination\", doc_filter)\n",
    "\n",
    "        #   package the provisions as a source to a prompt\n",
    "        sources = prompter.add_source_query_results(termination_provisions)\n",
    "\n",
    "        #   if you want to see more details about how the sources are packaged: uncomment this line-\n",
    "        #   print(\"update: sources - \", sources)\n",
    "\n",
    "        #   call the LLM and ask our question\n",
    "        response = prompter.prompt_with_source(\"What is the notice for termination for convenience?\")\n",
    "\n",
    "        #   post processing fact checking\n",
    "        stats = prompter.evidence_comparison_stats(response)\n",
    "        ev_source = prompter.evidence_check_sources(response)\n",
    "        for i, resp in enumerate(response):\n",
    "            print(\"update: llm response - \", resp)\n",
    "            print(\"update: compare with evidence- \", stats[i][\"comparison_stats\"])\n",
    "            print(\"update: sources - \", ev_source[i][\"source_review\"])\n",
    "        prompter.clear_source_materials()\n",
    "\n",
    "    # Save jsonl report with full transaction history to /prompt_history folder\n",
    "    print(\"\\nupdate: Prompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(),prompter.prompt_id))\n",
    "    prompter.save_state()\n",
    "\n",
    "    # Generate CSV report for easy Human review in Excel\n",
    "    csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n",
    "    print(\"\\nupdate: CSV output for human review - \", csv_output)\n",
    "    return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
