{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "     Fast Start Example #5 - RAG with Semantic Query<br>\n",
    "    This example illustrates the most common RAG retrieval pattern, which is using a semantic query, e.g.,<br>\n",
    "    a natural language query, as the basis for retrieving relevant text chunks, and then using as<br>\n",
    "    the context material in a prompt to ask the same question to a LLM.<br>\n",
    "    In this example, we will show the following:<br>\n",
    "    1.  Create library and install embeddings (feel free to skip / substitute a library created in an earlier step).<br>\n",
    "    2.  Ask a general semantic query to the entire library collection.<br>\n",
    "    3.  Select the most relevant results by document.<br>\n",
    "    4.  Loop through all of the documents - packaging the context and asking our questions to the LLM.<br>\n",
    "    NOTE: to use chromadb, you may need to install the python sdk:  pip3 install chromadb.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llmware.library import Library\n",
    "from llmware.retrieval import Query\n",
    "from llmware.setup import Setup\n",
    "from llmware.status import Status\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.configs import LLMWareConfig\n",
    "from importlib import util\n",
    "if not util.find_spec(\"chromadb\"):\n",
    "    print(\"\\nto run this example with chromadb, you need to install the chromadb python sdk:  pip3 install chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_rag (library_name, embedding_model_name, llm_model_name):\n",
    "    \"\"\" Illustrates the use of semantic embedding vectors in a RAG workflow\n",
    "        --self-contained example - will be duplicative with some of the steps taken in other examples \"\"\"\n",
    "\n",
    "    # Step 1 - Create library which is the main 'organizing construct' in llmware\n",
    "    print (\"\\nupdate: Step 1 - Creating library: {}\".format(library_name))\n",
    "    library = Library().create_new_library(library_name)\n",
    "\n",
    "    # Step 2 - Pull down the sample files from S3 through the .load_sample_files() command\n",
    "    #   --note: if you need to refresh the sample files, set 'over_write=True'\n",
    "    print (\"update: Step 2 - Downloading Sample Files\")\n",
    "    sample_files_path = Setup().load_sample_files(over_write=False)\n",
    "    contracts_path = os.path.join(sample_files_path, \"Agreements\")\n",
    "\n",
    "    # Step 3 - point \".add_files\" method to the folder of documents that was just created\n",
    "    #   this method parses all of the documents, text chunks, and captures in MongoDB\n",
    "    print(\"update: Step 3 - Parsing and Text Indexing Files\")\n",
    "    library.add_files(input_folder_path=contracts_path, chunk_size=400, max_chunk_size=600,\n",
    "                      smart_chunking=1)\n",
    "\n",
    "    # Step 4 - Install the embeddings\n",
    "    print(\"\\nupdate: Step 4 - Generating Embeddings in {} db - with Model- {}\".format(vector_db, embedding_model))\n",
    "    library.install_new_embedding(embedding_model_name=embedding_model_name, vector_db=vector_db)\n",
    "\n",
    "    # RAG steps start here ...\n",
    "    print(\"\\nupdate: Loading model for LLM inference - \", llm_model_name)\n",
    "    prompter = Prompt().load_model(llm_model_name)\n",
    "    query = \"what is the executive's base annual salary\"\n",
    "\n",
    "    #   key step: run semantic query against the library and get all of the top results\n",
    "    results = Query(library).semantic_query(query, result_count=50, embedding_distance_threshold=1.0)\n",
    "\n",
    "    #   if you want to look at 'results', uncomment the two lines below\n",
    "    #   for i, res in enumerate(results):\n",
    "    #       print(\"update: \", i, res[\"file_source\"], res[\"distance\"], res[\"text\"])\n",
    "    for i, contract in enumerate(os.listdir(contracts_path)):\n",
    "        qr = []\n",
    "        if contract != \".DS_Store\":\n",
    "            print(\"\\nContract Name: \", i, contract)\n",
    "\n",
    "            #   we will look through the list of semantic query results, and pull the top results for each file\n",
    "            for j, entries in enumerate(results):\n",
    "                library_fn = entries[\"file_source\"]\n",
    "                if os.sep in library_fn:\n",
    "                    # handles difference in windows file formats vs. mac / linux\n",
    "                    library_fn = library_fn.split(os.sep)[-1]\n",
    "                if library_fn == contract:\n",
    "                    print(\"Top Retrieval: \", j, entries[\"distance\"], entries[\"text\"])\n",
    "                    qr.append(entries)\n",
    "\n",
    "            #   we will add the query results to the prompt\n",
    "            source = prompter.add_source_query_results(query_results=qr)\n",
    "\n",
    "            #   run the prompt\n",
    "            response = prompter.prompt_with_source(query, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "            #   note: prompt_with_resource returns a list of dictionary responses\n",
    "            #   -- depending upon the size of the source context, it may call the llm several times\n",
    "            #   -- each dict entry represents 1 call to the LLM\n",
    "            for resp in response:\n",
    "                if \"llm_response\" in resp:\n",
    "                    print(\"\\nupdate: llm answer - \", resp[\"llm_response\"])\n",
    "\n",
    "            # start fresh for next document\n",
    "            prompter.clear_source_materials()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "update: Step 1 - Creating library: example_5_library\n",
      "update: Step 2 - Downloading Sample Files\n",
      "update: Step 3 - Parsing and Text Indexing Files\n",
      "\n",
      "update: Step 4 - Generating Embeddings in chromadb db - with Model- industry-bert-contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsyyu\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hsyyu\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    LLMWareConfig().set_active_db(\"sqlite\")\n",
    "\n",
    "    #   for this example, we will use an embedding model that has been 'fine-tuned' for contracts\n",
    "    embedding_model = \"industry-bert-contracts\"\n",
    "\n",
    "    #   note: as of llmware==0.2.12, we have shifted from faiss to chromadb for the Fast Start examples\n",
    "    #   --if you are using a Python version before 3.12, please feel free to substitute for \"faiss\"\n",
    "    #   --for versions of Python >= 3.12, for the Fast Start examples (e.g., no install required), we\n",
    "    #   recommend using chromadb or lancedb\n",
    "\n",
    "    #   please double-check: `pip3 install chromadb` or pull the latest llmware version to get automatically\n",
    "    #   -- if you have installed any other vector db, just change the name, e.g, \"milvus\" or \"pg_vector\"\n",
    "    vector_db = \"chromadb\"\n",
    "\n",
    "    # pick any name for the library\n",
    "    lib_name = \"example_5_library\"\n",
    "    example_models = [\"llmware/bling-1b-0.1\", \"llmware/bling-tiny-llama-v0\", \"llmware/dragon-yi-6b-gguf\"]\n",
    "\n",
    "    # use local cpu model\n",
    "    llm_model_name = example_models[0]\n",
    "\n",
    "    #   to swap in a gpt-4 openai model - uncomment these two lines\n",
    "    #   llm_model_name = \"gpt-4\"\n",
    "    #   os.environ[\"USER_MANAGED_OPENAI_API_KEY\"] = \"<insert-your-openai-key>\"\n",
    "    semantic_rag(lib_name, embedding_model, llm_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
