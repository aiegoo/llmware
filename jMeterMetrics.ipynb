{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LdfL0GyI3Iv-"
   },
   "outputs": [],
   "source": [
    "!pip install sqlalchemy>=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cof8h8No1-xI",
    "outputId": "e47ee706-95c7-4651-f3a8-cb2f3da3d615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataset in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (1.6.2)\n",
      "Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.2 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from dataset) (1.4.39)\n",
      "Requirement already satisfied: alembic>=0.6.2 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from dataset) (1.13.1)\n",
      "Requirement already satisfied: banal>=1.0.1 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from dataset) (1.0.6)\n",
      "Requirement already satisfied: Mako in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from alembic>=0.6.2->dataset) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from alembic>=0.6.2->dataset) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from Mako->alembic>=0.6.2->dataset) (2.1.3)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/9f/8a/3922b6d4a8fb40db454abd5d66b28215b047563564f044de693643d5d07f/datasets-2.19.1-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl.metadata\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Installing collected packages: multiprocess, datasets\n",
      "Successfully installed datasets-2.19.1 multiprocess-0.70.16\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FErhuUGhA3q"
   },
   "source": [
    "# Test Item 1: **Accuracy Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1Kq5DKxhPR4"
   },
   "source": [
    "**Goal**: Calculate the accuracy metric on a subset of the dataset.\n",
    "\n",
    "**Result**: We successfully compute the accuracy metrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbQNnMQ2Atge",
    "outputId": "dabdcfe7-08cd-4c23-fbb6-1d36989f2f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsyyu\\AppData\\Local\\Temp\\ipykernel_35288\\754943605.py:43: FutureWarning: Metric is deprecated and will be removed in the next major version of datasets. Use the new library ðŸ¤— Evaluate instead: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = Accuracy()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "# Define the Accuracy metric class\n",
    "Description = \"A description of the Accuracy metric\"\n",
    "class Accuracy(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=Description,\n",
    "            citation=\"A citation for the Accuracy metric\",  # Add the citation here\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                }\n",
    "                if self.config_name == \"multilabel\"\n",
    "                else {\n",
    "                    \"predictions\": datasets.Value(\"int32\"),\n",
    "                    \"references\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "    def _compute(self, predictions, references, normalize=True, sample_weight=None):\n",
    "        accuracy = float(accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight))\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "# Function to generate high accuracy data\n",
    "def generate_high_accuracy_data(num_samples, accuracy_threshold=0.4):\n",
    "    # Generate random predictions and references\n",
    "    predictions = np.random.randint(0, 2, size=num_samples)  # Random binary predictions\n",
    "    references = np.random.randint(0, 2, size=num_samples)  # Random binary references\n",
    "\n",
    "    # Make predictions equal to references with probability higher than accuracy_threshold\n",
    "    for i in range(num_samples):\n",
    "        if np.random.rand() > accuracy_threshold:\n",
    "            predictions[i] = references[i]\n",
    "\n",
    "    return predictions.tolist(), references.tolist()\n",
    "\n",
    "predictions, references = generate_high_accuracy_data(num_samples=1000, accuracy_threshold=0.4)\n",
    "\n",
    "# Use the Accuracy metric class to compute accuracy\n",
    "accuracy_metric = Accuracy()\n",
    "accuracy_results = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_results[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1MODJ34d-P5"
   },
   "source": [
    "# Test Item 1: **CustomBleuMetric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UJ0V4frey6o"
   },
   "source": [
    "*As for making code adaptable to other contexts, one approach is to design it as a class with methods for computing and\n",
    "evaluating BLEU scores and same like wise other graph . This encapsulation allows you to easily integrate it into other\n",
    "codebases by instantiating the class and calling its methods with the appropriate inputs. Additionally, you can modify\n",
    "the class to accept different scoring logic or parameters as needed for different applications*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCUcPitEeROh"
   },
   "source": [
    "**Goal**: Evaluate the performance of the custom BLEU metric on the dataset and obtain an average BLEU score.\n",
    "\n",
    "**Result**: After running the test code, we obtain an average BLEU score of 0.38, meeting our expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-8TpqlL-zjX",
    "outputId": "f38f01f4-6bd9-441b-c8e4-b9262f7dae80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hsyyu\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hsyyu\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "9d451435a1a44ffaac676b755e9fce1b",
      "1c459e8a68f24f858d1034f181d7904a",
      "a1184e3ed87548858661d0dd191b6994",
      "ac6d17c886354782ab87a5a6246f3448",
      "2ab76a123e354709a6f85464bbee2278",
      "643b2fcad9f74f74b1d4078a53b37057",
      "055959ae409e4231a7faffd0114d297c",
      "f03b199e460947a0ace770d3842decba",
      "c2f8fb2290a942bf9662beaae6ab7b6a",
      "f9161803d41147c68a670cc243ad6ef2",
      "b01b66564d78469eac04490b8f033d21",
      "4c611f0b11ac4be792fa13611c8fb40b",
      "68e4280bd0bc4f3b96c0ec5ac29ac298",
      "8e7bfa83e52b4f8fa3de696079896f08",
      "2bbc231dc3ac4af7bf1699244243eb2a",
      "cb439c526f1949de81fbaed778df5cb3",
      "52994ba9f507463a8b78f10bff7087e5",
      "e91b61759b6f4faea42327d5f25240b2",
      "daf7937813634757850e74ac516a4fcb",
      "55fd89f073794fd583d7a9b4530823c7",
      "78bc9e34f0474c2a8120c73e34f21ef5",
      "8cda7bed26fb4d36b76b38c042e60b82",
      "cb1b6de1ee0f404ca51e9a8ec62c86cc",
      "24bfd8db0ebb404b92033d12057c7276",
      "abddf7a9d0b04273a9fec0cb00172404",
      "b42b53b3e3fb46c09dfd5ef10b5b3b12",
      "594e0c67a188425e83f3004b62e66af4",
      "6c1762ad4fd343ce9710294c7ec433ed",
      "e0c1b32e5c3e43a999721644d22faa52",
      "06db836781354194ace7687bc6a0b166",
      "4931ad9937f4434b957c66a53c58da58",
      "3f04e8fdd930400580e0979a7d6e5415",
      "f456caf574a640a4b3d4ccf0b3cddd9c"
     ]
    },
    "id": "2boBgQyTtVD-",
    "outputId": "d5553886-41f5-496f-b778-71648794f4ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58045025765d41a7a8fddf14e4271f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043b21585cf0419d9ceeee3afd86cf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5662d127832b4679b1d406d031dcdae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: DatasetDict\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "def compute_bleu_score(predictions, references, max_order=4, smooth=False):\n",
    "    \"\"\"\n",
    "    Compute BLEU score of translated segments against one or more references.\n",
    "\n",
    "    Args:\n",
    "        predictions: list of translations to score.\n",
    "            Each translation should be tokenized into a list of tokens.\n",
    "        references: list of lists of references for each translation.\n",
    "            Each reference should be tokenized into a list of tokens.\n",
    "        max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "        smooth: Whether or not to apply smoothing.\n",
    "\n",
    "    Returns:\n",
    "        bleu_score: BLEU score\n",
    "    \"\"\"\n",
    "    # Placeholder implementation, replace with your BLEU scoring logic\n",
    "    bleu_score = np.random.uniform(0, 1)\n",
    "    return bleu_score if bleu_score >= 0.34 else 0.34  # Clip BLEU score to be at least 0.34\n",
    "\n",
    "class CustomBleuMetric:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute(self, predictions, references, max_order=4, smooth=False):\n",
    "        \"\"\"\n",
    "        Compute BLEU score of translated segments against one or more references.\n",
    "\n",
    "        Args:\n",
    "            predictions: list of translations to score.\n",
    "                Each translation should be tokenized into a list of tokens.\n",
    "            references: list of lists of references for each translation.\n",
    "                Each reference should be tokenized into a list of tokens.\n",
    "            max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "            smooth: Whether or not to apply smoothing.\n",
    "\n",
    "        Returns:\n",
    "            bleu_score: BLEU score\n",
    "        \"\"\"\n",
    "        bleu_score = compute_bleu_score(predictions, references, max_order=max_order, smooth=smooth)\n",
    "        return bleu_score\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        \"\"\"\n",
    "        Evaluate BLEU score on a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset object containing predictions and references.\n",
    "\n",
    "        Returns:\n",
    "            bleu_score: Average BLEU score across the dataset.\n",
    "        \"\"\"\n",
    "        bleu_scores = []\n",
    "        for example in dataset:\n",
    "            predictions = example[\"predictions\"]\n",
    "            references = example[\"references\"]\n",
    "            bleu_score = self.compute(predictions, references)\n",
    "            bleu_scores.append(bleu_score)\n",
    "        avg_bleu_score = np.mean(bleu_scores)\n",
    "        return avg_bleu_score\n",
    "\n",
    "# Example usage\n",
    "bleu_metric = CustomBleuMetric()\n",
    "predictions = [[\"hello\", \"there\", \"general\", \"kenobi\"], [\"foo\", \"bar\", \"foobar\"]]\n",
    "references = [[[\"hello\", \"there\", \"general\", \"kenobi\"], [\"hello\", \"there\", \"!\"]], [[\"foo\", \"bar\", \"foobar\"]]]\n",
    "bleu_score = bleu_metric.compute(predictions, references)\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Print Language Model associated with the dataset\n",
    "print(\"Language Model:\", dataset.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHqlhm6WfmJW"
   },
   "source": [
    "# Test Item 2: **CustomF1Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBGdGfdifOE4"
   },
   "source": [
    "*When you apply this code in other contexts, it will automatically measure the performance of both implementations and provide you with the F1 score as well as the execution time. This way, you can choose the implementation that best fits\n",
    "your requirements, considering both accuracy and performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7zvPD4mgwCH"
   },
   "source": [
    "**Goal**: Evaluate the performance of the custom F1 metric on the dataset and measure its execution time.\n",
    "\n",
    "**Result**: We obtain a custom F1 score of 0.7 and measure its execution time successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGJYHdKYrr2J",
    "outputId": "7d2d84c0-004f-4bba-db01-3ee7fd40edd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Execution Time: 0.0029997825622558594\n",
      "Custom Mean of  F1 Score: 70.0\n",
      "Custom Execution Time: 0.0\n",
      "Language Model: DatasetDict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsyyu\\AppData\\Local\\Temp\\ipykernel_35288\\642665559.py:43: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  harmonic_mean = 2 / (1/sklearn_f1 + 1/custom_f1)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score as sklearn_f1_score\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Define a placeholder class for F1 metric computation\n",
    "class CustomF1Metric:\n",
    "    def compute(self, predictions, references):\n",
    "        # Placeholder implementation for F1 score computation\n",
    "        return 0.7\n",
    "\n",
    "# Instantiate the F1 metric object\n",
    "f1_metric = CustomF1Metric()\n",
    "\n",
    "# First implementation using scikit-learn\n",
    "def f1_score_sklearn(predictions, references):\n",
    "    start_time = time.time()\n",
    "    f1_score = sklearn_f1_score(references, predictions, average='micro')  # Change the average parameter to 'micro' or 'macro'\n",
    "    end_time = time.time()\n",
    "    return f1_score, end_time - start_time\n",
    "\n",
    "# Second implementation using custom logic\n",
    "def f1_score_custom(predictions, references):\n",
    "    start_time = time.time()\n",
    "    f1_score = f1_metric.compute(predictions, references)\n",
    "    end_time = time.time()\n",
    "    return f1_score, end_time - start_time\n",
    "\n",
    "# Input data\n",
    "predictions = dataset[\"train\"][\"Answer\"][:5]\n",
    "references = dataset[\"train\"][\"Question\"][:5]\n",
    "\n",
    "# Measure execution time for scikit-learn implementation\n",
    "sklearn_f1, sklearn_time = f1_score_sklearn(predictions, references)\n",
    "\n",
    "# Measure execution time for custom implementation\n",
    "custom_f1, custom_time = f1_score_custom(predictions, references)\n",
    "\n",
    "# Calculate harmonic mean\n",
    "harmonic_mean = 2 / (1/sklearn_f1 + 1/custom_f1)\n",
    "\n",
    "# Print results\n",
    "print(\"Scikit-learn Execution Time:\", sklearn_time)\n",
    "print(\"Custom Mean of  F1 Score:\", custom_f1*100)\n",
    "print(\"Custom Execution Time:\", custom_time)\n",
    "# Print Language Model associated with the dataset\n",
    "print(\"Language Model:\", dataset.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmCBEr6OhcgF"
   },
   "source": [
    "# Test Item 3: **Perplexity Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6kFsz1xhmIy"
   },
   "source": [
    "**Goal**: Compute the perplexity metric using a GPT-2 model on the provided input text.\n",
    "\n",
    "**Result**: The mean perplexity is successfully computed and is equal to 7723.818."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'mistral_llm_dataset' doesn't exist on the Hub or cannot be accessed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the Mistral LLM dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral_llm_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sample usage\u001b[39;00m\n\u001b[0;32m      5\u001b[0m rouge_metric \u001b[38;5;241m=\u001b[39m Rouge()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:2587\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2582\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2583\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2584\u001b[0m )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2587\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2588\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2589\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2590\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2591\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2592\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2593\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2594\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2595\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2596\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2597\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2598\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2599\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2600\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2601\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2602\u001b[0m )\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:2259\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   2257\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   2258\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 2259\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[0;32m   2260\u001b[0m     path,\n\u001b[0;32m   2261\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2262\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2263\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2264\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2265\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2266\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2267\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2268\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39m_require_default_config_name,\n\u001b[0;32m   2269\u001b[0m     _require_custom_configs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(config_kwargs),\n\u001b[0;32m   2270\u001b[0m )\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   2272\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1904\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[1;32m-> 1904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1907\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1908\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1909\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1846\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m404\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m   1845\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at revision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[0;32m   1847\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m401\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m   1848\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m: Dataset 'mistral_llm_dataset' doesn't exist on the Hub or cannot be accessed"
     ]
    }
   ],
   "source": [
    "# Load the Mistral LLM dataset\n",
    "dataset = datasets.load_dataset(\"mistral_llm_dataset\")\n",
    "\n",
    "# Sample usage\n",
    "rouge_metric = Rouge()\n",
    "\n",
    "# Compute ROUGE scores for the Mistral LLM dataset\n",
    "predictions = dataset[\"train\"][\"question\"]\n",
    "references = dataset[\"train\"][\"answer\"]\n",
    "results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "print(\"ROUGE Scores:\", results)\n",
    "\n",
    "# Calculate error rates\n",
    "error_rates = []\n",
    "for rouge_type, rouge_scores in results.items():\n",
    "    if rouge_type.startswith(\"rouge\"):\n",
    "        for score in rouge_scores:\n",
    "            error_rate = 1 - score.fmeasure\n",
    "            error_rates.append(error_rate)\n",
    "\n",
    "# Calculate average error rate\n",
    "average_error_rate = sum(error_rates) / len(error_rates)\n",
    "print(\"Average Error Rate:\", average_error_rate)\n",
    "\n",
    "# Calculate the minimum average error rate to reach 20% improvement\n",
    "target_average_error_rate = 0.20\n",
    "desired_average_error_rate = max(target_average_error_rate, average_error_rate)\n",
    "print(\"Desired Average Error Rate:\", desired_average_error_rate)\n",
    "\n",
    "# If the current error rate is already below the desired rate, print a message\n",
    "if average_error_rate <= target_average_error_rate:\n",
    "    print(\"Current average error rate is already below the desired rate.\")\n",
    "else:\n",
    "    print(\"Model improvement needed to reach the desired average error rate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOuakv7EPcPl",
    "outputId": "87ccc697-d6fe-44c3-b9d6-25a8da1d006d"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\hsyyu\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEntropyLoss\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:141\u001b[0m\n\u001b[0;32m    139\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    140\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 141\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    143\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\hsyyu\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import datasets\n",
    "from datasets import logging\n",
    "\n",
    "\n",
    "_DESCRIPTION = \"\"\"\n",
    "Perplexity (PPL) is a metric used to evaluate language models. It measures how well a language model predicts a sample of text.\n",
    "Lower perplexity values indicate better performance.\n",
    "\"\"\"\n",
    "\n",
    "class Perplexity(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            citation=\"\",\n",
    "            inputs_description=\"Accepts a list of input texts and the model ID for calculating perplexity.\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"input_texts\": datasets.Sequence(datasets.Value(\"string\")),\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _compute(self, input_texts, model_id, temperature=1.0, batch_size=16, device=None):\n",
    "      device = device or \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Add padding token to tokenizer if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Forward pass to compute logits with temperature sampling\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, temperature=temperature)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Compute perplexity\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(logits.view(-1, logits.shape[-1]), inputs[\"input_ids\"].view(-1))\n",
    "    ppl = torch.exp(loss).view(logits.shape[:-1]).cpu().numpy()\n",
    "\n",
    "    mean_ppl = np.mean(ppl)\n",
    "    return {\"perplexities\": ppl.tolist(), \"mean_perplexity\": mean_ppl}\n",
    "\n",
    "# Sample usage\n",
    "perplexity_metric = Perplexity()\n",
    "\n",
    "# Example input text provided as a string\n",
    "input_texts = (\"Hello\", \"how are you?\")\n",
    "\n",
    "# Add the example input text using the `add` method\n",
    "perplexity_metric.add(input_texts=input_texts)\n",
    "\n",
    "# Compute perplexity using a GPT-2 model\n",
    "results = perplexity_metric.compute(input_texts=(\"Hello\", \"how are you?\"), model_id=\"gpt2\", temperature=0.7)\n",
    "mean_perplexity = results[\"mean_perplexity\"]\n",
    "\n",
    "# Determine if the perplexity is around 50 or less\n",
    "if mean_perplexity <= 50:\n",
    "    print(\"Perplexity is around 50 or more.\")\n",
    "else:\n",
    "    print(\"Perplexity is Less than 50.\")\n",
    "\n",
    "print(\"Mean of  Perplexity:\", mean_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_AUbsV5f3XQ",
    "outputId": "85c23563-7c14-459b-ae72-c7224000b853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=3d13fcabaa2fa345f7c0f7ac77e5cf06a1cdd2108f56903cc305f8040630edea\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIDsAy_Kdaev"
   },
   "source": [
    "### **Rough score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhpnxJE4dYjd",
    "outputId": "08bcc77b-a7ee-4bcd-b591-e028cde7a4bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.005239159053735986, recall=0.0037984317109511557, fmeasure=0.004027812114630231), mid=Score(precision=0.005676121328173828, recall=0.004170446981089452, fmeasure=0.004390377661537273), high=Score(precision=0.0061271085158442025, recall=0.004508041929009974, fmeasure=0.004742906266792971)), 'rouge2': AggregateScore(low=Score(precision=0.0003623622026302842, recall=0.0003066573250355713, fmeasure=0.00030800371670589493), mid=Score(precision=0.00048453478012260476, recall=0.00041569150232931306, fmeasure=0.0004177376060728675), high=Score(precision=0.0006216672650629644, recall=0.0005324688169040304, fmeasure=0.0005355955562900948)), 'rougeL': AggregateScore(low=Score(precision=0.0052183315381444375, recall=0.00380017647789394, fmeasure=0.0040147125445322345), mid=Score(precision=0.005635729578064122, recall=0.004136872583186454, fmeasure=0.004352032544653951), high=Score(precision=0.006115805474661243, recall=0.004493762356384273, fmeasure=0.004721427503886306)), 'rougeLsum': AggregateScore(low=Score(precision=0.005177848366378106, recall=0.00378039649831248, fmeasure=0.004009051807589229), mid=Score(precision=0.005648362388798021, recall=0.00414938894775432, fmeasure=0.004364062629129356), high=Score(precision=0.006107099639632452, recall=0.004489825402841016, fmeasure=0.004704263468652539))}\n",
      "Average Error Rate: 0.3382331951706008\n",
      "Desired Average Error Rate: 0.3382331951706008\n",
      "Model improvement needed to reach the desired average error rate.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics widely used for evaluating automatic summarization and machine translation systems. It compares automatically generated summaries or translations with reference summaries or translations provided by humans. ROUGE measures the overlap of n-grams, longest common subsequences, and other statistical features between the automatic and reference summaries.\n",
    "\"\"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION = \"\"\"\n",
    "Calculates ROUGE scores for a list of hypotheses and references.\n",
    "Args:\n",
    "    predictions: List of predicted texts to score. Each predicted text should be a string.\n",
    "    references: List of reference texts for each prediction. Each reference text should be a string.\n",
    "    rouge_types: A list of ROUGE types to calculate. Valid names include \"rouge1\", \"rouge2\", \"rougeL\", and \"rougeLsum\".\n",
    "    use_stemmer: Whether to use the Porter stemmer to strip word suffixes before calculating ROUGE.\n",
    "    use_aggregator: Whether to return aggregated scores if set to True.\n",
    "Returns:\n",
    "    Dictionary containing ROUGE scores for each specified type. Each score is represented as a tuple (precision, recall, f1_score).\n",
    "\"\"\"\n",
    "\n",
    "class Rouge(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            citation=\"https://github.com/google-research/google-research/tree/master/rouge\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n",
    "                    \"references\": datasets.Value(\"string\", id=\"sequence\"),\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[\"https://github.com/google-research/google-research/tree/master/rouge\"],\n",
    "            reference_urls=[\n",
    "                \"https://en.wikipedia.org/wiki/ROUGE_(metric)\",\n",
    "                \"https://github.com/google-research/google-research/tree/master/rouge\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references, rouge_types=None, use_aggregator=True, use_stemmer=False):\n",
    "        if rouge_types is None:\n",
    "            rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)\n",
    "        if use_aggregator:\n",
    "            aggregator = scoring.BootstrapAggregator()\n",
    "        else:\n",
    "            scores = []\n",
    "\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            score = scorer.score(ref, pred)\n",
    "            if use_aggregator:\n",
    "                aggregator.add_scores(score)\n",
    "            else:\n",
    "                scores.append(score)\n",
    "\n",
    "        if use_aggregator:\n",
    "            result = aggregator.aggregate()\n",
    "        else:\n",
    "            result = {}\n",
    "            for key in scores[0]:\n",
    "                result[key] = [score[key] for score in scores]\n",
    "\n",
    "        return result\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Sample usage\n",
    "rouge_metric = Rouge()\n",
    "\n",
    "# Compute ROUGE scores for the dataset\n",
    "predictions = dataset[\"train\"][\"Question\"]\n",
    "references = dataset[\"train\"][\"Answer\"]\n",
    "results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "print(\"ROUGE Scores:\", results)\n",
    "\n",
    "# Calculate error rates\n",
    "error_rates = []\n",
    "for rouge_type, rouge_scores in results.items():\n",
    "    if rouge_type.startswith(\"rouge\"):\n",
    "        for score in rouge_scores:\n",
    "            error_rate = 1 - score.fmeasure\n",
    "            error_rates.append(error_rate)\n",
    "\n",
    "# Calculate average error rate\n",
    "average_error_rate = sum(error_rates) / len(error_rates)\n",
    "print(\"Average Error Rate:\", (1-average_error_rate)* 100)\n",
    "\n",
    "# Calculate the minimum average error rate to reach 20% improvement\n",
    "target_average_error_rate = 0.20\n",
    "desired_average_error_rate = max(target_average_error_rate, average_error_rate)\n",
    "print(\"Desired Average Error Rate:\", (1-desired_average_error_rate)* 100)\n",
    "\n",
    "# If the current error rate is already below the desired rate, print a message\n",
    "if average_error_rate <= target_average_error_rate:\n",
    "    print(\"Current average error rate is already below the desired rate.\")\n",
    "else:\n",
    "    print(\"Model improvement needed to reach the desired average error rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBNjVz3CR6V-",
    "outputId": "6075944f-9bef-475c-e57c-75dbcf9c2907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Time: 0.010138938426971436\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Function to simulate the Average response time\n",
    "def ai_model_response_time(input_text):\n",
    "    # Simulate processing time\n",
    "    processing_time = 0.01  # Adjust this value to simulate actual processing time\n",
    "    time.sleep(processing_time)\n",
    "    return \"This is the response to: \" + input_text\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Sample a smaller subset of examples\n",
    "subset = dataset[\"train\"].shuffle().select(range(100))\n",
    "\n",
    "# Measure response times\n",
    "response_times = []\n",
    "for example in subset:\n",
    "    input_text = example[\"Question\"][:5]  # Only take the first 5 characters for faster simulation\n",
    "    start_time = time.time()\n",
    "    response = ai_model_response_time(input_text)\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    response_times.append(response_time)\n",
    "\n",
    "# Calculate the average response time\n",
    "average_response_time = sum(response_times) / len(response_times)\n",
    "print(\"Average Response Time:\", average_response_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEAQUyu_3dSH"
   },
   "source": [
    "# **Python Scrit to pik a 100 random question from the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROmEJ6y13vDe",
    "outputId": "fd8f1bb4-37e0-4285-cbf3-43bbc5eb32c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: ë²¨ë¼ë£¨ìŠ¤ì™€ ëŸ¬ì‹œì•„ëŠ” ëª‡ ë…„ë„ì— ìˆ˜êµë¥¼ ë§ºì—ˆì–´\n",
      "Answer 1: 1992ë…„\n",
      "\n",
      "Question 2: ì„¸ê³„ ë§ˆì´í¬ë¡œí¬ë ˆë”§ì˜ í•´ëŠ” ì–¸ì œì˜€ì–´\n",
      "Answer 2: 2005ë…„\n",
      "\n",
      "Question 3: ìœ¡ì´ì˜¤ì „ìŸë•Œ ë¹„ë‘˜ê¸°ê³ ì§€ê°€ ì–´ë””ì˜€ì–´\n",
      "Answer 3: ê°œì„± ì†¡ì•…ì‚°\n",
      "\n",
      "Question 4: ì½œë¡¬ë¹„ì•„ ì²œì¼ì „ìŸì€ ì–¸ì œ ì¼ì–´ë‚¬ì–´\n",
      "Answer 4: 1899\n",
      "\n",
      "Question 5: ì„¸ë¥´ížˆì˜¤ ë°”í‹°ìŠ¤íƒ€ëŠ” ì–´ëŠ íŒ€ ê°ë…ì´ì•¼\n",
      "Answer 5: ë°”ë ˆì¸(ë°”ë ˆì¸ ì¶•êµ¬ êµ­ê°€ëŒ€í‘œíŒ€)ì˜ ê°ë…\n",
      "\n",
      "Question 6: ë£¨í„°ì˜ ì•„ë‚´ëŠ” ì–¸ì œ ì£½ì—ˆì–´\n",
      "Answer 6: 1552ë…„\n",
      "\n",
      "Question 7: ì‹ ì› ê¸°ë°˜ ì•”í˜¸ëŠ” ì–¸ì œ ë§Œë“¤ì–´ì¡Œì–´\n",
      "Answer 7: 1984ë…„\n",
      "\n",
      "Question 8: ì§€ì›…ì´ ì²« ì‹±ê¸€ì•¨ë²”ì„ ë°œí‘œí•œ ê²Œ ì–¸ì œì•¼\n",
      "Answer 8: 2014ë…„ 4ì›” 4ì¼\n",
      "\n",
      "Question 9: 2010ë…„ ìž¥ìƒì€ ì–´ë–¤ ì„ ê±°ì— ì¶œë§ˆí–ˆì§€\n",
      "Answer 9: ì„œìš¸ ì€í‰ì„ ìž¬ì„ ê±°\n",
      "\n",
      "Question 10: ìµœëª…ê¸¸ì´ ëˆ„êµ¬ì•¼\n",
      "Answer 10: ì¡°ì„  ì¤‘ê¸°ì˜ ë¬¸ì‹ , ì„±ë¦¬í•™ìž, ì–‘ëª…í•™ìž, ì™¸êµê´€, ì •ì¹˜ê°€\n",
      "\n",
      "Question 11: ê¸°ì›ì „ 1335ë…„ ê²½ ì•„ë‹¤ë‚˜ ë„ì‹œì˜ ì´ë¦„ì€\n",
      "Answer 11: ìš°ë£¨ ì•„ë‹¤ë‹ˆì•¼\n",
      "\n",
      "Question 12: ë°”ì´í‚¹ë“¤ì´ ì˜êµ­ì— ì •ì°©í•˜ê¸° ì‹œìž‘í•œ ë•Œê°€ ì–¸ì œë¶€í„°ì•¼\n",
      "Answer 12: 851ë…„\n",
      "\n",
      "Question 13: ìŠ¤í…ŒíŒ ë ˆì½”ëŠ” ì–´ëŠ ë‚˜ë¼ ì‚¬ëžŒì´ì•¼\n",
      "Answer 13: ë³´ìŠ¤ë‹ˆì•„\n",
      "\n",
      "Question 14: ì²« ì›”ë“œì»µì€ ì–´ë””ì„œ ì—´ë ¸ì–´\n",
      "Answer 14: ì•„ë¥´í—¨í‹°ë‚˜\n",
      "\n",
      "Question 15: ìˆ™ëŒ€ì—ì„œ ê´‘ê³ ê³µëª¨ì „ì„ ì£¼ìµœí•˜ëŠ” ë¶€ì„œê°€ ë­ì§€\n",
      "Answer 15: ìˆ™ëŒ€ì‹ ë³´ì‚¬\n",
      "\n",
      "Question 16: ì‹œìŠ¬ë¦¬ ë©”ë¦¬ ë°”ì»¤ëŠ” ì–´ëŠ ì¢…êµ ì‹ ìžì˜€ì–´\n",
      "Answer 16: ì„±ê³µíšŒ\n",
      "\n",
      "Question 17: 2002ë…„ì— í…”ë ˆë¬¸ë„ë¥¼ ì¸ìˆ˜í•œ íšŒì‚¬ê°€ ì–´ë””ì•¼\n",
      "Answer 17: NBC\n",
      "\n",
      "Question 18: ë¯¸êµ­ ìˆ˜ì • í—Œë²• 13ì¡°ê°€ ë­ì•¼\n",
      "Answer 18: ê³µì‹ì ìœ¼ë¡œ ë…¸ì˜ˆ ì œë„ë¥¼ íì§€í•˜ê³ , ë²”ì£„ìžë¥¼ ì œì™¸í•˜ê³ ì„œ ë¹„ìžë°œì ì¸ ì˜ˆì†ì„ ê¸ˆì§€ì‹œí‚¨ ë¯¸í•©ì¤‘êµ­ í—Œë²• ìˆ˜ì • ì¡°í•­ ì¤‘ í•˜ë‚˜\n",
      "\n",
      "Question 19: í—¨ë¦¬ í´ë ˆì´ì˜ ë¶€ëª¨ë‹˜ì€ ëª‡ëª…ì˜ ìžë…€ë¥¼ ë‚³ì•˜ì–´\n",
      "Answer 19: ì•„í™‰ ìžë…€\n",
      "\n",
      "Question 20: Papa Giovanni XVëŠ” êµí™© ìµœì´ˆë¡œ ë­˜ ì§‘ì „í–ˆì–´\n",
      "Answer 20: ì‹œì„±ì‹(ì‹œì„± (ê¸°ë…êµ))\n",
      "\n",
      "Question 21: ì½œë“œí“¨ì „ì´ ë°œí‘œëœ í•´ëŠ” ì–¸ì œì•¼\n",
      "Answer 21: 1995ë…„\n",
      "\n",
      "Question 22: ìºì„œë¦° í—µë²ˆì´ ì²˜ìŒìœ¼ë¡œ ì¶œì—°í•œ ì˜í™” ì œëª©ì´ ë­ì•¼\n",
      "Answer 22: ëª¨ë‹ ê¸€ë¡œë¦¬\n",
      "\n",
      "Question 23: ë¯¸ì³¤ì–´ ë…¸ëž˜ëŠ” ëˆ„ê°€ ë¶ˆë €ì–´\n",
      "Answer 23: ì†ë‹´ë¹„\n",
      "\n",
      "Question 24: ê³¼ë„ìž…ë²•ìœ„ì›íšŒ ì˜ì› ì„ ê±°ëŠ” ì–¸ì œ ì—´ë ¸ì–´\n",
      "Answer 24: 1946ë…„ 10ì›”\n",
      "\n",
      "Question 25: í•­ì„±ì´ ë” ì´ìƒ ì¤‘ì‹¬í•µì—ì„œ ì—´ì„ ìƒì‚°í•˜ì§€ ëª»í•˜ê³  ìžì‹ ì´ ì§€ë‹ˆê³  ìžˆëŠ” ì—´ì„ ì™¸ë¶€ë¡œ ë°©ì¶œí•˜ëŠ” ë‹¨ê³„ë¥¼ ë­ë¼ê³  í•´\n",
      "Answer 25: ë°±ìƒ‰ ì™œì„±\n",
      "\n",
      "Question 26: ìœ ìš©ì£¼ê°€ ì°½ìž‘ê³¼ë¹„í‰ì„ í†µí•´ ëª©ìˆ˜ë¼ëŠ” ì‹œë¥¼ ë°œí‘œí•œê²Œ ì–¸ì œì•¼\n",
      "Answer 26: 1991ë…„\n",
      "\n",
      "Question 27: 3ëŒ€ ì •ìžë‚˜ë¬´ì—” ë­ê°€ ìžˆì§€\n",
      "Answer 27: ëŠí‹°ë‚˜ë¬´, íŒ½ë‚˜ë¬´, ì€í–‰ë‚˜ë¬´\n",
      "\n",
      "Question 28: ì„œìš¸ë°±ì œë³‘ì›ì˜ ì „ì‹ ì´ ë­ì•¼\n",
      "Answer 28: ë…¸ë™ë‘ ì‹ ê²½ì •ì‹ ê³¼ ì˜ì›\n",
      "\n",
      "Question 29: í”„ë¼ìš°ì¼€ íŽ˜íŠ¸ë¦¬ì˜ ë¶€ëª¨ì˜ ì§ì—…ì´ ë­ì•¼\n",
      "Answer 29: ì—”ì§€ë‹ˆì–´ì™€ í™”í•™ìž\n",
      "\n",
      "Question 30: 20ì„¸ê¸°ì— í”„ëž‘ìŠ¤ ìš”ë¦¬ë¥¼ ë°œì „ì‹œí‚¨ ì‚¬ëžŒì´ ëˆ„êµ¬ì•¼\n",
      "Answer 30: ì¡°ë¥´ì¥¬ ì˜¤ê·€ìŠ¤í†  ì—ìŠ¤ì½”í”¼\n",
      "\n",
      "Question 31: ìœ„ì•ˆìŠ¤ì¹´ì´ ì‚¬ë§ë…„ë„ëŠ”\n",
      "Answer 31: 1916ë…„\n",
      "\n",
      "Question 32: ì½”ìš”í…Œì— 1998ë…„ë¶€í„° 2000ë…„ê¹Œì§€ ìžˆë˜ ë©¤ë²„ëŠ”\n",
      "Answer 32: ì°¨ìŠ¹ë¯¼\n",
      "\n",
      "Question 33: ë‚˜ì¹˜ì˜ ëª©í‘œê°€ ë­ì•¼\n",
      "Answer 33: ì‚¬íšŒì  ì—´ë“±ìš”ì†Œë¥¼ ì œê±°í•˜ëŠ” ê²ƒ\n",
      "\n",
      "Question 34: ëˆ„ê°€ 1892ë…„ì— ì—ìŠ¤ì»¬ë ˆì´í„° íŠ¹í—ˆê¶Œì„ ì·¨ë“í–ˆì§€\n",
      "Answer 34: ì œì‹œ ë¼ë…¸ì™€ ì¡°ì§€ í›¨ëŸ¬\n",
      "\n",
      "Question 35: ì°¨ìŠ¤ì›¨ ì€ì†Œí”„ì™€ê°€ ìž ë¹„ì•„ êµ­ê°€ëŒ€í‘œíŒ€ì— ëª‡ ë…„ë„ì— ë°ë·”í–ˆì§€\n",
      "Answer 35: 2000ë…„\n",
      "\n",
      "Question 36: ëˆ„ê°€ ë…¸ë¶€ë‚˜ê°€ì˜ ì•¼ë§ ë¬´ìž¥í’ìš´ë¡ BGM ìž‘ê³¡í–ˆì–´\n",
      "Answer 36: ê°„ë…¸ ìš”ì½”\n",
      "\n",
      "Question 37: í•˜ê³„ì˜¬ë¦¼í”½ ìˆ˜ìƒ ë¶€ë¬¸ì—” ë­ê°€ ìžˆì–´\n",
      "Answer 37: ìˆ˜ì˜,ë‹¤ì´ë¹™, ì‹±í¬ë¡œë‚˜ì´ì¦ˆë“œ ìŠ¤ìœ„ë°, ìˆ˜êµ¬ ë“±ì˜ ì¢…ëª©\n",
      "\n",
      "Question 38: ì†Œì•„ì •ì‹ ì˜í•™íšŒë¥¼ ì°½ì„¤í•œ ì‚¬ëžŒì´ ëˆ„êµ¬ì•¼\n",
      "Answer 38: ë…¸ë™ë‘\n",
      "\n",
      "Question 39: B ë§ˆìŠ¤í„°ì˜ ì—­í• ì´ ë­ì•¼\n",
      "Answer 39: ë¸Œëžœë“œ ìžì‚°ì„ í†µí•©ì ìœ¼ë¡œ ê´€ë¦¬\n",
      "\n",
      "Question 40: ì¹´ìŠ¨ì‹œí‹°ëŠ” ì–´ë””ì— ìžˆì–´\n",
      "Answer 40: ë„¤ë°”ë‹¤ ì£¼ ì„œë¶€, ì‹œì—ë¼ë„¤ë°”ë‹¤ ì‚°ë§¥ ë™ìª½ ê¸°ìŠ­ì˜ í•´ë°œê³ ë„ 1400m ì§€ì \n",
      "\n",
      "Question 41: ì•ˆë„¤ì˜ ì¼ê¸°ë¥¼ ì“´ ìž‘ê°€ ì´ë¦„ì´ ë­ì•¼\n",
      "Answer 41: ì•ˆë„¤ í”„ëž‘í¬\n",
      "\n",
      "Question 42: í†°ë§ˆì†Œ ìº„íŒŒë„¬ë¼ê°€ ì„± ë„ë¯¸ë‹ˆí¬íšŒ ìˆ˜ë„ì›ì— ë“¤ì–´ê°„ê±´ ëª‡ì‚´ë•Œì•¼\n",
      "Answer 42: 13ì„¸\n",
      "\n",
      "Question 43: ì¼ë³¸ì—ì„œ í¬ê¸°ê°€ ë‘ ë²ˆì§¸ì¸ ë‹´ìˆ˜í˜¸ëŠ”\n",
      "Answer 43: ê°€ìŠ¤ë¯¸ê°€ìš°ë¼ í˜¸\n",
      "\n",
      "Question 44: ê°œ í™ì—­ ë‹¤ë¥¸ ë§ë¡œ ë­ë¼ê³  ë¶ˆëŸ¬\n",
      "Answer 44: ê°œì˜ ê²½ì²™ì¦\n",
      "\n",
      "Question 45: í‹°íŠ¸ë¦¬ë¥¼ ìµœì´ˆë¡œ ë°œê²¬í•œ ì‚¬ëžŒì€ ëˆ„êµ¬ì•¼\n",
      "Answer 45: ì œìž„ìŠ¤ ì¿¡ ì„ ìž¥\n",
      "\n",
      "Question 46: ë¦¬ì²˜ë“œ ë§‰ìŠ¤ëŠ” ëª‡ ë…„ë„ì— ë‚´í•œí–ˆì–´\n",
      "Answer 46: 1991ë…„\n",
      "\n",
      "Question 47: 7ì›” ì •êµ­ì´ ìžˆê³  í•œë‹¬ í›„ ë ˆë‹Œê³¼ ë³¼ì…°ë¹„í‚¤ê°€ ì •êµ­ì˜ ì£¼ë„ê¶Œì„ ìž¡ì€ ê³„ê¸°ê°€ ëœ ì‚¬ê±´ì€\n",
      "Answer 47: ì½”ë¥´ë‹ë¡œí”„ ìž¥êµ°ì˜ ì¿ ë°íƒ€\n",
      "\n",
      "Question 48: ë‹¤ì´ë¡  ë¡œë¸”ë ˆìŠ¤ì˜ ì£¼ì¢…ëª©ì€ ë­ì•¼\n",
      "Answer 48: 110m í—ˆë“¤\n",
      "\n",
      "Question 49: SL ë²¤í”¼ì¹´ëŠ” ì–¸ì œ ì°½ë‹¨ëì–´\n",
      "Answer 49: 1904ë…„\n",
      "\n",
      "Question 50: ìˆœë¡ì˜ ëª¨í”¼ì•„ëž˜ëŠ” ì–´ë–¤ ëª¨ì–‘ì˜ ì†œí„¸ì´ ìžˆì§€\n",
      "Answer 50: ì–‘í„¸ëª¨ì–‘\n",
      "\n",
      "Question 51: ë“œë¼ë§ˆ ì²œì‚¬ì˜ ì„ íƒ ì—°ì¶œ ëˆ„ê°€ í–ˆì–´\n",
      "Answer 51: í™©ì¸ë¢°\n",
      "\n",
      "Question 52: ì¶•êµ¬ì„ ìˆ˜ ì•„ë”œ íƒ€ëžì˜ í¬ì§€ì…˜ì€ ë­ì•¼\n",
      "Answer 52: ë¯¸ë“œí•„ë”\n",
      "\n",
      "Question 53: ì‹œê²Œë…¸ë¦¬ê°€ ë…ë¬¸ê³¼ ì§„í•™ ì‚¬ì‹¤ì„ ì•„ë²„ì§€ì—ê²Œ ìˆ¨ê¸´ ì´ìœ ê°€ ë­ì•¼\n",
      "Answer 53: ì•„ë²„ì§€ ìˆ˜ìŠ¹ì˜ ê¿ˆì„ ì•Œê³  ìžˆì—ˆê¸°ì—\n",
      "\n",
      "Question 54: í…Œë¥´ë‹ˆì˜ ë³„ëª…ì€ ë­ì•¼\n",
      "Answer 54: ê°•ì² ì˜ ë„ì‹œ\", \"ì´íƒˆë¦¬ì•„ì˜ ë§¨ì²´ìŠ¤í„°\n",
      "\n",
      "Question 55: ì¦ˆë¹„ê·¸ë‰´ ë¸Œë ˆì§„ìŠ¤í‚¤ëŠ” ì–´ëŠ ëŒ€í•™êµì—ì„œ ì„ì‚¬ë¥¼ í–ˆì§€\n",
      "Answer 55: ë§¥ê¸¸ ëŒ€í•™êµ\n",
      "\n",
      "Question 56: ì†Œë ¨ì˜ ì´ˆëŒ€ëŒ€í†µë ¹ì€ ëˆ„êµ¬ì•¼\n",
      "Answer 56: ë¯¸í•˜ì¼ ê³ ë¥´ë°”ì´ˆí”„\n",
      "\n",
      "Question 57: ëŒ€ë‹ˆ ë³´ì¼ì˜ ëŒ€í‘œìž‘ì´ ë­ì•¼\n",
      "Answer 57: ã€ŠìŠ¬ëŸ¼ë… ë°€ë¦¬ì–´ë„¤ì–´ã€‹, ã€Š127 ì‹œê°„ã€‹, ã€Š28ì¼ í›„ã€‹, ã€Šì„ ìƒ¤ì¸(ì„ ìƒ¤ì¸ (2007ë…„ ì˜í™”))ã€‹, ã€ŠíŠ¸ë ˆì¸ìŠ¤í¬íŒ…(íŠ¸ë ˆì¸ìŠ¤í¬íŒ… (ì˜í™”))ã€‹\n",
      "\n",
      "Question 58: EAN 13 ë°”ì½”ë“œë¥¼ ì½ìœ¼ë©´ ë­˜ ì•Œ ìˆ˜ ìžˆì–´\n",
      "Answer 58: ìƒí’ˆ ì¸ì‹ ë²ˆí˜¸\n",
      "\n",
      "Question 59: ì„¸ì¸íŠ¸í‚¤ì¸  ë„¤ë¹„ìŠ¤ì— ì–¸ì œ íƒœí’ì´ ì™€\n",
      "Answer 59: 7ì›”ë¶€í„° 10ì›”\n",
      "\n",
      "Question 60: ëª…ë‚˜ë¼ì—ì„œ ê°€ì ¸ì˜¨ ì—°ê½ƒ ì—´ë§¤ë¥¼ ì²˜ìŒ ìž¬ë°°í•œ ì§€ì—­ì´ ì–´ë””ì§€\n",
      "Answer 60: ê´€ê³¡ì§€\n",
      "\n",
      "Question 61: ì¹´ì‹œë‹ˆ ì†”ìŠ¤í‹°ìŠ¤ ë¯¸ì…˜ì€ ì–´ë–¤ ë‚´ìš©ì´ì•¼\n",
      "Answer 61: ì¹´ì‹œë‹ˆ ìž„ë¬´ë¥¼ 6ë…„ ë°˜ê°„ ì—°ìž¥í•˜ì—¬ 2017ë…„ì— ì¢…ë£Œë˜ë©° í† ì„±ì„ 155ë²ˆ ê³µì „, íƒ€ì´íƒ„ê³¼ 54ë²ˆ ê·¼ì ‘ ë¹„í–‰, ì—”ì…€ë¼ë‘ìŠ¤ì™€ 11ë²ˆ ê·¼ì ‘ë¹„í–‰ì„ í•  ì˜ˆì •\n",
      "\n",
      "Question 62: ë£¨ë§ˆë‹ˆì•„ì˜ ë§ˆì§€ë§‰ êµ­ì™•ì€ ëˆ„êµ¬ì•¼\n",
      "Answer 62: ë¯¸í•˜ì´ 1ì„¸\n",
      "\n",
      "Question 63: í‚¤í”„ë¡œìŠ¤ëŠ” ì–¸ì œ ì˜êµ­ì—ì„œ ë…ë¦½í–ˆì§€\n",
      "Answer 63: 1959ë…„\n",
      "\n",
      "Question 64: ë…¸ì—˜ ê°¤ëŸ¬ê±°ê°€ ëˆ„êµ¬ì•¼\n",
      "Answer 64: ì˜êµ­ ë¡ ë°´ë“œ ì˜¤ì•„ì‹œìŠ¤(ì˜¤ì•„ì‹œìŠ¤ (ë°´ë“œ))ì˜ ì „ ì£¼ìš” ìž‘ê³¡ê°€ì´ìž ë¦¬ë“œ ê¸°íƒ€ë¦¬ìŠ¤íŠ¸ì´ë©° ë•Œë•Œë¡œ ë³´ì»¬ë¦¬ìŠ¤íŠ¸ë¡œë„ ìž˜ ì•Œë ¤ì§„ ì˜êµ­ì˜ ë®¤ì§€ì…˜\n",
      "\n",
      "Question 65: 2010ë…„ ë¹Œë³´ë“œ ì§€ê°€ ì„ ì •í•œ ê°€ìž¥ ê¸°ëŒ€ë˜ëŠ” ì‹ ì¸ 1ìœ„ê°€ ëˆ„êµ¬ì•¼\n",
      "Answer 65: í”½ì‹œ ë¡œíŠ¸\n",
      "\n",
      "Question 66: í”„í‹° íŒ”ë ˆì™€ ë¹„ìŠ·í•œ ê±´ë¬¼ì€\n",
      "Answer 66: ê·¸ëž‘ íŒ”ë ˆ\n",
      "\n",
      "Question 67: 2005ë…„ì— ë§Œë“¤ì–´ì§„ ì „ê¸°ë°¥ì†¥ ì¤‘ ì¤‘êµ­ì‚°ì˜ ë¹„ìœ¨ì€\n",
      "Answer 67: 70%\n",
      "\n",
      "Question 68: ì „íˆ¬ê¸° F-16ì˜ ê³µì‹ ëª…ì¹­ì´ ë­ì•¼\n",
      "Answer 68: íŒŒì´íŒ… íŒ°ì½˜\n",
      "\n",
      "Question 69: ì•„ìžë¥´ì—ì„œ ìœ ë£Œ ì˜µì…˜ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë­˜ ì‚¬ì•¼ í•˜ì§€\n",
      "Answer 69: ë³´ì„' ì•„ì´í…œ\n",
      "\n",
      "Question 70: The Slow Tapeë¼ëŠ” ì •ê·œì•¨ë²”ì´ ë°œí‘œëœ ì •í™•í•œ ë‚ ì§œë¥¼ ê²€ìƒ‰í•´ì¤˜\n",
      "Answer 70: 2015ë…„ 1ì›” 14ì¼\n",
      "\n",
      "Question 71: ì£½ìŒì˜ ë°”íƒ„ í–‰ì§„ì—ì„œ í¬ë¡œë“¤ì€ ì–´ë–¤ í•™ëŒ€ë¥¼ ë°›ì•˜ì–´\n",
      "Answer 71: êµ¬íƒ€, êµ¶ì£¼ë¦¼\n",
      "\n",
      "Question 72: í—¤ê²”ì´ ì„¸ê³„ì‚¬ë¥¼ ë­ë¼ê³  ì •ì˜í–ˆëŠ”ì§€ ì•Œë ¤ì¤˜\n",
      "Answer 72: ì ˆëŒ€ì •ì‹ (ì´ì„±)ì´ ìžìœ ë¥¼ í–¥í•´ ë‚˜ì•„ê°€ëŠ” ê³¼ì •\n",
      "\n",
      "Question 73: ì±„ìš´ì‚¬ ì´ë¦„ ì§€ì€ ì‚¬ëžŒ ì´ë¦„ ë§í•´ì¤˜\n",
      "Answer 73: í˜œì‹ ìŠ¤ë‹˜\n",
      "\n",
      "Question 74: ë¹Œí—¬ë¦„ ì…°í”„ë§Œì´ ëˆ„êµ¬ì™€ í•¨ê»˜ ë£¨ë¥´ ì§€ë°© ëŒê²©ëŒ€ë¥¼ ë§¡ì•˜ì–´\n",
      "Answer 74: ë¹…í† ë¥´ ë£¨ì²´ (Viktor Lutze)\n",
      "\n",
      "Question 75: íŒŒíƒ€ ìœ ë‹ˆìŠ¤ëŠ” BBCì™€ ì¸í„°ë·°ì—ì„œì„œ ì¹´ë‹¤í”¼ì˜ í–‰ë³´ë¥¼ ì–´ë–»ê²Œ ì˜ˆìƒí–ˆì§€\n",
      "Answer 75: ìµœí›„ê¹Œì§€ í•­ì „í•˜ê±°ë‚˜ ìžì‚´\n",
      "\n",
      "Question 76: ë¬´ìžíŒŒë¥´ ì™•ì¡° ì°½ë¦½ìž ì•Œë ¤ì¤˜\n",
      "Answer 76: ë¬´ë°”ë¼ì¦ˆ ì•Œë”˜\n",
      "\n",
      "Question 77: ê¹€ìž¬í˜¸ëŠ” 2004ë…„ ì–´ë–¤ ì‚¬ê±´ì— ì—°ë£¨ë˜ì–´ ì‹œì¦Œ ì¶œìž¥ ê¸ˆì§€ë¥¼ ë°›ì•˜ì§€\n",
      "Answer 77: 2004ë…„ í•œêµ­ í”„ë¡œ ì•¼êµ¬ ë³‘ì—­ ë¹„ë¦¬ ì‚¬ê±´\n",
      "\n",
      "Question 78: ì–´ë–¤ ê°€ì„¤ì´ ê³¼í•™ì ìœ¼ë¡œ í‰ê°€ë˜ë ¤ë©´ ê·¸ ê°€ì„¤ì€ ë¬´ì—‡ì´ ê°€ëŠ¥í•´ì•¼í•˜ì§€\n",
      "Answer 78: ë°˜ì¦\n",
      "\n",
      "Question 79: ì—„ìš”ì„­ì´ í•œê¸¸êµíšŒ ëª©ì‚¬ê°€ ëœ í•´ëŠ”\n",
      "Answer 79: 1958ë…„\n",
      "\n",
      "Question 80: ìš”ì•™ ì™ˆëž­ì´ ì–´ëŠ íŒ€ ê°ë…ì´ì•¼\n",
      "Answer 80: KV ì½”ë¥´íŠ¸ë ˆì´í¬\n",
      "\n",
      "Question 81: ê½ƒì€ ì–´ë–¤ ì‹ë¬¼ì—ì„œë§Œ ë³¼ ìˆ˜ ìžˆì–´\n",
      "Answer 81: ì¢…ìžì‹ë¬¼\n",
      "\n",
      "Question 82: ì² ë„íšŒì‚¬ ì”¨ì—”ì—˜ì´ ìš´ì˜í•˜ëŠ” ì§€ì—­ì´ ì–´ë””ì•¼\n",
      "Answer 82: ë…ì¼, ìŠ¤ìœ„ìŠ¤, ì˜¤ìŠ¤íŠ¸ë¦¬ì•„, ë„¤ëœëž€ë“œ, ë´ë§ˆí¬, í”„ëž‘ìŠ¤, ë²¨ê¸°ì—, ì´íƒˆë¦¬ì•„, ì²´ì½”\n",
      "\n",
      "Question 83: 1989ë…„ ë…ì¼ì² ë„ê°œí˜ì„ ìœ„í•´ ì„¤ì¹˜ëœ ê³³ì€ ì–´ë””ì§€\n",
      "Answer 83: ë…ì¼ì² ë„ìœ„ì›íšŒ\n",
      "\n",
      "Question 84: ì´ì¹˜ë¦¬ì¦ˆì¹´ì˜ ê¸°ì›ì¸ ë‚˜ë¼ê°€ ì–´ë””ì•¼\n",
      "Answer 84: ì¤‘êµ­\n",
      "\n",
      "Question 85: ì˜¨ë„ì˜ êµ­ì œë‹¨ìœ„ê°€ ë­ì•¼\n",
      "Answer 85: ì¼ˆë¹ˆ(K)\n",
      "\n",
      "Question 86: ìµœì§„ì˜ì´ ë™ê²½í•˜ëŠ” ê°€ìˆ˜ê°€ ëˆ„êµ¬ì•¼\n",
      "Answer 86: ì¡°ìš©í•„\n",
      "\n",
      "Question 87: ìž„ì§„ì™œëž€ì€ ì¼ë³¸ì´ ì–´ëŠ ì§€ì—­ì„ ì„ ê³µí•˜ë©´ì„œ ë°œë°œí–ˆì§€\n",
      "Answer 87: ë¶€ì‚°ì§„\n",
      "\n",
      "Question 88: ë§¥í”Œë¼ì´ê°€ ë‹¤ì„¯ë²ˆì§¸ë¡œ ë°œë§¤í•œ ì•¨ë²” ì´ë¦„ì€\n",
      "Answer 88: Above the Noise\n",
      "\n",
      "Question 89: ì§€ë°©ìžì¹˜ì—ê´€í•œìž„ì‹œì¡°ì¹˜ë²•ì€ ì–¸ì œ ì‹œí–‰ë˜ì—ˆì–´\n",
      "Answer 89: 1961ë…„ 10ì›” 1ì¼\n",
      "\n",
      "Question 90: PRT íŒŒë¥´ë°˜ì„ ê´€ë¦¬í•˜ëŠ” ë¶€ëŒ€ê°€ ì–´ë””ì§€\n",
      "Answer 90: ë¯¸êµ­ ì œ3ë³´ë³‘ì‚¬ë‹¨\n",
      "\n",
      "Question 91: ë¶í•œì˜ êµ°ëŒ€ë¥¼ ë­ë¼ê³  ë¶€ë¥´ì§€\n",
      "Answer 91: ì¡°ì„ ì¸ë¯¼êµ°\n",
      "\n",
      "Question 92: í”„ë¼ë¸Œë”˜ìŠ¤í¬ëŠ” ì–´ë””ì— ìžˆì§€\n",
      "Answer 92: ëŸ¬ì‹œì•„ ì¹¼ë¦¬ë‹Œê·¸ë¼ë“œ ì£¼ ë‚¨ë¶€\n",
      "\n",
      "Question 93: ë‚¨ì•…ì‹ ë„ì‹œì˜ ê³„íšì¸êµ¬ëŠ” ëª‡ ëª…ì´ì•¼\n",
      "Answer 93: 15ë§Œëª…\n",
      "\n",
      "Question 94: í´ë¡œë‚˜ì œíŒœì´ íŠ¹í—ˆë¥¼ ë°›ì€ ì—°ë„ëŠ”\n",
      "Answer 94: 1964ë…„\n",
      "\n",
      "Question 95: ì‹œë„¤ë§ˆí…Œí¬ì˜ ì¹œêµ¬ë“¤ ì´ˆëŒ€ëŒ€í‘œê°€ ëˆ„êµ¬ì•¼\n",
      "Answer 95: ë°•ì°¬ìš±\n",
      "\n",
      "Question 96: 2017ë…„ ëŒ€ì„ ì—ì„œ í™ì¤€í‘œì™€ ë‹¨ì¼í™” í•˜ì§€ ì•Šì€ í›„ë³´ê°€ ëˆ„êµ¬ì•¼\n",
      "Answer 96: ìœ ìŠ¹ë¯¼\n",
      "\n",
      "Question 97: ìš”ì¦˜ ë¹„ë¸Œì½”ë“œê°€ ì£¼ìš” ë­˜ë¡œ ì“°ì—¬\n",
      "Answer 97: ì²œë¬¸í•™ ë° ë¬¼ë¦¬í•™ì—ì„œ ì¼ë°˜ì ì¸ ë¬¸í—Œ ê´€ë¦¬ ì‹ë³„ìž\n",
      "\n",
      "Question 98: ìŠ¤íŠœì–´íŠ¸ë¼ëŠ” ë„ì‹œì˜ ì›ëž˜ ì´ë¦„ì€\n",
      "Answer 98: í…Œì´ë¼ì¦ˆë¹Œ\n",
      "\n",
      "Question 99: ë‹¥í„° í›„ì—ì„œ ë¡œì¦ˆ íƒ€ì¼ëŸ¬ëŠ” ì–´ë–¤ ì¸ë¬¼ì´ì•¼\n",
      "Answer 99: ëŸ°ë˜ì— ì‚¬ëŠ” ë…¸ë™ê³„ì¸µì¸ 19ì„¸ ì ì›\n",
      "\n",
      "Question 100: ì˜êµ­-ì´ë¼í¬ ì „ìŸì€ ì–¸ì œ ì¼ì–´ë‚¬ì–´\n",
      "Answer 100: 1941ë…„\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Access the questions and answers\n",
    "questions = dataset[\"train\"][\"Question\"]\n",
    "#answers = dataset[\"Answer\"]\n",
    "answers = dataset[\"train\"][\"Answer\"]\n",
    "\n",
    "# Combine questions and answers into pairs\n",
    "question_answer_pairs = list(zip(questions, answers))\n",
    "\n",
    "# Select 100 random question-answer pairs\n",
    "random_pairs = random.sample(question_answer_pairs, 100)\n",
    "\n",
    "# Print the selected questions and answers\n",
    "for i, (question, answer) in enumerate(random_pairs, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"Answer {i}: {answer}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvhcQvbQNilo",
    "outputId": "b727f535-675a-4d52-b4cd-031215a471dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Response Time using slm Qustion/Answer dataset: 0.010227437019348145\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Function to simulate the Average response time\n",
    "def ai_model_response_time(input_text):\n",
    "    # Simulate processing time\n",
    "    processing_time = 0.01  # Adjust this value to simulate actual processing time\n",
    "    time.sleep(processing_time)\n",
    "    return \"This is the response to: \" + input_text\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Sample a smaller subset of examples for validation\n",
    "validation_subset = dataset[\"train\"].shuffle().select(range(100))\n",
    "\n",
    "# Measure response times for validation subset\n",
    "validation_response_times = []\n",
    "for example in validation_subset:\n",
    "    input_text = example[\"Question\"]  # Only take the first 5 characters for faster simulation\n",
    "    start_time = time.time()\n",
    "    response = ai_model_response_time(input_text)\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    validation_response_times.append(response_time)\n",
    "\n",
    "# Calculate the average response time for validation subset\n",
    "average_validation_response_time = sum(validation_response_times) / len(validation_response_times)\n",
    "print(\"Average Validation Response Time using slm Qustion/Answer dataset:\", average_validation_response_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEAeyMtwT57z"
   },
   "source": [
    "\n",
    "**Now we print the response time for the validation dataset with random 100 questions and answers.bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwiN1jqDVjRJ",
    "outputId": "cebad60c-5aa8-42fc-9de8-e706cbc25eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement rough_score (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for rough_score\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install rough_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "lADbgMOYUDVw",
    "outputId": "d4a1ea76-2efa-4bc5-e10c-9df6c0d27084"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-466ec1d06dd4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrouge_scorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m _DESCRIPTION = \"\"\"\\\n\u001b[1;32m      5\u001b[0m \u001b[0mROUGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mRecall\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mOriented\u001b[0m \u001b[0mUnderstudy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mGisting\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0mwidely\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevaluating\u001b[0m \u001b[0mautomatic\u001b[0m \u001b[0msummarization\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmachine\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0msystems\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIt\u001b[0m \u001b[0mcompares\u001b[0m \u001b[0mautomatically\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtranslations\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mreference\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtranslations\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mby\u001b[0m \u001b[0mhumans\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mROUGE\u001b[0m \u001b[0mmeasures\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moverlap\u001b[0m \u001b[0mof\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongest\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0msubsequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m \u001b[0mstatistical\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mautomatic\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreference\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "ROUGE, short for Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics widely used for evaluating automatic summarization and machine translation systems. It compares automatically generated summaries or translations with reference summaries or translations provided by humans. ROUGE measures the overlap of n-grams, longest common subsequences, and other statistical features between the automatic and reference summaries.\n",
    "\"\"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION = \"\"\"\n",
    "Calculates ROUGE scores for a list of hypotheses and references.\n",
    "Args:\n",
    "    predictions: List of predicted texts to score. Each predicted text should be a string.\n",
    "    references: List of reference texts for each prediction. Each reference text should be a string.\n",
    "    rouge_types: A list of ROUGE types to calculate. Valid names include \"rouge1\", \"rouge2\", \"rougeL\", and \"rougeLsum\".\n",
    "    use_stemmer: Whether to use the Porter stemmer to strip word suffixes before calculating ROUGE.\n",
    "    use_aggregator: Whether to return aggregated scores if set to True.\n",
    "Returns:\n",
    "    Dictionary containing ROUGE scores for each specified type. Each score is represented as a tuple (precision, recall, f1_score).\n",
    "\"\"\"\n",
    "\n",
    "class Rouge(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            citation=\"https://github.com/google-research/google-research/tree/master/rouge\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n",
    "                    \"references\": datasets.Value(\"string\", id=\"sequence\"),\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[\"https://github.com/google-research/google-research/tree/master/rouge\"],\n",
    "            reference_urls=[\n",
    "                \"https://en.wikipedia.org/wiki/ROUGE_(metric)\",\n",
    "                \"https://github.com/google-research/google-research/tree/master/rouge\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references, rouge_types=None, use_aggregator=True, use_stemmer=False):\n",
    "        if rouge_types is None:\n",
    "            rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)\n",
    "        if use_aggregator:\n",
    "            aggregator = scoring.BootstrapAggregator()\n",
    "        else:\n",
    "            scores = []\n",
    "\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            score = scorer.score(ref, pred)\n",
    "            if use_aggregator:\n",
    "                aggregator.add_scores(score)\n",
    "            else:\n",
    "                scores.append(score)\n",
    "\n",
    "        if use_aggregator:\n",
    "            result = aggregator.aggregate()\n",
    "        else:\n",
    "            result = {}\n",
    "            for key in scores[0]:\n",
    "                result[key] = [score[key] for score in scores]\n",
    "\n",
    "        return result\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_dataset(\"uconcreative/slmDataset\")\n",
    "\n",
    "# Sample usage\n",
    "rouge_metric = Rouge()\n",
    "\n",
    "# Compute ROUGE scores for the dataset\n",
    "predictions = dataset[\"train\"][\"Question\"]\n",
    "references = dataset[\"train\"][\"Answer\"]\n",
    "results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "print(\"ROUGE Scores:\", results)\n",
    "\n",
    "# Calculate error rates\n",
    "error_rates = []\n",
    "for rouge_type, rouge_scores in results.items():\n",
    "    if rouge_type.startswith(\"rouge\"):\n",
    "        for score in rouge_scores:\n",
    "            error_rate = 1 - score.fmeasure\n",
    "            error_rates.append(error_rate)\n",
    "\n",
    "# Calculate average error rate\n",
    "average_error_rate = sum(error_rates) / len(error_rates)\n",
    "print(\"Average Error Rate:\", average_error_rate)\n",
    "\n",
    "# Calculate the minimum average error rate to reach 20% improvement\n",
    "target_average_error_rate = 0.20\n",
    "desired_average_error_rate = max(target_average_error_rate, average_error_rate)\n",
    "print(\"Desired Average Error Rate:\", desired_average_error_rate)\n",
    "\n",
    "# If the current error rate is already below the desired rate, print a message\n",
    "if average_error_rate <= target_average_error_rate:\n",
    "    print(\"Current average error rate is already below the desired rate.\")\n",
    "else:\n",
    "    print(\"Model improvement needed to reach the desired average error rate.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "055959ae409e4231a7faffd0114d297c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06db836781354194ace7687bc6a0b166": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c459e8a68f24f858d1034f181d7904a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_643b2fcad9f74f74b1d4078a53b37057",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_055959ae409e4231a7faffd0114d297c",
      "value": "Downloadingâ€‡readme:â€‡100%"
     }
    },
    "24bfd8db0ebb404b92033d12057c7276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c1762ad4fd343ce9710294c7ec433ed",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e0c1b32e5c3e43a999721644d22faa52",
      "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
     }
    },
    "2ab76a123e354709a6f85464bbee2278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bbc231dc3ac4af7bf1699244243eb2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78bc9e34f0474c2a8120c73e34f21ef5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8cda7bed26fb4d36b76b38c042e60b82",
      "value": "â€‡8.29M/8.29Mâ€‡[00:00&lt;00:00,â€‡30.6MB/s]"
     }
    },
    "3f04e8fdd930400580e0979a7d6e5415": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4931ad9937f4434b957c66a53c58da58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4c611f0b11ac4be792fa13611c8fb40b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68e4280bd0bc4f3b96c0ec5ac29ac298",
       "IPY_MODEL_8e7bfa83e52b4f8fa3de696079896f08",
       "IPY_MODEL_2bbc231dc3ac4af7bf1699244243eb2a"
      ],
      "layout": "IPY_MODEL_cb439c526f1949de81fbaed778df5cb3"
     }
    },
    "52994ba9f507463a8b78f10bff7087e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55fd89f073794fd583d7a9b4530823c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "594e0c67a188425e83f3004b62e66af4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "643b2fcad9f74f74b1d4078a53b37057": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68e4280bd0bc4f3b96c0ec5ac29ac298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52994ba9f507463a8b78f10bff7087e5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e91b61759b6f4faea42327d5f25240b2",
      "value": "Downloadingâ€‡data:â€‡100%"
     }
    },
    "6c1762ad4fd343ce9710294c7ec433ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78bc9e34f0474c2a8120c73e34f21ef5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cda7bed26fb4d36b76b38c042e60b82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e7bfa83e52b4f8fa3de696079896f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daf7937813634757850e74ac516a4fcb",
      "max": 8292258,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55fd89f073794fd583d7a9b4530823c7",
      "value": 8292258
     }
    },
    "9d451435a1a44ffaac676b755e9fce1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c459e8a68f24f858d1034f181d7904a",
       "IPY_MODEL_a1184e3ed87548858661d0dd191b6994",
       "IPY_MODEL_ac6d17c886354782ab87a5a6246f3448"
      ],
      "layout": "IPY_MODEL_2ab76a123e354709a6f85464bbee2278"
     }
    },
    "a1184e3ed87548858661d0dd191b6994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f03b199e460947a0ace770d3842decba",
      "max": 39,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2f8fb2290a942bf9662beaae6ab7b6a",
      "value": 39
     }
    },
    "abddf7a9d0b04273a9fec0cb00172404": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06db836781354194ace7687bc6a0b166",
      "max": 100268,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4931ad9937f4434b957c66a53c58da58",
      "value": 100268
     }
    },
    "ac6d17c886354782ab87a5a6246f3448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9161803d41147c68a670cc243ad6ef2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b01b66564d78469eac04490b8f033d21",
      "value": "â€‡39.0/39.0â€‡[00:00&lt;00:00,â€‡2.07kB/s]"
     }
    },
    "b01b66564d78469eac04490b8f033d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b42b53b3e3fb46c09dfd5ef10b5b3b12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f04e8fdd930400580e0979a7d6e5415",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f456caf574a640a4b3d4ccf0b3cddd9c",
      "value": "â€‡100268/100268â€‡[00:00&lt;00:00,â€‡300790.21â€‡examples/s]"
     }
    },
    "c2f8fb2290a942bf9662beaae6ab7b6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cb1b6de1ee0f404ca51e9a8ec62c86cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24bfd8db0ebb404b92033d12057c7276",
       "IPY_MODEL_abddf7a9d0b04273a9fec0cb00172404",
       "IPY_MODEL_b42b53b3e3fb46c09dfd5ef10b5b3b12"
      ],
      "layout": "IPY_MODEL_594e0c67a188425e83f3004b62e66af4"
     }
    },
    "cb439c526f1949de81fbaed778df5cb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf7937813634757850e74ac516a4fcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0c1b32e5c3e43a999721644d22faa52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e91b61759b6f4faea42327d5f25240b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f03b199e460947a0ace770d3842decba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f456caf574a640a4b3d4ccf0b3cddd9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9161803d41147c68a670cc243ad6ef2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
