{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\nThis example demonstrates creating vector embeddings (used for doing semantic queries)<br>\n", "      Note: lancedb is not used in the example below as it requires an API key.  If you have a lancedb account, you can set these two variables:<br>\n", "         os.environ.get(\"USER_MANAGED_lancedb_API_KEY\") = <your-lancedb-api-key><br>\n", "         os.environ.get(\"USER_MANAGED_lancedb_ENVIRONMENT\") = <your-lancedb-environment> (for example \"gcp-starter\")<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "from llmware.library import Library\n", "from llmware.retrieval import Query\n", "from llmware.setup import Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def embeddings_lancedb (library_name):\n\n", "    # Create and populate a library\n", "    print (f\"\\nstep 1 - creating and populating library: {library_name}...\")\n", "    library = Library().create_new_library(library_name)\n", "    sample_files_path = Setup().load_sample_files()\n", "    library.add_files(input_folder_path=os.path.join(sample_files_path, \"Agreements\"))\n\n", "    # To create vector embeddings you just need to specify the embedding model and the vector embedding DB\n", "    # For examples of using HuggingFace and SentenceTransformer models, see those examples in this same folder\n", "    embedding_model = \"mini-lm-sbert\"\n", "    print (f\"\\n > Generating embedding vectors and storing in lancedb ...\")\n\n", "    #   note: the only code change to use a different vector_db is changing the name in this method below\n", "    library.install_new_embedding(embedding_model_name=embedding_model, vector_db=\"lancedb\")\n\n", "    # Then when doing semantic queries, the most recent vector DB used for embeddings will be used.\n\n", "    # We just find the best 3 hits for \"Salary\"\n", "    q = Query(library)\n", "    print (f\"\\n > Running a query for 'Salary'...\")\n", "    query_results = q.semantic_query(query=\"Salary\", result_count=10, results_only=True)\n", "    print(\"query-\",query_results)\n", "    for i, entries in enumerate(query_results):\n\n", "        # each query result is a dictionary with many useful keys\n", "        text = entries[\"text\"]\n", "        document_source = entries[\"file_source\"]\n", "        page_num = entries[\"page_num\"]\n", "        vector_distance = entries[\"distance\"]\n\n", "        #  for display purposes only, we will only show the first 100 characters of the text\n", "        if len(text) > 125:  text = text[0:125] + \" ... \"\n", "        print(\"\\nupdate: query results - {} - document - {} - page num - {} - distance - {} \"\n", "              .format( i, document_source, page_num, vector_distance))\n", "        print(\"update: text sample - \", text)\n", "    return query_results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    library_name = \"embedding_test_0\"\n\n", "    # note: these two environmental variables will be checked to apply your lancedb keys\n", "    # os.environ[\"USER_MANAGED_lancedb_API_KEY\"] = \"your-lancedb-api-key\"\n", "    # os.environ[\"USER_MANAGED_lancedb_ENVIRONMENT\"] = \"your-lancedb-environment\"\n", "    embeddings_lancedb(\"embedding_test\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}