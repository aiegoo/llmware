{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n This example provides a basic framework to build a Chatbot UI interface in conjunction with LLMWare<br>\n", "    using Streamlit Chat UI.<br>\n", "    To run this example requires an install of Streamlit, e.g., `pip3 install streamlit`<br>\n", "    To execute the script, run from the command line with:  `streamlit run using_with_streamlit_ui.py`<br>\n", "    Also, please note that the first time you run with a new model, the model will be downloaded and cached locally,<br>\n", "    so expect a delay on the 'first run' which will be much faster on every successive run.<br>\n", "    All components of the chatbot will be running locally, so the speed will be determined greatly by the<br>\n", "    CPU/GPU capacities of your machine.<br>\n", "    We have set the max_output at 250 tokens - for faster, set lower ...<br>\n", "    For more information on the Streamlit Chat UI,<br>\n", "    see https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import streamlit as st\n", "from llmware.models import ModelCatalog"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def simple_chat_ui_app (model_name):\n", "    st.title(f\"Simple Chat with {model_name}\")\n", "    model = ModelCatalog().load_model(model_name, temperature=0.3, sample=True, max_output=250)\n\n", "    # initialize chat history\n", "    if \"messages\" not in st.session_state:\n", "        st.session_state.messages = []\n\n", "    # display chat messages from history on app rerun\n", "    for message in st.session_state.messages:\n", "        with st.chat_message(message[\"role\"]):\n", "            st.markdown(message[\"content\"])\n\n", "    # accept user input\n", "    prompt = st.chat_input(\"Say something\")\n", "    if prompt:\n", "        with st.chat_message(\"user\"):\n", "            st.markdown(prompt)\n", "        with st.chat_message(\"assistant\"):\n", "            model_response = model.inference(prompt)\n\n", "            # insert additional error checking / post-processing of output here\n", "            bot_response = model_response[\"llm_response\"]\n", "            st.markdown(bot_response)\n", "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n", "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": bot_response})\n", "    return 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n\n", "    #   a few representative good chat models that can run locally\n", "    #   note: will take a minute for the first time it is downloaded and cached locally\n", "    chat_models = [\"phi-3-gguf\",\n", "                   \"llama-2-7b-chat-gguf\",\n", "                   \"llama-3-instruct-bartowski-gguf\",\n", "                   \"openhermes-mistral-7b-gguf\",\n", "                   \"zephyr-7b-gguf\"]\n", "    model_name = chat_models[0]\n", "    simple_chat_ui_app(model_name)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}