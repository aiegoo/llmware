{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n This example shows how to build a simple UI RAG application for longer documents in which a retrieval query step<br>\n", "    is required to build a context from selected text chunks in the document.<br>\n", "    This example is build with a Streamlit UI. To run, it requires a separate `pip install streamlit`, and<br>\n", "    to execute the script, you should run from the command line with:<br>\n", "     `streamlit run using_with_streamlit_ui.py`<br>\n", "    For more information about Streamlit, check out their docs:  https://docs.streamlit.io/develop/tutorials<br>\n", "    To build out the application, you would replace the very simple 'text search' mechanism used below with<br>\n", "    techniques outlined in examples in Embeddings and Retrieval.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import streamlit as st"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llmware.prompts import Prompt\n", "from llmware.setup import Setup"]}, {"cell_type": "markdown", "metadata": {}, "source": ["st.set_page_config(layout=\"wide\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def simple_analyzer_with_topic_query ():\n", "    st.title(\"Simple RAG Analyzer with Focusing Query\")\n", "    prompter = Prompt()\n", "    sample_files_path = Setup().load_sample_files(over_write=False)\n", "    doc_path = os.path.join(sample_files_path, \"Agreements\")\n", "    files = os.listdir(doc_path)\n", "    file_name = st.selectbox(\"Choose an Agreement\", files)\n\n", "    #   ** topic_query ** = this is a proxy for a more complex focusing retrieval strategy to target only a\n", "    #   specific part of the document, rather then the whole document\n", "    #   in this case, this will run a 'text match' search against the topic query to reduce the\n", "    #   text chunks reviewed in trying to answer the question\n", "    topic_query = st.text_area(\"Filtering Topic (hint: 'vacation')\")\n\n", "    #   ** prompt_text ** - this is the question that will be passed to the LLM\n", "    prompt_text = st.text_area(\"Question (hint: 'how many vacation days will the executive receive'\")\n", "    model_name = st.selectbox(\"Choose a model for answering questions\", [\"bling-phi-3-gguf\",\n", "                                                                         \"bling-tiny-llama-1b\",\n", "                                                                         \"bling-stablelm-3b-tool\",\n", "                                                                         \"llama-3-instruct-bartowski-gguf\",\n", "                                                                         \"dragon-llama-answer-tool\"])\n", "    if st.button(\"Run Analysis\"):\n", "        if file_name and prompt_text and model_name:\n", "            prompter.load_model(model_name, temperature=0.0, sample=False)\n\n", "            #   parse the PDF in memory and attach to the prompt\n", "            if not topic_query:\n", "                sources = prompter.add_source_document(doc_path,file_name)\n", "            else:\n", "                # this is where we use the topic_query to filter the parsed document\n", "                sources = prompter.add_source_document(doc_path,file_name, query=topic_query)\n\n", "            #   run the inference with the source\n", "            response = prompter.prompt_with_source(prompt_text)\n\n", "            #   fact checks\n", "            fc = prompter.evidence_check_numbers(response)\n", "            cs = prompter.evidence_check_sources(response)\n", "            if len(response) > 0:\n", "                if \"llm_response\" in response[0]:\n", "                    response = response[0][\"llm_response\"]\n", "                    st.write(f\"Answer: {response}\")\n", "                    if len(fc) > 0:\n", "                        if \"fact_check\" in fc[0]:\n", "                            fc_out = fc[0][\"fact_check\"]\n", "                            st.write(f\"Numbers Check: {fc_out}\")\n", "                    if len(cs) > 0:\n", "                        if \"source_review\" in cs[0]:\n", "                            sr_out = cs[0][\"source_review\"]\n", "                            st.write(f\"Source review: {sr_out}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}