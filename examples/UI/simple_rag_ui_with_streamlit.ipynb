{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n This example shows how to build a simple RAG application with UI with Streamlit and LLMWare.<br>\n", "    Note: it requires a separate `pip install streamlit`, and to run the script, you should run from the<br>\n", "    command line with:<br>\n", "     `streamlit run using_with_streamlit_ui.py`<br>\n", "    For this example, we will be prompting against a set of Invoice documents, provided in the LLMWare<br>\n", "    sample files.<br>\n", "    If you would like to substitute longer documents then please look at the UI example:<br>\n", "        -- rag_ui_with_query_topic_with_streamlit.py<br>\n", "    as a framework to get started integrating a retrieval step before the prompt of the source<br>\n", "    For more information about Streamlit, check out their docs:  https://docs.streamlit.io/develop/tutorials<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import streamlit as st"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llmware.prompts import Prompt\n", "from llmware.setup import Setup"]}, {"cell_type": "markdown", "metadata": {}, "source": ["st.set_page_config(layout=\"wide\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def simple_analyzer ():\n", "    st.title(\"Simple RAG Analyzer\")\n", "    prompter = Prompt()\n", "    sample_files_path = Setup().load_sample_files(over_write=False)\n", "    doc_path = os.path.join(sample_files_path, \"Invoices\")\n", "    files = os.listdir(doc_path)\n", "    file_name = st.selectbox(\"Choose an Invoice\", files)\n", "    prompt_text = st.text_area(\"Question (hint: 'what is the total amount of the invoice?'\")\n", "    model_name = st.selectbox(\"Choose a model for answering questions\", [\"bling-phi-3-gguf\",\n", "                                                                         \"bling-tiny-llama-1b\",\n", "                                                                         \"bling-stablelm-3b-tool\",\n", "                                                                         \"llama-3-instruct-bartowski-gguf\",\n", "                                                                         \"dragon-llama-answer-tool\"])\n", "    if st.button(\"Run Analysis\"):\n", "        if file_name and prompt_text and model_name:\n", "            prompter.load_model(model_name, temperature=0.0, sample=False)\n\n", "            #   parse the PDF in memory and attach to the prompt\n", "            sources = prompter.add_source_document(doc_path,file_name)\n\n", "            #   run the inference with the source\n", "            response = prompter.prompt_with_source(prompt_text)\n\n", "            #   fact checks\n", "            fc = prompter.evidence_check_numbers(response)\n", "            cs = prompter.evidence_check_sources(response)\n", "            if len(response) > 0:\n", "                if \"llm_response\" in response[0]:\n", "                    response = response[0][\"llm_response\"]\n", "                    st.write(f\"Answer: {response}\")\n", "                    if len(fc) > 0:\n", "                        if \"fact_check\" in fc[0]:\n", "                            fc_out = fc[0][\"fact_check\"]\n", "                            st.write(f\"Numbers Check: {fc_out}\")\n", "                    if len(cs) > 0:\n", "                        if \"source_review\" in cs[0]:\n", "                            sr_out = cs[0][\"source_review\"]\n", "                            st.write(f\"Source review: {sr_out}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}