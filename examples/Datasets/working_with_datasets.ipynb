{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ca9e2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6652d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This example demonstrates creating and using datasets\n",
    "    1. Datasets suitable for fine tuning embedding models\n",
    "    2. Completion and other types of datasets\n",
    "    3. Generating datasets from all data in a library or with filtered data\n",
    "    4. Creating datasets from AWS Transcribe transcripts\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee906a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from llmware.dataset_tools import Datasets\n",
    "from llmware.library import Library\n",
    "from llmware.retrieval import Query\n",
    "from llmware.setup import Setup\n",
    "from llmware.configs import LLMWareConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51418ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_use_dataset(library_name):\n",
    "\n",
    "    # Setup a library and build a knowledge graph.  Datasets will use the data in the knowledge graph\n",
    "    print (f\"\\n > Creating library {library_name}...\")\n",
    "    library = Library().create_new_library(library_name)\n",
    "    sample_files_path = Setup().load_sample_files()\n",
    "    library.add_files(os.path.join(sample_files_path,\"SmallLibrary\"))\n",
    "    library.generate_knowledge_graph()\n",
    "\n",
    "    # Create a Datasets object from library\n",
    "    datasets = Datasets(library)\n",
    "\n",
    "    # Build a basic dataset useful for industry domain adaptation for fine-tuning embedding models\n",
    "    print (f\"\\n > Building basic text dataset...\")\n",
    "\n",
    "    basic_embedding_dataset = datasets.build_text_ds(min_tokens=500, max_tokens=1000)\n",
    "    dataset_location = os.path.join(library.dataset_path, basic_embedding_dataset[\"ds_id\"])\n",
    "\n",
    "    print (f\"\\n > Dataset:\")\n",
    "    print (f\"(Files referenced below are found in {dataset_location})\")\n",
    "\n",
    "    print (f\"\\n{json.dumps(basic_embedding_dataset, indent=2)}\")\n",
    "    sample = datasets.get_dataset_sample(datasets.current_ds_name)\n",
    "\n",
    "    print (f\"\\nRandom sample from the dataset:\\n{json.dumps(sample, indent=2)}\")\n",
    "    \n",
    "    # Other Dataset Generation and Usage Examples:\n",
    "\n",
    "    # Build a simple self-supervised generative dataset- extracts text and splits into 'text' & 'completion'\n",
    "    # Several generative \"prompt_wrappers\" are available - chat_gpt | alpaca | \n",
    "    basic_generative_completion_dataset = datasets.build_gen_ds_targeted_text_completion(prompt_wrapper=\"alpaca\")\n",
    "    \n",
    "    # Build a generative self-supervised training sets created by pairing 'header_text' with 'text'\n",
    "    xsum_generative_completion_dataset = datasets.build_gen_ds_headline_text_xsum(prompt_wrapper=\"human_bot\")\n",
    "    topic_prompter_dataset = datasets.build_gen_ds_headline_topic_prompter(prompt_wrapper=\"chat_gpt\")\n",
    "    \n",
    "    # Filter a library by a key term as part of building the dataset\n",
    "    filtered_dataset = datasets.build_text_ds(query=\"agreement\", filter_dict={\"master_index\":1})\n",
    "    \n",
    "    # Pass a set of query results to create a dataset from those results only\n",
    "    query_results = Query(library=library).query(\"africa\")\n",
    "    query_filtered_dataset = datasets.build_text_ds(min_tokens=250,max_tokens=600, qr=query_results)\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    LLMWareConfig().set_active_db(\"sqlite\")\n",
    "\n",
    "    build_and_use_dataset(\"test_txt_datasets_0\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
