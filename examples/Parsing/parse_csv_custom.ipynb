{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n This script illustrates options for parsing csv and tsv files into a Library in LLMWare, including methods for<br>\n", "    mapping custom configured csv or tsv file, which is intended for use with 'pseudo-db' CSV files<br>\n", "    Option #1 - Standard CSV/TSV Parsing<br>\n", "        --  when using a bulk ingest Parsing method, the parser will route csv files to the 'standard'<br>\n", "            TextParser - which will look to extract and 'aggregate' the text from the csv as source content, and<br>\n", "            add to \"text\" and/or \"table\" attributes -> no \"structure\" information or targeted keys are captured<br>\n", "        -- the standard TextParser() is designed for ad hoc extraction of text content<br>\n", "        -- if file is csv, then by default delimiter assumed to be ',' (which can be adjusted)<br>\n", "        -- if file is tsv, then by default delimited assumed to be '\\t' (which can be adjusted)<br>\n", "        -- this is illustrated in example 1 and example 2 below.<br>\n", "    Option #2 - Custom Configured CSV/TSV Parsing<br>\n", "        -- if the CSV file is a pseudo-db with structured attributes, then it can be configured using a special<br>\n", "           parsing method, as outlined below in examples 3 and 4.<br>\n", "        -- the benefit of this custom mapping is that key column attributes will be saved as \"metadata\" in addition<br>\n", "           to the option to provide a custom over-write of doc_ID and block_ID parameters for indexing of text<br>\n", "            chunks in the database<br>\n", "  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llmware.parsers import Parser, TextParser\n", "from llmware.library import Library\n", "from llmware.retrieval import Query\n", "from llmware.configs import LLMWareConfig"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import time\n", "import ast"]}, {"cell_type": "markdown", "metadata": {}, "source": ["  All three text databases supported (mongo, postgres, and sqlite)<br>\n", "  if it is highly varied unstructured content, we would recommend Mongo given its flexibility<br>\n", "  if any validation errors with Postgres or SQLite, then we would recommend either further preprocessing the csv or<br>\n", "  ... trying with Mongo"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["LLMWareConfig().set_active_db(\"mongo\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def standard_csv_parsing(fp, fn, delimiter=\",\"):\n", "    \"\"\" Example #1 - This example shows the 'standard' text handler for csv \"\"\"\n\n", "    #   the standard csv parser will interpret the output as a table, trying to preserve the 'row-by-row' and\n", "    #   'cell-by-cell' structure (without any keys/labels).   There are several options how the output text will\n", "    #   be aggregated and saved as a single 'text' or 'table' entry:\n\n", "    #       --batch_size: this is the number of rows that will be aggregated into each text entry\n", "    #           e.g.,   if batch_size == 1, then each row will be a single entry in the database\n", "    #                   if batch_size == 10, then 10 rows will be aggregated into a single 'text' entry in the db\n\n", "    #       --interpret_as_table:\n", "    #           if true, then text will be packaged as a string wrapping a nested list.\n", "    #           if false, then text will be packaged as a single text stream separated by \"\\t\" between entries\n\n", "    #       --optional parameters allow configuration of encoding ('utf-8-sig'), errors ('ignore'),\n", "    #           and separator (\"\\n\") (applied at end of each row)\n\n", "    #   to experiment with the expected output, try the method below, which will not write to the DB, but outputs\n", "    #   a list in memory\n", "    output = TextParser().csv_file_handler(fp,fn,interpret_as_table=False,batch_size=1, delimiter=delimiter)\n", "    return output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def standard_csv_parsing_into_library(fp, library_name):\n", "    \"\"\" Example #2 - building on the first example, this example will parse a set of 'standard' CSV files\n", "        directly into the library - if file type is 'tsv' then delimiter automatically applied as '\\t' \"\"\"\n\n", "    #   create new library\n", "    lib = Library().create_new_library(library_name)\n\n", "    #   create parser object, and pass the library to use to write the parsing output\n", "    parser = Parser(lib)\n\n", "    #   directly call the parse_text method, which will parse text files (csv, tsv, json, jsonl, txt, md)\n", "    #   this parsed output will be saved to the database by default\n", "    output = parser.parse_text(fp, interpret_as_table=False,batch_size=1,delimiter=\",\", encoding=\"utf-8-sig\",\n", "                               write_to_db=True)\n", "    return output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def configured_csv_parsing(fp, fn,library_name):\n", "    \"\"\" Example #3 - This example shows how to use mappings for a customized csv \"\"\"\n\n", "    #   metadata is a dictionary mapping of key names to columns in the csv file\n", "    #   the 'keys' correspond to the keys that will be added to the library\n", "    #   the 'values' correspond to the columns found in the source CSV (starting with 0 index)\n\n", "    #   metadata map must have \"text\" mapping\n", "    #   if \"doc_ID\" or \"block_ID\" mapping provided, then will \"over-write\" the default doc_ID and block_ID and\n", "    #       use the mapping provided in the source CSV\n\n", "    #   for all other attributes (e.g., not text, doc_ID, block_ID), the keys will be stored in \"special_field1\" of\n", "    #   the database.  For Mongo, the keys will be stored directly as a dictionary, while for Postgres and SQLite,\n", "    #   it will be stored as text string, which must be converted upon use back into a dictionary (see below for\n", "    #   retrieval example)\n\n", "    #   step 1 - create metadata mapping,\n", "    #   e.g., number indexes map to columns in the csv, 0-index and negative slicing supported (-1 is last column)\n", "    metadata = {\"text\": -1, \"doc_ID\": 0, \"key1\": 1, \"key2\": 2, \"key3\": 3, \"key4\": 4}\n", "    columns = 6\n\n", "    #   step 2 - create library\n", "    lib = Library().create_new_library(library_name)\n", "    parser = Parser(lib)\n\n", "    #   step 3 - invoke parse_csv_config method\n", "    #   -- note: if file is not comma delimited, then set delimiter\n", "    #   -- if file is tab delimited, e.g. tsv, then delimiter = \"\\t\"\n", "    print(\"step 1 - parsing\")\n", "    t0 = time.time()\n", "    parser_output = parser.parse_csv_config(fp, fn, cols=columns, mapping_dict=metadata,delimiter=\",\")\n", "    print(f\"done parsing - time - {time.time() - t0} - summary - {parser_output}\")\n", "    return parser_output"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}