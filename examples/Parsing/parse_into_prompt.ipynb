{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n This example demonstrates how to parse a document 'in-flight' as part of a Prompt using \"Prompt with Sources\"<br>\n", "    1.  Load sample documents<br>\n", "    2.  Create a Prompt object<br>\n", "    3.  Load a locally-run BLING model (may take a few minutes to download the first time from HuggingFace)<br>\n", "    4.  Add Document as Source to Prompt<br>\n", "        -- this will automatically parse the source document, text chunk, and package into prompt context window<br>\n", "        -- optional query filter to narrow the list of text chunks packaged into the source<br>\n", "    5.  Invoke .prompt_with_sources method<br>\n", "        -- this will take the packaged source, and run inference on the LLM<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llmware.prompts import Prompt\n", "from llmware.setup import Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prompt_source (model_name):\n", "    print(f\"\\nExample:  Parse and Filter Documents Directly in Prompt to LLM\")\n\n", "    #   load the llmware sample files\n", "    print (f\"\\nstep 1 - loading the llmware sample files\")\n", "    sample_files_path = Setup().load_sample_files()\n", "    contracts_path = os.path.join(sample_files_path,\"Agreements\")\n\n", "    #   load bling model which will be used for the inference (will run on local laptop CPU)\n", "    #   --note:  typically requires 16 GB laptop RAM\n", "    print (f\"step 2 - loading model {model_name}\")\n\n", "    #   create prompt object\n", "    prompter = Prompt()\n", "    prompter.load_model(model_name)\n\n", "    #   this is the question that we will ask to each document\n", "    research = {\"topic\": \"base salary\", \"prompt\": \"What is the executive's base salary?\"}\n", "    for i, contract in enumerate(os.listdir(contracts_path)):\n\n", "        #   (optional) safety check to exclude Mac-specific file artifact\n", "        if contract != \".DS_Store\":\n", "            print(\"\\nAnalyzing Contract - \", str( i +1), contract)\n", "            print(\"Question: \", research[\"prompt\"])\n\n", "            # contract is parsed, text-chunked, and then filtered by \"base salary'\n", "            #   --note: query is optional - if no query, then entire document will be returned and added as source\n", "            source = prompter.add_source_document(contracts_path, contract, query=research[\"topic\"])\n\n", "            # take a look at the created source\n", "            print(\"Source created from document: \", source)\n\n", "            # calling the LLM with 'source' information from the contract automatically packaged into the prompt\n", "            responses = prompter.prompt_with_source(research[\"prompt\"], prompt_name=\"just_the_facts\", temperature=0.3)\n", "            for r, response in enumerate(responses):\n", "                print(\"\\nLLM Response: \", response[\"llm_response\"])\n\n", "            # We're done with this contract, clear the source from the prompt\n", "            #   -- note: if looking to aggregate or keep 'running' source, then do not clear\n", "            prompter.clear_source_materials()\n", "    return 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    model_name = \"llmware/bling-1b-0.1\"\n", "    prompt_source(model_name)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}